---
layout: page
title: Notes
subtitle: From Paul Jones at Rutgers University
---
{% raw %}
<h1>
Topics in Philosophy of Psychology <small>with Professor Frances Egan</small>
</h1>

<h2>
Syllabus
</h2>

<h3>
Course Description
</h3>

<p>
The topic of the seminar is psychological explanation. We will focus on two issues: (1) The role of representation in psychological explanation, considering recent challenges – by (among others) proponents of extended, embodied, and enactive cognition – to the traditional view that psychological processes are to be understood as operations on symbol structures. We will consider the requirements for a theoretical construct to count as a representation, and whether there are any distinctively mental representations. (2) The relation of psychology to neuroscience, considering the recent challenge – by proponents of the so-called 'new mechanism' view in philosophy of science – to the view that psychological explanation of human cognitive capacities (in particular, computational explanations of cognitive capacities) can be constructed and confirmed independently of an account of how these capacities are realized in the brain. The new mechanists argue that genuine explanations of cognition are mechanistic explanations – they must bear a transparent relationship to accounts of realizing neural mechanisms.
</p>

<h3>
Readings
</h3>

<h4>
The role of representation in psychological explanation
</h4>

<ul>
<li><p>
The traditional view – strong representationalism
</p>

<ul>
<li>
Jerry Fodor. <em>Psychosemantics</em>, ch.1, Appendix
</li>
<li>
Zenon Pylyshyn. &quot;The Explanatory Role of Representation&quot;
</li>
</ul></li>
<li><p>
Implications and challenges
</p>

<ul>
<li>
David Kirsh. &quot;When is Information Explicitly Represented?&quot;
</li>
<li>
William Ramsey. <em>Representation Reconsidered</em> (ch.3 and ch.4)
</li>
</ul></li>
<li><p>
The challenge from extended, embodied, and enactive cognition
</p>

<ul>
<li>
Rodney Brooks. &quot;Intelligence without Representation&quot;
</li>
<li>
John Haugeland. &quot;Mind Embodied and Embedded&quot;
</li>
<li>
Clark &amp; Chalmers. &quot;The Extended Mind&quot;
</li>
<li>
Adams &amp; Aizawa. &quot;Defending the Bounds of Cognition&quot;
</li>
<li>
Robert Rupert. &quot;Challenges to the Hypothesis of Extended Cognition&quot;
</li>
<li>
Lawrence Shapiro. <em>Embodied Cognition</em> (excerpt)
</li>
<li>
Andy Clark. &quot;An Embodied Cognitive Science?&quot;
</li>
<li>
Clark &amp; Toribio. &quot;Doing without Representing?&quot;
</li>
<li>
Anthony Chemero. &quot;Anti-representationalism and the Dynamical Stance&quot;
</li>
<li>
Shaun Gallagher. &quot;Are Minimal Representations Still Representations?&quot;
</li>
<li>
Daniel Hutto. &quot;Radically Enactive Cognition in our Grasp&quot;
</li>
<li>
Mark Sprevak. &quot;Fictionalism about Neural Representations&quot;
</li>
</ul></li>
</ul>

<h4>
The autonomy of psychology
</h4>

<ul>
<li><p>
The traditional view
</p>

<ul>
<li>
Jerry Fodor. &quot;Special Sciences&quot;
</li>
<li>
Robert Cummins. &quot;Functional Analysis&quot;
</li>
<li>
John Haugeland. &quot;The Nature and Plausibility of Cognitivism&quot;
</li>
</ul></li>
<li><p>
The 'new mechanist' challenge
</p>

<ul>
<li>
David Michael Kaplan. &quot;Explanation and Description in Computational Neuroscience&quot;
</li>
<li>
Piccinini &amp; Craver. &quot;Integrating Psychology and Neuroscience: Functional Analyses as Mechanism Sketches&quot;
</li>
<li>
Daniel Weiskopf. &quot;Models and Mechanisms in Psychological Explanation&quot;
</li>
<li>
Frances Egan. &quot;Function-Theoretic Explanation and Neural Mechanisms&quot;
</li>
</ul></li>
</ul>

<h3>
Course Requirements
</h3>

<p>
Students taking the course for credit will be expected to write a paper due at the end of the semester. Papers must be on topics related to the course materials; topics should be cleared with me beforehand. Graduate students enrolled in the course will be expected to lead the discussion of assigned materials for one session.
</p>

<h3>
Attendance
</h3>

<p>
Attendance at the seminar is mandatory. You should let me know if you have to miss a class.
</p>

<h3>
Course Objectives
</h3>

<p>
Like any advanced philosophy graduate seminar this seminar aims to give students an opportunity to explore a set of fundamental issues in depth give students opportunities to further develop and hone their analytical skills give students a chance to write and revise a major research paper that will hopefully be interesting, original, and important provide ample opportunities to further develop one's oral skills during discussions
</p>

<h2>
January 28th, 2013 <small>Reading</small>
</h2>

<h3>
Psychosemantics <small>Jerry Fodor</small>
</h3>

<h4>
Chapter 1 <small>The Persistence of Attitudes</small>
</h4>

<h5>
Introduction
</h5>

<ul>
<li>
Quotes <em>A Midsummer Night's Dream</em> for an example of implicit, nondemonstrative, theoretical evidence.
</li>
<li><p>
Here is how the inference must have gone:
</p>

<ul>
<li><p>
Hermia had reason to believe herself beloved of Lysander.
</p>

<ul>
<li>
Because he told her.
</li>
</ul></li>
<li><p>
If Lysander loves Hermia, then Lysander wishes Hermia well.
</p></li>
<li>
If Lysander wishes Hermia well, then Lysander does not voluntarily desert Hermia in the night in the woods.
</li>
<li>
But Hermia was deserted by Lysander.
</li>
<li>
Therefore, not voluntarily.
</li>
<li>
Therefore, it is plausible Lysander has come to harm, and plausibly by Demetrius's hands, for Demetrius is Lysander's rival for the love of Hermia.
</li>
</ul></li>
<li><p>
Hermia believes (correctly) that: If <em>x</em> wants that <em>P</em>, and <em>x</em> believes that not-<em>P</em> unless <em>Q</em>, and <em>x</em> believes that <em>x</em> can bring about <em>Q</em>, then (ceteris paribus) <em>x</em> tries to being it about that <em>Q</em>.
</p></li>
<li><p>
But Hermia has it wrong about Demetrius.
</p>

<ul>
<li>
The intricate theory that she relies on to make sense of her peers, that we rely on to make sense of Hermia, and what Shakespeare relies on to predict and manipulat our sympathies.
</li>
</ul></li>
<li><p>
This theory, Fodor wants to emphasize:
</p>

<ol>
<li>
How often it goes right,
</li>
<li>
How deep it is,
</li>
<li>
How much we do depend on it.
</li>
</ol></li>
</ul>

<h5>
How often it works
</h5>

<ul>
<li><p>
Applications of commonsense mediate our relations with one another, and when its predictions fail these relations break down.
</p>

<ul>
<li>
Failures make for great theater.
</li>
<li>
Succeses are practically invisible and ubiquitous.
</li>
</ul></li>
<li><p>
Commonsense psychology is like those mythical Rolls Royce cars whose engines are seals when they leave the factory.
</p>

<ul>
<li>
Someone I don't know calls me and asks me to lecture in Arizone on Tuesday.
</li>
<li>
Fodor responds, &quot;Yes, thank, I'll be at your airport on the 3 p.m. flight.&quot;
</li>
<li>
<p>That's <em>all</em> that happens.</p>
<ul>
<li>
The theory is used to gap the bridge betwen utterances and actions.
</li>
<li>
If Fodor doesn't show it, the theory makes predicts about the likelihood of why.
</li>
</ul></li>
</ul></li>
<li><p>
The point is that the theory from which we get this extrodinary predictive power is just good old commonsense belief/desire psychology.
</p>

<ul>
<li>
It tells us how to infer people's intentions from the sounds they make.
</li>
<li>
It tells us how to infer people's behavior from their intentions.
</li>
<li>
And all of this works on your friends and your spouses and absolute strangers.
</li>
</ul></li>
<li><p>
But what about all those ceteris paribuses?
</p>

<ul>
<li>
Philosophers love the &quot;false or vacuous&quot; dilemma.
</li>
</ul></li>
<li><p>
Consider the defeasibility of &quot;if someone utters the form of words, 'I'll be at your airport on the 3 p.m. flight,' then he intends to be at your aiport on the 3 p.m. flight.&quot;
</p>

<ul>
<li><p>
This generalization does <em>not</em> hold if:
</p>

<ul>
<li>
The speaker is lying,
</li>
<li>
Monoligual speaker of Urdu who uttered by accident,
</li>
<li>
If the speaker is talking in his sleep,
</li>
<li>
Or ... whatever.
</li>
</ul></li>
<li><p>
Perhaps all that this means with a <em>ceteris paribus</em> clause is, if it doesn't happen, then it still true because <em>ceteris paribus</em>.
</p></li>
</ul></li>
<li><p>
A lot of philosophers are moved by this.
</p>

<ul>
<li>
<p>But the predictions often come out true!</p>
<ul>
<li>
So how could it be <em>empty</em>?
</li>
</ul></li>
</ul></li>
<li><p>
The reliance on uncashed <em>ceteris paribus</em> clauses is a general property of the <em>explicit</em> generalizations in <em>all</em> the special sciences.
</p>

<ul>
<li><p>
For example, &quot;A meandering river erodes its outside bank&quot;
</p>

<ul>
<li>
False or vacuous a philosopher may complain.
</li>
</ul></li>
<li><p>
A <em>ceteris paribus</em> clause here cause &quot;If <em>p</em> then <em>q</em> unless not <em>p</em> then <em>q</em>.&quot;
</p></li>
</ul></li>
<li><p>
Something must have gone wrong, surely this explanation is meaningful despite the big <em>ceteris paribus</em> clause.
</p>

<ul>
<li>
Surely these statements are stronger than &quot;<em>P</em> in any world where not not-<em>P</em>.&quot;
</li>
</ul></li>
<li><p>
There is a face similarity between implicit generalizations and commonsense psychology and explicity generalization in special sciences.
</p>

<ul>
<li>
We can get rid of the <em>ceteris paribus</em> clauses by <em>actually enumerating</em> the conditions.
</li>
</ul></li>
<li><p>
By this criterion, the only real science is basic physics.
</p>

<ul>
<li>
We can't enumarate all the conditions on geology by sticking to the vocabulary of geology.
</li>
<li>
The events in the <em>ceteris paribus</em> clause about the rivers aren't geological events, it's outside the domain. (Also hard.)
</li>
</ul></li>
<li><p>
Exceptions to generalizations of a special science are typically inexplicable from the point of view of that science.
</p>

<ul>
<li>
This is what makes it <em>special</em>.
</li>
<li>
<p>It's possible to enumerate them in the vocaubalry of another science.</p>
<ul>
<li>
You go &quot;down&quot; one or more levels and use the vocabulary of the more &quot;basic&quot; science.
</li>
</ul></li>
</ul></li>
<li><p>
If the world is describable at all in a closed causal system, it is with vocabulary of the most basic science.
</p>

<ul>
<li>
But the psychologist and the geologist needn't worry about this.
</li>
<li>
If you want to know where Fodor will be next Thursday, mechancins is <em>no use to you at all</em>.
</li>
</ul></li>
</ul>

<h5>
The depth of the theory
</h5>

<ul>
<li><p>
It's tempting to think that commonsense psychology is a toolkit of truisms one learns on Granny's knee.
</p>

<ul>
<li><p>
Like,
</p>

<ul>
<li>
The burnt child fears the fire.
</li>
<li>
Money cannot buy happiness.
</li>
<li>
Reinforcment affect response rate.
</li>
<li>
A way to a man's heart is through his stomach.
</li>
</ul></li>
<li><p>
None of these are worth save, but commonsense psychology is not like this.
</p></li>
<li>
<p>There are two parts to this:</p>
<ol>
<li>
The theory's underlying generalizations are defined over unobservables.
</li>
<li>
They lead to its predictions by iterating and interacting rather than being directly instantiated.
</li>
</ol></li>
</ul></li>
<li><p>
Behavior is casued by mental states and this causation is intricate.
</p>

<ul>
<li>
<p>Roughly, <em>If x is y's rival, the x prefers y's discomfiture, all else being equal</em>.</p>
<ul>
<li>
Doesn't mention behavior.
</li>
<li>
It <em>leads to</em> behvaioral predictions.
</li>
</ul></li>
</ul></li>
<li><p>
It is a deep fact about the world that the most powerful etiological generalizations hold of unobservable causes.
</p>

<ul>
<li>
Meterology is <em>not</em> deep because it's genralization are of the form, &quot;Red at night, sailors delight.&quot;
</li>
<li>
<p>Psychology <em>is</em> a deep theory, because we do not have access to mental states.</p>
<ul>
<li>
We're born mentalisms and realists, and we stay that way until common sense is driven out by bad philosophy.
</li>
</ul></li>
</ul></li>
</ul>

<h5>
Its indispensability
</h5>

<ul>
<li><p>
We have no alternative to the vocabulary of commonsense psychological explanation.
</p>

<ul>
<li>
There is no other way of describing our behaviors and their causes if we want our behaviors and their causes to be subsumed by any counterfactual-supporting generalizations that we know about.
</li>
</ul></li>
<li><p>
Without commonsense psychological generalizations, we cannot even describe the utterances as forms of words.
</p>

<ul>
<li>
Word is a <em>psychological</em> category.
</li>
<li>
<p>There are non acoustic properties that all and only <em>fully intelligble</em> tokens of the same word type share.</p>
<ul>
<li>
Which is why our best technology cannot build a typewriter you can dictate to.
</li>
</ul></li>
</ul></li>
<li><p>
We have <em>no</em> vocabulary for describing event types with these conditions:
</p>

<ol>
<li>
My behvior in uttering, &quot;I'll be there on Thursday&quot; counts as an event of type <span class="math">\(T_i\)</span>.
</li>
<li>
My arriving there on Thursday counts as an event of type <span class="math">\(T_j\)</span>.
</li>
<li>
'Events of type <span class="math">\(T_j\)</span> are consequent upon events of type <span class="math">\(T_i\)</span>' is even roughly true and counterfactual supporting.
</li>
<li>
Categories <span class="math">\(T_i\)</span> and <span class="math">\(T_j\)</span> are other than irreducibly psychological.
</li>
</ol></li>
<li><p>
Physics describes organisms <em>qua</em> motions, but not <em>qua</em> organismic.
</p>

<ul>
<li>
It dissolves the behav<em>er</em> and the behav<em>ior</em> into atoms in the void.
</li>
</ul></li>
<li><p>
Even if psychology was dispensible <em>in principle</em>, it's not argument for dispensing with it <em>in practice</em>.
</p></li>
</ul>

<h6>
The essence of the attitudes
</h6>

<blockquote>
  <p>
How do we tell whether a psychology <em>is</em> a belief/desire psychology? How, in general, do we know if propositional attitudes are among the entities that the ontology of a theory acknowledges? ... How do you distinguish elimination from reduction and reconstruction?
</p>
</blockquote>

<ul>
<li>
<p>Fodor will view psyhology as being commensensical about the attitudes just in case it postulates states satisfying:</p>
<ol>
<li>
Thy are semnatically evaluable.
</li>
<li>
The have causal powers.
</li>
<li>
The implicit generalizations of commonsense belief/desire psychology are largely true of them.
</li>
</ol></li>
</ul>

<blockquote>
  <p>
Squabbling about intuitions strikes me as vulgar.
</p>
</blockquote>

<h6>
<abbr title="Representational theory of mind">RTM</abbr>
</h6>

<ul>
<li>
<strong>Thesis</strong>: We have no reason to doubt - indeed, we have substantial reason to believe - that it is possible to have a scientific psychology that vindicates commonsense belief/desire explanation.
</li>
<li>
Fodor will argue that the sorts of objections philosophers have recently raised against belief/desire explanation are not conclusive against the best vindicating theory currently available.
</li>
<li>
At the heart of <abbr title="Representational theory of mind">RTM</abbr> is the postulation of a <abbr title="Language of thought">LOT</abbr>, an infinite set of 'mental representations' which function both as immediate objects of propositional attitudes and as the domains of mental processes.
</li>
<li><p>
The two claims he's making:
</p>

<ol>
<li><p>
<strong>The nature of propositional attitudes</strong>:
</p>

<ul>
<li>
<p>For any organism <em>O</em>, and any attitude <em>A</em> towards the proposition <em>P</em>, there is a (computational/functional) relation <em>R</em> and a mental representation <em>MP</em> such that:</p>
<ul>
<li>
<em>MP</em> means that <em>P</em>.
</li>
<li>
<em>O</em> has <em>A</em> just in case <em>O</em> bears <em>R</em> to <em>MP</em>.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>The nature of mental processes</strong>:
</p>

<ul>
<li>
Mental processes are causal sequences of tokenings of mental representations.
</li>
</ul></li>
</ol></li>
<li><p>
A train of thought is a causal sequence of tokenings of mental representations which express the propositions that are the objects of the thoughts.
</p></li>
<li>
<p><abbr title="Representational theory of mind">RTM</abbr> underlies practicaly all current psychological research on mentation, and the best science is ipso fact the best estimate of what there is and what it's made of.</p>
<ul>
<li>
Philosophers do not think this convincing, and Fodor is blushing for them.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
There is a stricking parallelism between the causal relations among mental states, on the one hand, and the semantic relations that hold among propositional objects, on the other.
</p>
</blockquote>

<ul>
<li>
Trains of thought are largely truth preserving<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">1</a></sup>.
</li>
<li>
<p>The &quot;trick&quot; is to combine the postulation of mental representations with the &quot;computer metaphor.&quot;</p>
<ul>
<li>
<p>Computers show us how to connect semantics with causal properties for symbols.</p>
<ul>
<li>
If having propositional attitude involves tokening a symbol, then we can get some leverage on connection semantical properties with causal ones for <em>thoughts</em>.
</li>
</ul></li>
</ul></li>
</ul>

<h2>
January 28th, 2014 <small>Seminar</small>
</h2>

<h3>
On representation
</h3>

<ul>
<li><p>
Advanced &quot;beings&quot; like us have evolved with our environment, this is with representation.
</p>

<ul>
<li>
These somehow causally affect behavior.
</li>
<li>
This is an &quot;inference to the best explanation.&quot;
</li>
</ul></li>
<li><p>
To the degree which organisms can react to a changing circumstance causes the representation to change, and the changed representations change behavior.
</p></li>
<li>
Some people think that dynamic behavior requires representationalism.
</li>
</ul>

<dl>
<dt>
Representation
</dt>
<dd>
A capacity of an organism.
</dd>

<dt>
Representation<em>s</em>
</dt>
<dd>
Concrete objects in the brain.
</dd>

<dd>
<p>
Some properties: - Physically realized. - They have causal powers, can have straightforward causal roles.
</p>

<ul>
<li>
<p>They have content, they are meaningful.</p>
<ul>
<li>
This requires a distinction between a vehicle of representation, they thing physically realized.
</li>
</ul></li>
</ul>
</dd>
</dl>

<ul>
<li>
Vision resonates with the environment like a tuning fork. Gibson's theory of perception.
</li>
<li>
Our brains or us might have the structure for representations, but we can't &quot;poke at&quot; any representation.
</li>
<li><p>
One way that representat<em>ion</em> could get cached out is <em>dispositionally</em>.
</p>

<ul>
<li>
Some theories might posit capacities that are best characterized as <em>representational</em>.
</li>
<li>
<p>There might not be an isolable state at the computational level that's like, &quot;There's the representation!&quot;</p>
<ul>
<li>
A real property of an organism, but diffuse.
</li>
<li>
Like mass.
</li>
</ul></li>
</ul></li>
<li><p>
Many theories posit something analogous to &quot;sentences in the head.&quot;
</p>

<ul>
<li>
<p>But there are other models of cognition.</p>
<ul>
<li>
Connectionist
</li>
<li>
Dynamical
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Vehicle-side
</h3>

<dl>
<dt>
Symbols
</dt>
<dd>
Little hooks on which you can hook meaning. Ripe for having content attributed to them, representation paradigmatically.
</dd>

<dt>
Connectionists models
</dt>
<dd>
Don't posit anything like &quot;sentences in the head.&quot; The carriers of meaning are networks of nodes that are connected in various ways. Do get interpreted, but do individual nodes get interpreted? <em>That</em> node refers to red <em>in a network</em>.
</dd>

<dd>
<p>
Typically are not construed as a strong representational view.
</p>
</dd>

<dt>
Dynamical models
</dt>
<dd>
Model the behavior of the system over time.
</dd>
</dl>

<ul>
<li><p>
What are the <em>vehicles</em> of meaning in non-symbol cognitive models?
</p>

<ul>
<li>
Is there anything in here that could be plausible construed as a representation?
</li>
</ul></li>
<li><p>
In classical models, what the vehicles of representation are, symbols are hooks on which to hang interpretation. Waving red flags, interpret me.
</p></li>
</ul>

<h3>
Content-side
</h3>

<ul>
<li><p>
Content has satisfaction conditions.
</p></li>
<li><p>
The problem of intentionality: How do mental states <em>get</em> their meaning? What is it about mental representations that given them their meaning or content.
</p>

<ul>
<li><p>
Public representation
</p>

<ul>
<li>
Public language content get their meaning by convention.
</li>
<li>
Icons and images get meaning by their resemblance and convention.
</li>
</ul></li>
<li><p>
Private representation
</p>

<ul>
<li>
Some relation that isn't <em>already presuming</em> meaning or intentionality.
</li>
<li>
The &quot;meaning in the head&quot; or &quot;state of affairs&quot; which it representations, <strong>the naturalism semantics project</strong>.
</li>
<li>
&quot;Complex causal relations&quot; represented in my head.
</li>
<li>
Teleological function of the brain, dealing with objects and properties I'm used, and my ancestors are, used to.
</li>
</ul></li>
</ul></li>
<li><p>
No one has be successful in naturalistic conditions on vehicle in the head having determinate content.
</p></li>
<li>
<p>One constraint on all of these account is that mental states can not only <em>represent</em> but also <em>mis</em>represent.</p>
<ul>
<li>
<p>The contrast is with Grice's notion of natural meaning.</p>
<ul>
<li>
The presence of smoke cannot misrepresent the presence of fire <em>unless</em> an agent misinterprets the smoke or something.
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Strong representationalism
</h3>

<dl>
<dt>
Strong representationalism
</dt>
<dd>
Posits structure entities that have content.
</dd>

<dd>
<p>
Mental processes are defined over the structures.
</p>
</dd>
</dl>

<ul>
<li>
An important issue: is there anything in non-representational views that could count as a vehicle.
</li>
</ul>

<dl>
<dt>
Extended cognition
</dt>
<dd>
The mind extends into the environment in some sense.
</dd>

<dd>
<p>
The extended thesis is not centrally a critique of representationalism, but it does have theses that bring to bear on representationalism.
</p>
</dd>

<dt>
Embodied cognition
</dt>
<dd>
Human cognition is necessarily embedded, the mind-brain is the executive that controls the body.
</dd>

<dt>
Dynamical cognition
</dt>
<dd>
Cognition consists in a dynamical interaction between a subject and an environment, and it's wrong to characterize the interaction as contentful.
</dd>
</dl>

<h3>
Jerry Fodor's &quot;Psychosemantics&quot;
</h3>

<ul>
<li><p>
He gives two arguments for the view known as &quot;representational theory of mind.&quot;
</p>

<ul>
<li><p>
Folk psychology is indispendable.
</p>

<ul>
<li>
The only way to vindicate it is if something like <abbr title="Representational theory of mind">RTM</abbr> is true.
</li>
</ul></li>
<li><p>
Eliminativism would be an absolute disaster.
</p></li>
</ul></li>
<li><p>
The second is the striking parrallelism between trains of thought and inferences.
</p></li>
<li>
The argument focuses on prediction, the only way to get around in the world is to ask someone something and predict that, in general, you do what you say and similar elsewhere.
</li>
<li>
Intentional realism is the only appropriate attitude to take towards folk psychology.
</li>
</ul>

<dl>
<dt>
Intentional realism
</dt>
<dd>
<ol style="list-style-type: decimal">
<li>They are semantically evaluable.</li>
<li>They have causal powers.</li>
<li>The implicit generalizations of commonsense belief/desire psychology are largely true of them.
</dd>
</li>
</ol>
<dt>
<abbr title="Representational theory of mind">RTM</abbr>
</dt>
<dd>
The best hope for realizing intentional realism.
</dd>

<dd>
<ol>
<li>
For any organism O, and any attitude A toward the proposition P, there is a ('computational'/'functional') relation R and amental representation MP such that MP means that PI and O has A iff O bears R to MP.
</li>
<li>
Mental processes are causal sequences of tokenings of mental representations.
</li>
</ol>
</dd>

<dd>
<p>
There are representation in a full-blooded sense.
</p>
</dd>
</dl>

<ul>
<li><p>
If the <abbr title="Language of thought">LOT</abbr> is true for propoisitonal attitudes, then the <abbr title="Representational theory of mind">RTM</abbr> is true because it's the weaker view.
</p>

<ul>
<li>
There's a big project on mental logic.
</li>
</ul></li>
<li><p>
Oftentimes, people's reason doesn't follow logic.
</p>

<ul>
<li>
People will more often affirm the consequent, more common in mental reasoning than <em>modus tollens</em>.
</li>
</ul></li>
<li><p>
&quot;I believe there is beer in the refridgerator.&quot;
</p>

<ul>
<li>
What was causally efficaious was that I wanted beer and believed there was beer, so I got up to get beer.
</li>
<li>
This is the &quot;belief box.&quot;<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">2</a></sup>
</li>
</ul></li>
<li><p>
There has to be a pretty good argument for the moving from property of one structure to the structure of the other. Specifically, they share semantics, but we need a good argument about syntax. Moving from the logical scheme to the empirical scheme.
</p>

<ul>
<li>
An argument <em>for this</em> is the tempurature case.
</li>
</ul></li>
<li><p>
<strong>We'll pick up next time with the Dennett counter-example.</strong>
</p></li>
</ul>

<h2>
February 4th, 2014 <small>Seminar</small>
</h2>

<h3>
On Fodor's <em>Psychosemantics</em>, Chapter 1 &quot;The Persistence of Attitudes&quot;
</h3>

<ul>
<li><p>
Fodor is looking for a theory for states that interprets attitudes as states with propositions, beliefs and desires <em>at least</em>.
</p>

<ul>
<li>
Chomsky: &quot;We <em>cognize</em> the grammar of English.&quot;
</li>
<li>
&quot;We <em>believe</em> the grammar of our language.&quot;
</li>
</ul></li>
<li><p>
This is the &quot;last gasp&quot; of narrow content for Fodor.
</p></li>
<li>
<p>On Dennet's counterexample.</p>
<ul>
<li>
<p>There is a distinction between core and derivative.</p>
<ul>
<li>
Those that <em>explicitly</em> represented and those that are <em>implicitly</em> represented.
</li>
<li>
Our heads just aren't big enough.
</li>
</ul></li>
</ul></li>
</ul>

<h2>
February 11th, 2014 <small>Reading</small>
</h2>

<h3>
Zenon Pylyshyn. &quot;The Explanatory Role of Representation&quot;
</h3>

<h4>
Introduction
</h4>

<ul>
<li>
<p>The hardest puzzle is consciousness.</p>
<ul>
<li>
Second hardest is <em>meaning</em>, which this work explains.
</li>
<li>
Does <em>not</em> solve the puzzle of meaning.
</li>
<li>
The author aims to <strong>describe how the idea of the semantic content of representations is implicitly viewed within the field of cognitive science, and discuss why this view is justifiable</strong>.
</li>
</ul></li>
</ul>

<dl>
<dt>
Representations
</dt>
<dd>
Generalizations stated over the contents of representations are not mere functional generalization in the usual sense.
</dd>

<dt>
Function generalizations
</dt>
<dd>
A theory that does not refer to physical properties of the particular system in question, only how it operates.
</dd>
</dl>

<ul>
<li>
There will be a <em>representational level</em> and a <em>symbol-processing level</em>.
</li>
</ul>

<h4>
The Appeal to Representations
</h4>

<ul>
<li><p>
Law-like generalization and explanations can differ in several ways, consider:
</p>

<ol>
<li>
A certain object accelerated at <em>a</em> meters per second per second because a steady force was applied that was equal to <em>ma</em>.
</li>
<li>
A certain neuron fired because a potential of <em>v</em> millivolts was applied along two of its sentries and that it had been inactive during the previous <em>t</em> milliseconds
</li>
<li>
A bit pattern of certain computer register came to have a particular configuration because of the particular contents present in the instruction register and the program counter, and because the system is wired according to a certain transfer protocol.
</li>
<li>
The computer printed numbers 2, 4, 6, because it started with the number 2 and added 2 repeatedly or because it applied the successor function repeatedly and double the value before printing.
</li>
<li>
The pedestrian dialed 911 because he believed it to be the emergency number and had recognized the urgent need for assistance.
</li>
</ol></li>
<li><p>
Accounts (1), (2), and (3), all the terms refer to properties of objects within the closed system.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">3</a></sup>
</p></li>
<li>
Accounts (4) and (6) are different in this important respect: Both make substantive reference to entities or properties that are not an intrinsic part of their state description, that is <em>numbers</em> and <em>need for assistance</em>.
</li>
</ul>

<blockquote>
  <p>
How is it possible for properties of the world to determine behavior when the properties are not causally related in the required sense to to the functional states of the system? --- <strong>Brentano's problem</strong>
</p>
</blockquote>

<ul>
<li>
The notion of representation is necessary only in the context of explanation.
</li>
<li>
Behavior is being caused by certain states of one's brain, and so mental states themselves are related to agent's actions.
</li>
<li><p>
Brain states are not causally connected in appropriate ways to walking or mountains.
</p>

<ul>
<li><p>
The relationship <em>is one of content</em>, a semantic, not causal, relationship.
</p>

<ul>
<li>
The notion of content is roughly that of what the states <em>are about</em>.
</li>
</ul></li>
<li><p>
Brain states cause certain movements. If these movements are view as members of equivalence classes described as &quot;writing a sentence about walking in the Santa Cruz mountains&quot; the brains states must be treated as embodying representations of these codes by certain rules.
</p></li>
</ul></li>
<li><p>
Contrast the brain to a watch -- a watch's &quot;behavior&quot; is considered coextensive with the set of movements corresponding to the physical description of behavior.
</p>

<ul>
<li>
Two ways of explaining human behavior capture extremely different generalizations.
</li>
</ul></li>
</ul>

<h4>
Representational and Functional Levels
</h4>

<ul>
<li>
This shows that <strong>a <em>functional</em> description of mental processes is not enough, there must also be content</strong>.
</li>
</ul>

<blockquote>
  <p>
If the content makes a difference to behavior, is it not also a functional difference?
</p>
</blockquote>

<ul>
<li><p>
To be in a certain representational state is to have a certain symbolic expression in some part of memory.
</p>

<ul>
<li>
The expression <em>encodes</em> the semantic interpretation and the combinatorial structure of the expression encodes the relation among the contents of the subexpressions, much as in the combinatorial system of predicate calculus.
</li>
</ul></li>
<li><p>
The reason there must be symbolic codes is that they can enter in causal relations.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">4</a></sup>
</p></li>
<li>
<p>If there is a unique symbolic expression corresponding to each content, one might expect functional states and representational states to once again be one-to-one relation. Not so, because:</p>
<ol>
<li>
There may be codes with the same semantic content which are functionally but not semantically distinguishable.
</li>
<li>
<p>Merely possessing a certain symbolic expression that encodes semantic content is insufficient to produce behavior.</p>
<ul>
<li>
You need to <em>interpret</em> the symbols.
</li>
</ul></li>
</ol></li>
</ul>

<dl>
<dt>
Semantic-level generalization
</dt>
<dd>
Generalizations expressible in terms of the semantic content of representations.
</dd>

<dd>
<p>
Newell calls this &quot;knowledge-level.&quot;
</p>
</dd>

<dt>
Symbol-level generalizations
</dt>
<dd>
Generalizations expressible in terms of functional properties of the functional architecture.
</dd>
</dl>

<h4>
Representational Content as Defining a Level of Description
</h4>

<ul>
<li><p>
We abandoned a biological vocabulary because of arbitrarily large disjunctions corresponding to processes like &quot;thinking.&quot;
</p>

<ul>
<li>
Functional generalizations cannot be captured in a finite neurophysiological description.
</li>
<li>
<p>There is a vocabulary in between &quot;<em>n</em> fired at <em>t</em> with <em>v</em>&quot; and &quot;He called 911 because he believed he was in an emergency.&quot;</p>
<ul>
<li>
And it's <em>functional</em>.
</li>
<li>
And it's in <em>semantic terms</em>.
</li>
</ul></li>
</ul></li>
<li><p>
&quot;The principal of rationality is a major reason for our belief that a purely functional account will fail to capture certain generalizations, hence, that a distinct new level is required.&quot;
</p></li>
</ul>

<h5>
Levels and Constraints on Realizability
</h5>

<ul>
<li>
<p>For a description, that description might be compatible with other levels.</p>
<ul>
<li>
Newtons laws &quot;are compatible with&quot; biological taxonomy.
</li>
</ul></li>
</ul>

<h3>
Kirsh, &quot;When is Information Explicitly Represented?&quot;
</h3>

<h4>
Introduction
</h4>

<ul>
<li>
Computation is the process of making <em>ex</em>plicit what was <em>im</em>plicit.
</li>
<li>
We know what is explicit, the problem is what information is implicit.
</li>
<li>
<strong>Suppose</strong>: To understand a computation it is necessary to track the trajectory of informational state the computing system follows as it winds its way to an explicit answer.
</li>
<li><p>
Different kinds of computational mechanisms:
</p>

<ul>
<li>
PDP systems
</li>
<li>
Massive cellular automata
</li>
<li>
Analog relaxation systems
</li>
</ul></li>
<li><p>
How far do you have to remove &quot;explicitness&quot; until it's implicit? That is, intuitively.
</p></li>
<li>
Suppose a system has highly ambigious encodings and must deliberate to choose the right interpretation.
</li>
</ul>

<blockquote>
  <p>
Computer and cognitive scientists talk as if they have a precise idea of these concepts (of implicit and explicit,) but that do not.
</p>
</blockquote>

<ul>
<li>
<p><strong>Intent</strong>: Articulate a particular conception of explicit information that at least may serve as a stable base for subsequent inquiries into the meaning of implicit information.</p>
<ol>
<li>
Will show why notions of explicit and implicit need elucidation, that they are not consistent.
</li>
<li>
Will explore efforts to identify explicit information with syntatically and semantically well-defined representations.
</li>
<li>
Will mention some implications of the view.
</li>
</ol></li>
</ul>

<h4>
Our intuitions about explictness are inconsistent
</h4>

<ul>
<li>
<p>Perhaps the intuition is that &quot;if it's there for all to see&quot;, then it's explicit.</p>
<ul>
<li>
<p>Four tempting properties of explicitness:</p>
<ol>
<li>
<strong>Locality</strong>: They are visible structures with a definite defintion.
</li>
<li>
<strong>Movability</strong>: No matter where in a book a word is found or where in the library a book is stored, that word retains its meaning and retains its explicitness.
</li>
<li>
<strong>Meaning</strong>: Words have a definite information content.
</li>
<li>
<strong>Availability</strong>: The information content of a word is directly available to the system reading it, no elaborotate transition or interpretation process is necessary to extract the information it represents.
</li>
</ol></li>
</ul></li>
</ul>

<blockquote>
  <p>
The trouble with using immediate grapability, or better <em>immediate readability</em> as the mark of explicitness is that we run into problems as soon as we ask whether to count accessing time as part of the reading process.
</p>
</blockquote>

<ul>
<li><p>
Are elements in large sets immediate readable?
</p>

<ul>
<li>
This takes <em>computational energy</em> but is somehow <em>constant</em>/
</li>
</ul></li>
<li><p>
From a process perspective information is explicit only when it is <em>ready to be used</em>
</p>

<ul>
<li>
No computation necessary.
</li>
</ul></li>
<li><p>
Explicitness is tied to usability.
</p></li>
</ul>

<h4>
Our intuitions about implcitness are inconsistent
</h4>

<ul>
<li><p>
Our concept of implicit runs into problems when we try to pin down what &quot;in principle, recoverable&quot; means.
</p>

<ul>
<li>
Is it how much effort is required?
</li>
<li>
Our all lemmas, &quot;nearby&quot; or &quot;far&quot;, equally as implicit?
</li>
</ul></li>
<li><p>
To make it more natural, perhaps the conception of <em>that which is not explicit but which could be made so</em>.
</p></li>
</ul>

<h4>
Why it matters whether our intuitions are unsetteled
</h4>

<h4>
Towards a theory of explicitness
</h4>

<h5>
Four condition on explicitness
</h5>

<dl>
<dt>
Locality
</dt>
<dd>
<p>
They states, structures, or processes - henceforth symbols - which explcitly encode information must be easily seperable from each other.
</p>
</dd>

<dt>
Movability
</dt>
<dd>
<p>
An ambigious language may explicitly encode information only if it is trivial to indetify the syntatic and semantic indentity of the symbol.
</p>
</dd>

<dt>
Immediately readable
</dt>
<dd>
<p>
Symbols explicitly encode information if they are either: - Readable in constant time. - Sufficiently small to fall in the attention span of an operator.
</p>
</dd>

<dt>
Meaning
</dt>
<dd>
<p>
The information which a symbol explictly encodes is given by the set of associated states, structures, or processes it activates in constant time.
</p>
</dd>
</dl>

<h4>
Implications
</h4>

<ul>
<li>
<p>One of the following is false:</p>
<ul>
<li>
The <abbr title="Language of thought">LOT</abbr> is the best level of analysis to represent perspicuously the episodes of in our <em>mental life</em>.
</li>
<li>
The events in our mental life are identical with operations on explicit representations.
</li>
<li>
The <abbr title="Language of thought">LOT</abbr> perspicously describes human information processing.
</li>
</ul></li>
</ul>

<h2>
February 11th, 2013 <small>Seminar</small>
</h2>

<dl>
<dt>
Eliminativism
</dt>
<dd>
<p>
Unless there's a transparent relationship between folk psychology and scientific psychology, then the attitude to hold towards folk psychology is <em>elimination</em>.
</p>
</dd>
</dl>

<ul>
<li>
<p>Ramsey still agrees with Fodor in <em>eliminativism</em>.</p>
<ul>
<li>
Stich, in the 80s and 90s, was an eliminativist.
</li>
<li>
Especially, &quot;The Case Against Belief&quot; didn't think there'd be a transparent relationship.
</li>
<li>
<p>Egan thinks there <em>both</em> wrong, that is the eliminativists.</p>
<ul>
<li>
The paper on this is &quot;Folk Psychology and Cognitive Architechure&quot;
</li>
</ul></li>
</ul></li>
</ul>

<h3>
On Pylyshyn
</h3>

<ul>
<li><p>
He wants to justify and vindicate justification with regards to representation.
</p>

<ul>
<li>
Why content?
</li>
</ul></li>
<li><p>
The argument for a certain way of construing vehicles pervades the whole book.
</p>

<ul>
<li>
<em>We cannot eliminate appeal to content</em>.
</li>
</ul></li>
<li><p>
Stich argued that if we really are symbol systems, like Fodor thinks, then why do we need content?
</p>

<ul>
<li>
Pylyshyn is trying to justify the appeal to content.
</li>
</ul></li>
</ul>

<h4>
The First Puzzle
</h4>

<ul>
<li><p>
Brain states are not causally connected to ...
</p>

<ul>
<li>
&quot;I believe there's beer in the refrigerator.&quot;
</li>
<li><p>
There's not causal connection between the object and my brain states.
</p>

<ul>
<li>
There <em>needn't</em> be a causal connection.
</li>
</ul></li>
<li><p>
I'm thinking about Paris and that might cause me to book a vacation to Paris.
</p></li>
</ul></li>
<li><p>
The solution is how mental states fit into the physical world.
</p></li>
</ul>

<blockquote>
  <p>
Why does he think we need content if it is the physical stuff that is realized?
</p>
</blockquote>

<ul>
<li>
Pylyshyn uses the capturing language as a way of &quot;getting the generalization.&quot;
</li>
<li>
<p>The difference between us and watches.</p>
<ul>
<li>
We get convinience from &quot;thermometer gets the tempurature.&quot;
</li>
<li>
We really need do need generalizations to explain our behavior, it's only convinient for everything else.
</li>
</ul></li>
</ul>

<h3>
The Scheme
</h3>

<p>
<span class="math">\[ \lbrace I_1 ... I_n \rbrace \]</span> <span class="math">\[ f_I \]</span> <span class="math">\[ \lbrace S_1 ... S_n \rbrace \]</span> <span class="math">\[ f_R \]</span> <span class="math">\[ \lbrace P_1 ... P_n \rbrace \]</span>
</p>

<ul>
<li><p>
The three levels:
</p>

<ol>
<li>
Interpretations level
</li>
<li>
Interpretation function
</li>
<li>
Syntatitic level
</li>
<li><p>
Realization function
</p>

<ul>
<li>
Maps physical state to symbol (numeral) 2 or 3 ...
</li>
<li>
Nothing stops it from a bizzare representation.
</li>
<li>
Indepedant from meaning
</li>
</ul></li>
<li><p>
Physical states
</p>

<ul>
<li>
These have <em>many</em> causal functions.
</li>
<li>
These can bring about states, but those states can <em>bring about</em> cognitively important states.
</li>
</ul></li>
</ol></li>
<li><p>
Fodor think that we need an <abbr title="Language of thought">LOT</abbr>, so he wants to identify a level where structures that function as words and they have <em>constituency relationships</em>.
</p></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: If you want to be representational about this, you can say that the mapping is just useful to us ...
</p>
</blockquote>

<pre><code> 2   3     5
 ^   ^     ^
 |   |     |
 |   |     |
S1  S2 -&gt; S3
 ^   ^     ^
 |   |     |
 |   |     |
P1  P2 -&gt; P3

An "adding machine" on this model.
</code></pre>

<h3>
What's the role of content?
</h3>

<ul>
<li>
<p>If the <em>I</em>s and the <em>P</em>s are in a one-one relationship, then you get not explanatory leverage.</p>
<ul>
<li>
Why can't we just explain the systems behvior in terms of the causally efficacious states.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
What is implicit in this is not that there is a one-to-one function, but that there are arbitrarily many disjunctions
</p>
</blockquote>

<ul>
<li>
The rationality principle says that these transition states need to be <em>truth-preserving</em>.
</li>
<li><p>
There is an <em>implicit assumption</em> about explaining cognition that we are doing something inherently rational already.
</p></li>
<li><p>
Here's something that is paradigm of cognitive but not rational:
</p>

<ul>
<li>
If we can succesfully characterize someone in this way, if they have the <em>p</em> and the <em>p</em> to <em>q</em>
</li>
<li>
These models are used to describe data that <em>is</em>. &quot;Cognitive function.&quot;
</li>
</ul></li>
<li><p>
This kind of explantory projects is going to be applied to sub-cognitive functions and phenomena.
</p>

<ul>
<li>
Not just the personal level of behavior.
</li>
<li>
Open up the principle of rationality
</li>
</ul></li>
<li><p>
Fodor in the appendix appeals to theory or processing, these theories are commited to mental representations, so this provides support for the <abbr title="Representational theory of mind">RTM</abbr> and by commitment the <abbr title="Language of thought">LOT</abbr>.
</p>

<ul>
<li>
A lot of these examples, the successes do not fit the model the content of attitudes, beliefs, desires.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: Is the argument that you want to get rid of the interpretation level but that's silly because that's where we began to explain.
</p>
</blockquote>

<ul>
<li>
<p>No, that's Prof. Egan's argument.</p>
<ul>
<li>
The argument <em>here</em> is that there's going to be a series of unprincipled generalizations without principles of rationality.
</li>
<li>
<p>These are understood as generalizations.</p>
<ul>
<li>
Unless we can understand these truth-preserving.
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Ben</strong>: Couldn't you weaken it, the more plausible claim is if we lose the generalizations, we lose these great explanations.
</p>
</blockquote>

<ul>
<li>
<p>Derivability isn't a basic notion.</p>
<ul>
<li>
We can reconstruct it to be truth preserving.
</li>
</ul></li>
</ul>

<dl>
<dt>
Derivability
</dt>
<dd>
<p>
What goes on in the middle level, the higher-level causation.
</p>
</dd>
</dl>

<h4>
Egan's View
</h4>

<ul>
<li><p>
Unless what the system is doing is <em>recognizabily cognitive</em>, this isn't worth doing.
</p>

<ul>
<li><p>
It can add, it can compute, it can speak, it can see, it can recover the 3D structure of the scene.
</p>

<ul>
<li>
To do this, we need content.
</li>
</ul></li>
<li><p>
Unless you can attribute content, you <em>can</em> talk about causal stories, you cannot explain Mary's <em>behavior</em>.
</p>

<ul>
<li>
The folk psychological explaination makes it rational.
</li>
</ul></li>
</ul></li>
<li><p>
This point will come up in Ramsey's <abbr title="Input-output">IO</abbr> representations.
</p>

<ul>
<li>
Unless we can construe the inputs and outputs of the system, as cognitive function, then the project <em>doesn't make any sense.</em>
</li>
<li>
We're trying to explain a cognitive competence. We at least need to interpret the inputs and outputs of the system as contentful.
</li>
</ul></li>
</ul>

<h4>
On the third argument, pg. 27
</h4>

<ul>
<li>
My brain states are not causally connected to Paris, or walking, or mountains.
</li>
<li>
The relationship must be in terms in content.
</li>
</ul>

<h3>
On Kirsch
</h3>

<ul>
<li><p>
The main point is that intuitions about explicitness and implicitness are very broken, but that does not stop philosophers and others from using these notions fairly heavily.
</p>

<ul>
<li>
Our concept on explicit representation rests pretty heavily on the printed word.
</li>
<li>
<p>Construing explicit representation on the printed word, litterally sentences in the head, assumes that explicit representation can be understood in purely <em>functional</em> terms.</p>
<ul>
<li>
The big point he wants to get across is that this has to come across in <em>procedural</em> terms.
</li>
</ul></li>
</ul></li>
<li><p>
It's often assumed that we know what <em>explicit</em> is, but not what <em>implicit</em> is, and so implicit is defined in terms of not-explicit.
</p></li>
<li>
He distinguishes between <em>exploiting</em> regularities in the environment and <em>representing</em> regularities in the environment.
</li>
</ul>

<h3>
On Marr
</h3>

<blockquote>
  <p>
So why are we reading Marr? A lot of the work he's done has been falsified.
</p>
</blockquote>

<ul>
<li>
Not interested in the details, interested in the methodology
</li>
</ul>

<h4>
Marr's Methodology
</h4>

<ul>
<li><p>
He thought that an account of a cognitive system has three different levels.
</p>

<ul>
<li>
<p>Those are:</p>
<ol>
<li>
<strong>Theory of computation</strong>: Specification of the function computed. That is, the <em>what</em> that the system is doing. This is the level at which the competence of the system is, knowledge, processing details.
</li>
<li>
<strong>Representation &amp; Algorithm</strong>: This level specific the alogirthm which computes the function speficied in the theory of computation (level 1). And it specifies structures over which the alogirthm which it is defined. <em>Ex</em>. A function can be added, the algorithm can be Roman or Arabic.
</li>
<li>
<strong>Neural interpretation</strong>: This is, in some sense, the <em>how</em>.
</li>
</ol></li>
</ul></li>
<li><p>
The big picture:
</p>

<ol>
<li>
Gray level array
</li>
<li>
Primal sketch
</li>
<li><p>
Has multiple:
</p>

<ul>
<li>
<abbr title="Structure for motion">SFM</abbr>
</li>
<li>
Stereoscopic
</li>
<li>
Texture
</li>
<li>
Shading
</li>
</ul></li>
<li><p>
<span class="math">\(2 \frac{1}{2}\)</span>D
</p></li>
<li>
3D
</li>
</ol></li>
<li><p>
<strong>Next time</strong>: <abbr title="Structure for motion">SFM</abbr>
</p></li>
</ul>

<h2>
February 18th, 2014 <small>Reading</small>
</h2>

<h3>
Ramsey, <em>Representation Reconsidered</em>
</h3>

<h4>
Section 1.2 <small>The job description challenge</small>
</h4>

<ul>
<li><p>
Operating on the <strong>assumption</strong>: By reflecting a bit on ordinary notions of representation, we could gain a better understanding of what it is that scientists are referring to when they claim the brain uses such states.
</p>

<ul>
<li>
<p><strong>Worry</strong>: Why need to look at commonsense notions of representation at all.</p>
<ul>
<li>
A theoretical notion is introduced in scientific theories in such a manner that give the posit its specific explantory role.
</li>
<li>
For example, position genens involes a specification of the diferent relations and causal roles that <em>we think</em> genes perform.
</li>
<li>
With this is mind, we go a look for the physical thing to fit the bill.
</li>
</ul></li>
</ul></li>
<li><p>
For representation, things are not so simple.
</p>

<ul>
<li>
We already have a notion of representation that is &quot;home&quot; in non-scientifc contexts, and this contrains what representation can qualify as representational.
</li>
<li>
<strong>Want</strong>: These notions of representation to provide a specification of the essential features of representation.
</li>
<li>
<strong>Want</strong>: A <em>job description</em> for representation like genes or protons have.
</li>
</ul></li>
<li><p>
<strong>Problem</strong>: Our commonsense notions do not do this.
</p>

<ul>
<li>
These notions have core features and offer job descriptions representational states.
</li>
<li><p>
<strong>Problem <em>clearer</em></strong>: Stems from sort of features and roles that are associated with these notions.
</p>

<ul>
<li><p>
The relevant roles include things like:
</p>

<ol>
<li>
Informing
</li>
<li>
Denoting
</li>
<li>
Standing for something else
</li>
</ol></li>
<li><p>
How are these suppose to be cashed out naturalistically.
</p></li>
</ul>

<blockquote>
  <p>
Many scientific theories of the mind attempt to explain cognition in neurological or computaional terms. But our ordinary undersntadning of representation involves features and roles that can't be translated into such terms in any obvious way.
</p>
</blockquote></li>
</ul></li>
<li><p>
Consider our ordinary notions of mental representation.
</p>

<ul>
<li>
<p><strong>Claim</strong>: Common sense understnading of beliefs, desires, and other folk-representational states assigns them some sort of &quot;underived or intrinsic intentionalality.&quot;</p>
<ul>
<li>
Intentionality clearly isn't basic, functionally or causally.
</li>
<li>
So when we look at a system for these states, we don't really know what we're looking for.
</li>
</ul></li>
</ul></li>
<li><p>
A similar <strong>problem</strong> arises with regard to the commonsense understanding of non-mental representation.
</p>

<ul>
<li><p>
Every day examples of non-mental represenatations, like:
</p>

<ul>
<li>
Road signs
</li>
<li>
Pieces of written text,
</li>
<li>
Warning signs, so on
</li>
</ul></li>
<li><p>
These all invole an <em>agents</em> who use the representations to stand for something else.
</p></li>
<li>
The folk notions of representations, therefore, will not suffice if transplanted directly into cognitive science.
</li>
</ul></li>
</ul>

<dl>
<dt>
Job description challenge
</dt>
<dd>
<p>
There needs to be some unique role or set of causal relations that warrants our saying some structure or states serves a representational function.
</p>
</dd>
</dl>

<blockquote>
  <p>
What might a successful job description for cognitive representation look like?
</p>
</blockquote>

<ul>
<li><p>
<strong>Depends on</strong>: What notion of representation we use.
</p>

<ul>
<li>
Reductive theories cannot use representation as an explantory primitive.
</li>
<li>
If we understand processes as representational, then we need an account of representation in computational, mechanical, or causal physical terms.
</li>
<li>
<strong>Conclusion</strong>: Positing inner representations needs to include some sort of story about how the structure or state in question actually plays a representational role.
</li>
</ul></li>
<li><p>
<strong>Analogy</strong>: Someone offers an account of some organic process, and the account needs a structure called a pump.
</p>

<ul>
<li>
The advocate of this account would need to offer an explnanation of how this thing actually serves as a pump, as opposed to say a sponge.
</li>
<li>
And this explnanation will be a <em>functional</em> one.
</li>
</ul></li>
<li><p>
<strong>Conclusion</strong>: Cognitive researchers who invoke a notion of inner representation in their reductive accounts must provide some explanation of how the thing they positi serves as a representation.
</p>

<ul>
<li>
This is the <strong>problem of pan-representationalism</strong>.
</li>
<li>
<strong>Goal</strong>: Argue that this hypothetical situation is in fact the actual sitation in a wide range of newer cognitive theories.
</li>
</ul></li>
<li><p>
The meeting of the job description is not the same as providing a naturalistic account of content.
</p>

<ul>
<li>
This would present the set of physical or causal conditions that ground the content of the representation.
</li>
<li><p>
Different version of naturalistic content:
</p>

<ul>
<li>
Nomic dependency relations
</li>
<li>
Causal links to the world
</li>
<li>
Evolutionary function
</li>
<li>
Conceptual roles within a given system
</li>
</ul></li>
<li><p>
These explain a certain <em>relation</em>, but not that relations <em>function</em> as a representation in a physical system.
</p></li>
</ul></li>
<li><p>
<strong>Analogy</strong>: A compass.
</p>

<ul>
<li>
One type of question: How the compass actually functions as a representational device, how it informs a cognitive agent.
</li>
<li>
Another type of question: What conditions are responsible for the representational content of the compass.
</li>
</ul></li>
<li><p>
Why this is pressing:
</p>

<blockquote>
  <p>
&quot;Look, I'm not completely sure how state X comes to the content it has, but in my explanation of cogintive, there needs to be a state X that serves as a representation in the following way.&quot;
</p>
</blockquote></li>
<li><p>
The <em>crux</em> of the job description challenge
</p>

<ul>
<li>
If conditions on representation are too strong, something &quot;left explained&quot;
</li>
<li>
If condition on represestation are too week, it's ambiguous and ubiquitous.
</li>
</ul></li>
<li><p>
Millikan attempts to provide something that applies to mental and non-mental representation.
</p>

<ul>
<li><p>
To function as a representation is to be &quot;consumed&quot; by an interpreter that treats the state in question as indicating some condition.
</p>

<ul>
<li>
Fine for non-mental.
</li>
<li>
Unclear for mental, consumption <em>inside</em> a system.
</li>
</ul></li>
<li><p>
This account is &quot;under-reduced.&quot;
</p></li>
</ul></li>
<li><p>
Dretske, as it will be shown, over-redcuded.
</p>

<ul>
<li>
Intutively, his conditions have nothing to do with representation at all.
</li>
</ul></li>
<li><p>
It may seem impossible to meet the job description challenge, but no, certain theories in the CCTC paradigm have done it.
</p>

<ul>
<li>
<strong>Claim</strong>:the difference between the two corresponds to the division classical theories of computation and non-classical accounts of cognition.
</li>
</ul></li>
</ul>

<dl>
<dt>
Job description challenge, <em>improved</em>
</dt>
<dd>
<p>
There are two parts:
</p>

<ol>
<li>
Is it possible to describe physical or computational processes in representational terms?
</li>
<li>
Is it absolutely necessary to describe physical or computational processes in representational terms?
</li>
</ol>
</dd>
</dl>

<blockquote>
  <p>
Even a rock can be described as acting on the belief that it needs to sit very still.
</p>
</blockquote>

<ul>
<li>
The reason this fails is because the notion of representation is too weak.
</li>
<li>
In the &quot;other way&quot;, we can described biological systems <em>in terms of</em> molecules and toms, it is nevery <em>necessary</em> to invoke representational langauge in theory characterization of a representational syste,
</li>
</ul>

<dl>
<dt>
Job Description Challenge, <em>improved ... again </em>
</dt>
<dd>
<p>
Three questions:
</p>

<ol>
<li>
Is there some explanotory benefit in described an internal elements of a physical or computational process in representational terms?
</li>
<li>
Is there an element of a proposed process or architecture that is functioning as a representation in a sufficienitly robust or recognizable manner, and if so, how does it doe this?
</li>
<li>
Given that theory X invokes internal representations in its account of process Y, are the internal states playing this sort of role, and if so, how?
</li>
</ol>
</dd>
</dl>

<ul>
<li>
<p>Unfortunately, &quot;explanatory benefit&quot; and &quot;sufficiently robust&quot; are not as robust as we would like.</p>
<ul>
<li>
Requires a &quot;judgement call.&quot;
</li>
</ul></li>
</ul>

<h4>
Section 3.1 <small>IO-representation</small>
</h4>

<ul>
<li><p>
<strong>Previously</strong>: Marr's model of cognitive science involved three level of
</p>

<ul>
<li><p>
The top level involved the specification of a function that defines the sort of cognitive capacity we want explained.
</p>

<blockquote>
  <p>
Consider again a simple operation like multiplication. Although we say various mechanical devices do multiplication, the transformation of numbers into products is something that, strictly speaking, no physical system could ever do. Numbers and products are abstract entities, and physical systems can't perform operations on abstract entities. So at the algorithmic level we positi symbolic representations of numbers as inputs to the system and symbolic representations of products as outputs. We re-define the task of multiplication as the task of transforming numerals of one sort into numerals of another sort.
</p>
</blockquote></li>
<li><p>
The input to a cognitive system: say, &quot;faces&quot;.
</p>

<ul>
<li>
The output of a cognitive system: say, &quot;That's so-and-so.&quot;
</li>
</ul></li>
</ul></li>
<li><p>
How do <abbr title="Input-output">IO</abbr>-representation concerns meet the job description challenge? <strong>Two responses</strong>:
</p>

<ol>
<li><p>
<strong>Avoid</strong> the question altogether. &quot;Outside the domain of cogntive theorizing.&quot;<sup id="fnref:10"><a href="#fn:10" class="footnote-ref">5</a></sup>
</p>

<ul>
<li>
We're actual in the business <em>of explaing the porcesses and operations that convert input representations into output representations</em><sup id="fnref:9"><a href="#fn:9" class="footnote-ref">6</a></sup>
</li>
</ul></li>
<li><p>
<strong>Say</strong>: Minds do certain things, and one of the main things they do is &quot;perform cogitive tasks properly described as types of representations.&quot;
</p>

<ul>
<li>
It is a <em>fact of nature</em>.
</li>
</ul></li>
</ol></li>
</ul>

<dl>
<dt>
Interior <abbr title="Input-output">IO</abbr> representations
</dt>
<dd>
<p>
Interior input–output representations are a sub-system’s own inputs and outputs that are internal to the larger super-system’s explanatory framework.
</p>
</dd>
</dl>

<h4>
Section 3.2 <small>S-representation</small>
</h4>

<h4>
Section 3.3 <small>Two objects and their replies</small>
</h4>

<h5>
Challenge 2: <abbr title="Input-output">IO</abbr>-representation and S-representation aren't sufficiently real
</h5>

<ul>
<li><p>
Made appeals to the <em>explantory benefit</em> and <em>explanatory pay-off</em> of <abbr title="Input-output">IO</abbr> and S notions of representation.
</p>

<ul>
<li>
And that <em>there is a</em> payoff.
</li>
<li>
Is it a <em>useful fiction</em>? Do it real?
</li>
</ul></li>
<li><p>
Dennet on the topics:
</p>

<ul>
<li><dl>
<dt>
Physical stance
</dt>
<dd>
<p>
Use an understanding of the physical inner workins of the system to explain and predict how it responds to different inputs.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Design stance
</dt>
<dd>
<p>
Predict behavior by using what we know about the osrt of tasks that the system was <em>designed</em> to perform.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Intetional stance
</dt>
<dd>
<p>
The intentional stance involves treating a system as a rational agent with beliefs, desires, and other folk-representationl states.
</p>
</dd>
</dl></li>
</ul>

<dl>
<dt>
True believer
</dt>
<dd>
<p>
Little more than being a system whose behavior can be succesfully explained and predicted through the ascription of beliefs and other propositional attitudes.
</p>
</dd>
</dl></li>
<li><p>
<strong>Challenge</strong>: If the CCTC is a theory that invokes <em>real</em> representations, then it needs objectively realy representations.
</p></li>
<li>
<p><strong>Response</strong>: They <em>are</em> real, and here's why.</p>
<ul>
<li>
<p>There's a sense in which most things are <em>observer-dependant</em>.</p>
<ul>
<li>
It's always possible to view a system as a series of interacting atoms.
</li>
<li>
<strong>If</strong> <abbr title="Input-output">IO</abbr> and S representations are unreal only in the sens which trees, minerals, hearts, mountains, and species are unreal, <strong>then</strong> a reality about representation should be able to live with <em>that</em> sort of &quot;anti-realism.&quot;
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Review of <abbr title="Representation Reconsidered">RR</abbr> <small>by Mark Sprevak</small>
</h3>

<ul>
<li><p>
<strong>Structure of Ramsey's <em>negative</em> argument</strong>:
</p>

<ol>
<li><p>
Argue that in order for something to be an <em>X</em>, it must satisfy a description <em>D</em>.
</p>

<ul>
<li>
This is the <em>job description</em> or <em>minimal conditions for representation</em>.
</li>
</ul></li>
<li><p>
Argue that to the best of our knowledge, nothing satisfies <em>D</em>.
</p>

<ul>
<li>
&quot;How do our best psychological theories use the notion of representation?&quot;
</li>
</ul></li>
<li><p>
Conclude that since nothing satifies <em>D</em>, there are no <em>X</em>s.
</p>

<ul>
<li>
Ramsey finds all theories unsatisfactory, there are no representations.
</li>
</ul></li>
</ol></li>
<li><p>
On step 1:
</p>

<ul>
<li><p>
For a state to be a representation:
</p>

<ol>
<li><dl>
<dt>
Non-derived intentionality
</dt>
<dd>
<p>
Be capable of having original intentional content.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Causality
</dt>
<dd>
<p>
Interact causally with other cognitive states.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Connection
</dt>
<dd>
<p>
<ol style="list-style-type: decimal">
<li>and (2) must be linked: The causal role that representation plus should be <em>determined by</em> it's intentional content.
</p>
</dd>
</dl></li>
</ol></li>
</ul></li>
<li><p>
On step 2:
</p>
</li>
</ol>
<ul>
<li><p>
The <strong>exception</strong>: <abbr title="Classicaly computational theory of cognition">CCTV</abbr>
</p>

<ul>
<li>
<p><abbr title="Classicaly computational theory of cognition">CCTV</abbr> is commited to representation in 2 ways:</p>
<ol>
<li>
<abbr title="Input-output">IO</abbr>-representation
</li>
<li>
S-representations
</li>
</ol></li>
</ul></li>
<li><p>
On <abbr title="Input-output">IO</abbr>-representation:
</p>

<ul>
<li>
Representations are needed as the gross-inputs and outputs of cognitive agents.
</li>
<li>
But also the steps of a computation need representations, so internal representations.
</li>
</ul></li>
<li><p>
On S-representation
</p>

<ul>
<li>
CCTC is commited to positive interinal representations. (I'm not sure why this or if this is bad?)
</li>
</ul></li>
</ul></li>
</ul>

<h2>
February 18th, 2014 <small>Seminar</small>
</h2>

<h3>
Structure for motion
</h3>

<ul>
<li><p>
The visual system is able to recover the 3D dimension of a scene with on 2D input.
</p>

<ul>
<li><p>
Some of the sources of information:
</p>

<ul>
<li>
<strong>Stereoptics</strong>: The visual system uses disparity to get a vantage point on a scene.
</li>
<li>
<strong>Motion</strong>: Relative motion, what the visual system gets is a series of images that are different in certain respects. A series of retinal images.
</li>
</ul></li>
<li><p>
One thing that Ullman and Hildreth point out is that there are an infinite number of 3D representations out of any 2D input.
</p>

<ul>
<li>
The visual system is aided by physical constraints and assumptions.
</li>
</ul></li>
</ul></li>
<li><p>
The relevant assumption for this mechanism: <em>rigitidy</em>.
</p></li>
</ul>

<dl>
<dt>
Rigidity
</dt>
<dd>
<p>
Objects in the environment when they're moving is that they are solids.
</p>

<blockquote>
  <p>
&quot;[T]hey assume that if it is possible to interpret the changing 2-D images as the projection of a rigid 3-D object in motion, then such an interpretation should be chosen.&quot;
</p>
</blockquote>
</dd>

<dt>
<abbr title="Structure for motion">SFM</abbr> Theorem
</dt>
<dd>
<p>
Three distinct views of 4 non-coplanar points is sufficient to determine a rigid configuration for the points.
</p>
</dd>
</dl>

<ul>
<li>
<p>If you see the object long enough and in good enough conditions, then when the <abbr title="Structure for motion">SFM</abbr> theorem is saying is that there is <em>only one</em> rigid configuration of points.</p>
<ul>
<li>
There's a lot more non-rigid configuration, but <em>assuming rigidity</em>, it is much easier to compute a representation. That is, there are less configurations.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: Why think that <em>points</em> are primitive, why not plains?<sup id="fnref:6"><a href="#fn:6" class="footnote-ref">7</a></sup>
</p>
</blockquote>

<ul>
<li>
An organism that <abbr title="Structure for motion">SFM</abbr> with minimum input will react to the world better than an organism that does not have <abbr title="Structure for motion">SFM</abbr>, rigidity, etc. More plausible for this reason?
</li>
<li><p>
If an object is getting bigger with regards to a visual field, what are the possible interpretations?
</p>

<ul>
<li>
It's getting <em>bigger</em>.
</li>
<li>
It's getting <em>closer</em>.
</li>
<li>
A mix of the two.
</li>
</ul></li>
<li><p>
This type of adaptation is only going to useful in an environment where most objects are rigid in translation<sup id="fnref:5"><a href="#fn:5" class="footnote-ref">8</a></sup>.
</p></li>
<li>
<p>The environmental assumptions are motivated by adaptation.</p>
<ol>
<li>
Powerful, rigidity forces a unique solution
</li>
<li>
True, doesn't make the organism &quot;screw up&quot;
</li>
<li>
Unspecific, true in most cases, general claim about the environment, will support a lot of counterfactuals.
</li>
</ol></li>
</ul>

<blockquote>
  <p>
What is the status of the rigidity assumption? Is it innate?
</p>
</blockquote>

<ul>
<li><p>
<em>Exploits</em> regularities but doesn't <em>represent</em> regularities.
</p>

<ul>
<li>
We can <em>count</em> it as <em>innate knowledge</em> <em>if we want</em>, but it isn't explicitly represented and almost certainly not <em>explicitly</em> represented.
</li>
</ul></li>
<li><p>
The environment can be exploited without be represented.
</p>

<ul>
<li>
<p>First step for the theorist is to look at the environment.</p>
<ul>
<li>
Spiky?
</li>
<li>
Squishy?
</li>
<li>
Solid?
</li>
</ul></li>
</ul></li>
<li><p>
Steps:
</p>

<ol>
<li>
Commonsense characterization of the problem, explandadum.
</li>
<li><p>
Put on lab coat and look at exactly what the competence is.
</p>

<ul>
<li>
Is this a competence for determining 3-D structure <em>simpliciter</em>?
</li>
<li>
No, 3-D <em>rigid</em> structure.
</li>
<li>
&quot;Looking for assumptions.&quot;
</li>
</ul></li>
<li><p>
Look for algorithms that explain the phenomena
</p></li>
</ol></li>
</ul>

<h3>
Ramsey
</h3>

<h4>
Chapter 1
</h4>

<blockquote>
  <p>
What is it for a state in the head to be a representation?
</p>
</blockquote>

<ul>
<li><p>
Couple of things to distinguish:
</p>

<ul>
<li><p>
As a question &quot;what it is for a thing in the head to have meaning or conent? What is it to have content? What determines contents of internal representations?&quot; <em>Problem of intentionality side</em>
</p>

<ul>
<li>
The answer for <em>external</em> representations is convention, we agree on meanings for items in the language.
</li>
<li>
This cannot be the answer for <em>internal</em> representation.
</li>
<li>
Cannot specificy little &quot;agents in the head.&quot; Because regress? No agents.
</li>
<li>
There's going to be a naturalistic account of how internal structures and states, naturalistic hope, relation between states and structures in the head and the objects in the world.
</li>
<li>
To say that it's naturalistic is to discount, say, semantic relations.
</li>
</ul></li>
<li><p>
Personal and sub-personal processes.
</p>

<ul>
<li>
Like a map, people use maps, we use them to navigate, etc.
</li>
<li>
This kind of answer isn't going to work for mental interpretations.
</li>
<li>
When theories posit these states or structures, it's only appropriate to think of these, they're not used by <em>people</em>, it's a category mistake to think of Prof. Egan as <em>using</em> the primal sketch to navigate the world, it's her <em>system</em> that uses the primal sketch.
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Ben</strong>: I don't see any <em>job</em> for it to do. What he said led me to believe that it not a necessary condition on representation that is has semantic content.
</p>
</blockquote>

<ul>
<li>
<p>This is a <em>possibility</em></p>
<ul>
<li>
A theory might posit a symbol structure
</li>
</ul></li>
</ul>

<blockquote>
  <p>
What Ramsey is thinking about is being a representation and not about what it is to have content.
</p>
</blockquote>

<ul>
<li>
<p>Remember the distinction between <em>personal</em> and <em>sub-personal</em> representation.</p>
<ul>
<li>
<p><strong>Question</strong>: What is it for a physical thing to represent another?</p>
<ul>
<li>
It should be used in a characteristcally representational way. <em>Useless</em>.
</li>
<li>
Content is in som way relevant to how it functions, or it's effects. So the representation is causally efficacious, causes effects in cognition, and the content is relevant to those effect <em>in some way</em>, I say in <em>in some way</em> because it isn't at all clear how
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Egan</strong>: Content is a way of summarizes what is <em>causally relevant</em> or the <em>causal</em> role of a belief<sup id="fnref:7"><a href="#fn:7" class="footnote-ref">9</a></sup>.
</p>
</blockquote>

<ul>
<li><p>
Dretske, on the other hand, thinks that content has to get its hands on the wheel.
</p>

<ul>
<li>
This can't be right, content is not driving the car.
</li>
</ul></li>
<li><p>
It cannot be a consequence of doing that job that too many thing count as as representational.
</p></li>
<li><p>
The theorizing we're doing do no presuppose propositional attitudes or representational capacities.
</p>

<ul>
<li>
A theory that is doing this may <em>under-explain</em>.
</li>
</ul></li>
<li><p>
If you describe cognition in terms of atoms, quarks, molecules, ...
</p>

<ul>
<li>
This is <em>over-explaining</em>.
</li>
<li>
<p>It's damning to say, &quot;Hey, where is the intentionality?&quot;</p>
<ul>
<li>
We have to be careful to not fall into this trap.
</li>
</ul></li>
</ul></li>
<li><p>
Avoid positing un-explained intelligence, but posit intelligence.
</p>

<ul>
<li>
How can intelligence arise, or emerge, from non-intelligent.
</li>
</ul></li>
<li><p>
It would illegimate to complain about they're being nothing conscious in an explanation of consciousness.
</p>

<ul>
<li><p>
You look at an organism in it's environment and that <em>is</em> to interpret what it's doing cognitively.
</p>

<ul>
<li>
It's computing some cognitive function.
</li>
</ul></li>
<li><p>
<strong>Egan</strong>: Using this is a really robust way, which systems are appropriate to explain psychological.
</p></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: Maybe it's true that we have evidence that something is cognitive if we attribute to a system doing a computation on representations. Is it intentional?
</p>
</blockquote>

<ul>
<li>
<p>At the moment, all (Egan) is doing is trying to put our cognitive attributions in context.</p>
<ul>
<li>
<em>When do we attribute cognition?</em>
</li>
</ul></li>
</ul>

<h4>
Chapter 3
</h4>

<ul>
<li>
To call mething cognitive is to call something an <abbr title="Input-output">IO</abbr> system
</li>
<li><p>
We go around attributing cognitive capacities to people, and for the animals, we do it on the basis of behavior.
</p>

<ul>
<li>
What we <em>attribute</em> is a <em>competence</em> and this goes <em>beyond the behavior</em>.
</li>
<li>
<em>How would the system behave if it was given different inputs?</em>
</li>
<li>
For Marr, the competence is at the top-level of the theory.
</li>
</ul></li>
<li><p>
Right now, this defintion of cognition is related to Haugeland's <abbr title="Intentional black box">IBB</abbr>.
</p>

<ul>
<li>
Only when we have behvaioral event that something is doing something cognitive.
</li>
</ul></li>
<li><p>
How do you say something plays chess?
</p>

<ul>
<li>
You treat it as a representing thing as representing chess moves, or something.
</li>
<li>
You have to treat the sub components, the subprocesses, these have to be operating on chess moves as well.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Ben</strong>: A requirement on a sub-representation is the same ... as a super- representation (or something). So is it syntatic and representational <em>all-the way down</em>?
</p>
</blockquote>

<ul>
<li><p>
In some of these decompostional reductions, whenever you're doing this, whether it be the big or small process, whenever you're invoking representations, you need to invoke contents.
</p>

<ul>
<li>
He's <em>not</em> saying that you always have to do it in this way.
</li>
<li>
Each, eventualy, these IBS <abbr title="Intentional black box">IBB</abbr> will &quot;bottom out&quot;
</li>
</ul></li>
<li><p>
As long as you're invoking representations, you have to go &quot;whole hog&quot; on it.
</p></li>
<li>
<em>Can't we treat them as uninterpreted symbols?</em>
</li>
</ul>

<blockquote>
  <p>
We could, of course, employ a syntactic type of task-decompositional explanation. We could track the causal roles of the syntactically individuated symbols, and thereby divide the internal processes into syntactic sub-processes. But we wouldn’t be able to make sense of these operations as computationally pertinent stages of the larger task being explained. <strong>Ramsey, pg. 76</strong>
</p>
</blockquote>

<ul>
<li><p>
He's <em>putting off</em> the big question: Is it <em>really</em> an adder?
</p>

<ul>
<li>
He falls back on the wholism response.
</li>
<li>
We're attributing the intentinos and representaitons together.
</li>
</ul></li>
<li><p>
Are intuitions important for cognitive theorizing?<sup id="fnref:8"><a href="#fn:8" class="footnote-ref">10</a></sup>
</p>

<ul>
<li>
In calling it <em>seeing</em>, we're pretty much committed to representation.
</li>
</ul></li>
<li><p>
There's certain structural similarities between entities on a map of NJ and NJ itsel
</p></li>
<li><p>
There's stccertain structural similarities between entities on a map of NJ and New JJ itself.
</p></li>
<li><p>
It's not an isomorphism, it's a morphism.
</p>

<ul>
<li>
What breaks the symmetry?
</li>
</ul></li>
<li><p>
Next time: Chapter 4, and the two objections
</p></li>
</ul>

<h2>
February 20th, 2014 <small>Office hours</small>
</h2>

<h3>
Questions
</h3>

<ol>
<li><p>
Levels of description
</p>

<table>
<thead>
<tr>
  <th>
Level of description
</th>
  <th>
Has representation?
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
...
</td>
  <td>
No (presumably)
</td>
</tr>
<tr>
  <td>
Atoms
</td>
  <td>
No
</td>
</tr>
<tr>
  <td>
...
</td>
  <td>
No
</td>
</tr>
<tr>
  <td>
Biology
</td>
  <td>
No (Maybe)
</td>
</tr>
<tr>
  <td>
Neuroscience
</td>
  <td>
???
</td>
</tr>
<tr>
  <td>
...
</td>
  <td></td>
</tr>
<tr>
  <td>
Cognition
</td>
  <td>
???
</td>
</tr>
<tr>
  <td>
Folk psychology
</td>
  <td>
Yes
</td>
</tr>
<tr>
  <td>
Utterances
</td>
  <td>
Yes
</td>
</tr>
</tbody>
</table>

<p>
At first, there's the physical states. It then moves to the biological states. And then the cognitive states.
</p>

<p>
Is this a good roadmap of the ontology? Of where we're trying to describe and ascribe representation? If not, what's better?
</p></li>
<li><p>
The debate
</p>

<ul>
<li><p>
Ramsey: <em>elimitavist about the mind on grounds of the best cognitive models not having it</em>
</p>

<ol>
<li>
<abbr title="Classicaly computational theory of cognition">CCTV</abbr> have representations, and their good ones (answer job description)
</li>
<li>
Neither <abbr title="Classicaly computational theory of cognition">CCTV</abbr> is true nor are representations possible?
</li>
<li>
Representation and content - isomorphism - capable of content
</li>
</ol></li>
<li><p>
Fodor:
</p>

<ol>
<li>
Realist about FP
</li>
<li>
FP <span class="math">\(\to\)</span> <abbr title="Language of thought">LOT</abbr> <span class="math">\(\to\)</span> <abbr title="Representational theory of mind">RTM</abbr>
</li>
<li>
<abbr title="Representational theory of mind">RTM</abbr> and <abbr title="Language of thought">LOT</abbr> have representations.
</li>
<li>
???
</li>
</ol></li>
</ul></li>
<li><p>
Circularity and striking parellelism
</p></li>
</ol>

<h2>
February 25th, 2013 <small>Reading</small>
</h2>

<h3>
Dretske, &quot;Misrepresentation&quot; (background)
</h3>

<ul>
<li><p>
<em>How do we manage to get things wrong?</em>
</p>

<ul>
<li>
<em>How is it possible for a physical system to <strong>misrepresent</strong> the state of their surroundings?</em>
</li>
</ul></li>
<li><p>
<strong>Assumption</strong>: Belief is a <em>non-derived</em> representational capacity the exercise of which <em>can</em> yield a representation.
</p></li>
<li>
<p>The capacity to misrepresent is only a part of the general problem of meaning and intentionality.</p>
<ul>
<li>
Once you have meaning, &quot;lavish&quot; it on to your descriptions.
</li>
<li>
Once you have intentionality, you can &quot;adopt the intentional stance.&quot;
</li>
</ul></li>
</ul>

<h4>
Natural Signs
</h4>

<ul>
<li><p>
Naturally occuring signs mean something, without assistance from us.
</p>

<blockquote>
  <p>
Water does not flow uphill; hence, a northerly flowing rivers means there is a downward gradient in that direction.
</p>
</blockquote>

<ul>
<li><p>
The power of these events or conditions to mean what they do is independant of the way we interpret them.
</p>

<ul>
<li>
Or whether we interpret them at all.
</li>
</ul></li>
<li><p>
There was meaning before intlligent organisms, capable of exploiting meaning.
</p></li>
<li>
If we are looking for meaning, misrpresentation is a promising place to begin.
</li>
</ul></li>
<li><p>
Natural signs are <em>indicators</em>, more or less <em>reliable</em>, and <em>what they mean</em> is what <em>they indicate to be so</em>.
</p>

<ul>
<li>
The power of a natural sign to mean something emerges from objective constraints, lawful relations, between the sign and the condition that constitutes its meaning.
</li>
<li>
It's usually <em>causal</em> or <em>lawful</em>.
</li>
<li>
It's <em>counterfactual supporting</em>.
</li>
</ul></li>
</ul>

<h3>
Ramsey, <em>Representation Reconsidered</em>, ch. 4
</h3>

<h4>
Introduction
</h4>

<ul>
<li><p>
Will <strong>argue</strong>: The notions of representation explored in the net two chapters do <em>not</em> mean the <em>job description challenge</em>.
</p>

<ul>
<li>
In fact, it also leads to <em>deep misconceptions</em>.
</li>
<li><p>
The <em>receptor notions</em> of representation.
</p>

<ul>
<li>
Alternatively, <em>detector</em>.
</li>
</ul></li>
<li><p>
Not a theoretically useful notion.
</p></li>
<li>
Will <strong>deny</strong>: Structures that do recepting are serving as representations.
</li>
</ul></li>
<li><p>
<strong>Structure</strong>
</p>

<ol>
<li>
Spell out the basic idea of receptor representation.
</li>
<li>
Will <strong>ask</strong> how well this notion fares with regard to the job description challenge.
</li>
<li>
Will <strong>show</strong>: Dretske's own account of representation overlaps a great deal with the receptor notion, yet is more sophiticated and robust.
</li>
<li>
Will <strong>argue</strong>: Neither representation nor misrepresneetation is without serioues flaw.
</li>
<li>
The receptor notion should be abandoned.
</li>
</ol></li>
</ul>

<h4>
The receptor notion
</h4>

<dl>
<dt>
Receptor notion
</dt>
<dd>
<p>
Because a given neural or computational structure is regularly and reliably activated by some distal condition, it should be regarded as having the role of representation (indicating, signaling) that condition.
</p>

<p>
Such structures are viewed as representations <em>because</em> of the way they are triggered to go into particular states by other conditions.
</p>
</dd>

<dd>
<p>
<em>Example</em>: Certain neurons should be viewed as &quot;detectors&quot; <em>precisely because</em> they reliably respond to certain stimuli.
</p>

<p>
Just a though a frog has &quot;convexity detectors&quot; in their brain.
</p>
</dd>

<dd>
<blockquote>
  <p>
[I]f a cell is claimed to represent a face, then it is necessary to show that it fires at a certain rate nearly every time a face is present and only very rarely reaches that rate at other times.
</p>
</blockquote>
</dd>
</dl>

<ul>
<li><p>
The receptor notion of representation is found in connectionist models in the similarity of &quot;internal units&quot; to &quot;neural receptors&quot;
</p>

<ul>
<li>
It's suggested they provide a more representational role than computational symbols.
</li>
</ul></li>
<li><p>
Three examples of the receptor notion in simple organisms:
</p>

<ol>
<li>
Certain neurons in a bugs brain.
</li>
<li>
&quot;Face cells&quot; in monkeys
</li>
<li>
Magnetosomes &quot;compass-like&quot; represenetation which tell where to go.
</li>
</ol></li>
<li><p>
Whatever the state is that the so called receptor brings about, it is &quot;viewed as having the role of representing the external condition because of this causual or nomic dependency relation.&quot;
</p></li>
<li>
<p>Philosophers have developed a notion &quot;natural meaning&quot; or &quot;information content&quot; that supposedly results from the way a state reliably co-varies with some other state of affairs.</p>
<ul>
<li>
This makes people say &quot;<em>this</em> smoke <em>means</em> <em>that</em> fire.&quot;
</li>
</ul></li>
</ul>

<h4>
The receptor notion and the job description challenge
</h4>

<ul>
<li><p>
The reception notion faces a <em>prima facie</em> difficulty:
</p>

<ul>
<li><p>
<strong>Problem</strong>: The reception notion does not provide an account that reveals <em>why</em> a given state or structure should be seen as serving as a representation.
</p>

<ul>
<li>
<strong>Counterexample</strong>: There are several <em>non-representational</em> internal states that must in their proper functioning reliably covary with various states of affairs. For instnace, <em>our immunse system consistently reacts to infectious insults to our body</em>.
</li>
</ul></li>
<li><p>
<strong>Fix</strong>: While nomic dependency may be an important <em>element</em> of representation, it is not <em>sufficient</em> to be representation.
</p></li>
</ul></li>
<li><p>
This <strong>problem</strong> leads to a <strong>dilemma</strong>:
</p>

<ul>
<li>
If to serve as a representation <em>just is</em> to serve as a state that reliably responds, then you get overly reduced accounts of representation that lead to <em>pan-representationalism</em> because of the immune-system objection.
</li>
<li>
Or, if to serve as representation includes factors that go substantially beyond the mere fact that they reliably respond to specific stimuli, then we have no clear sense of how states reliably responding to certain stimuli are supposed to function as representations.
</li>
</ul></li>
<li><p>
This is closely related to the problem of <em>pansemanticism</em>, that is that &quot;meaning is just about <em>everywhere</em>&quot; and &quot;is a natural conclusion to draw from informational analyses of content.&quot;
</p></li>
</ul>

<h4>
Dretske to the rescue?
</h4>

<ul>
<li><p>
Dretske offers an ambitious account of mental represnetation that is designed to be naturalistic about content, and show how content can produce behavior.
</p>

<ul>
<li>
What's wrong with Dretske's account will be illuminating with regaurds to a fundamental problem for the receptor notion of representation <em>in general</em>.
</li>
</ul></li>
<li><p>
Central to the account is the notion of <strong>indication</strong>, a relation based on law-like dependency.
</p></li>
</ul>

<dl>
<dt>
Indication
</dt>
<dd>
<blockquote>
For condition <em>C</em> to indicate another condition <em>F</em>, <em>C</em> must stand in relation to <em>F</em> characterized by subjunctives like the following: If <em>F</em> had not occured, <em>C</em> would not have occured.
</dd>
</dl>

</blockquote>
<ul>
<li><p>
<strong>Already saw</strong>: that mere nomic dependency is insufficient to bestow full-blown representational status on cognitive structures.
</p>

<ul>
<li>
This is the <strong>disjunction problem</strong>, that concerns the difficulty in account for misrepresentation when a state's content is based upon the way it is triggered by distal conditions.
</li>
<li>
The <strong>response</strong> to this: &quot;the core of any thoery of representation must contains an explanation of how misrepresentation can occur.&quot;
</li>
</ul></li>
<li><p>
The <strong>response</strong> is to offer teleological components, placing tighter restrictions on the sort of causal relations that matter.
</p></li>
</ul>

<blockquote>
  <p>
<strong>Dretske</strong>: Internal indicators are mental representations when they are recruited as a cause of certain motor output <em>because of</em> their relevant nomic dependancies.
</p>
</blockquote>

<ul>
<li><p>
This does two things for Dretske:
</p>

<ol>
<li><p>
Enables him to handle the problem of misrepresentation
</p>

<ul>
<li>
Error is no possible because we can say that whenever an indicator is triggered by something other than what it was <em>supposed to</em> indicate.
</li>
</ul></li>
<li><p>
Gives him a way of showing how informational content can be explanatorily relevant.
</p>

<ul>
<li>
Structures are recruited as causes to motor output <em>because</em> they indicate certain conditions.
</li>
<li>
Being an indicator is a causally relevant feature of a strcture, so being a type of <em>meaning</em> is causally relevant.
</li>
</ul></li>
</ol></li>
</ul>

<blockquote>
  <p>
<strong>Dretske</strong>: To serve as a representation is necessarilly to:
</p>
  
<ol>
  <li>
Stand in some sort of nomic dependancy relation to some distal state of affairs
</li>
  <li>
Becoming incorporated into the processing <em>because</em> of this dependancy, thereby acquiring the function of indicating those states of affairs.
</li>
  </ol>
</blockquote>

<h5>
Does Dretske's account of representation function help?
</h5>

<ul>
<li><p>
<strong>Dretske is committed to</strong>: <em>If</em> neural structures are actually recruited as causes of bug-catching movements because they are reliably caused to fire by the presence of bugs, <em>then</em> it certainly seems tempting to assume that they are serves as &quot;bug representations.&quot;
</p>

<ul>
<li>
<strong>Question</strong>: Does this arragment suffice for representation?
</li>
</ul></li>
<li><p>
To answer, distinguish between:
</p>

<ol>
<li>
The puraly causal/physical or nomic dependencies that are thought to &quot;underlie&quot; the indication relation
</li>
<li>
The quasi-semantic, information relation often said to be &quot;carried by&quot; these dependancies.
</li>
</ol></li>
</ul>

<blockquote>
  <p>
If <em>A</em> carries information about <em>B</em>, (1) is this supposed to be distinct from <em>A</em>'s being nomically dependant on <em>B</em>? (2) Or are those claims indentical?
</p>
</blockquote>

<ul>
<li><p>
On (1), the infromation carried by <em>A</em> is somehow seperate and distinc from properities of <em>A</em>. It <em>A</em> carries information about <em>B</em>, it does so whether or not anyone exploits it.
</p>

<ul>
<li><p>
This is <strong>ambigous</strong>, refering to either:
</p>

<ul>
<li>
The non-semantic nomic depancy
</li>
<li>
Something more semantically charged, like information
</li>
</ul></li>
<li><p>
This is the <em>realist interpretation of information and indication</em>.
</p></li>
</ul></li>
<li><p>
On (2), it's unambigous. This is the <em>deflationary interpretation of information and indication</em>.
</p></li>
<li><p>
The <strong>central problem</strong>: Dretske's account of representation appears to assume that if a given structures is incorporated into a system's processing because it nomically depends on a certain state of affairs, it automatically follows that it being used to stand for that state of affairs.
</p>

<ul>
<li>
This <strong>unsupported</strong> and <strong>almost certainly false</strong>.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Counterexample to (1)</strong>: The firing pin in a gun similarly bridges a causal gap between the pulling of the trigger and the discharge of the round. It also serves to reliably mediate between two distinct states of affairs – to reliably go into a specific state when and only when a certain condition obtains. However, no one thinks the firing pin serves as some sort of representational device.
</p>
  
<p>
<strong>Counter example to (2)</strong>: For example, if A is always larger than B, then, in the deflationary sense we are now using the term, A carries information about the size of B; that is, the size of A could be used to tell someone something about the size of B. If A is heavier than B, or if A is always within a certain distance of B, then the weight or position of A can serve to inform a cognitive agent about the weight or position of B. In all of these cases, specific types of law-like relations between two objects (larger than, heavier than, close to, etc.) can and sometimes are exploited by cognitive systems like ourselves such that our knowledge of the status of one of the objects can generate knowledge of the status of the other one as well. When this happens, one of the objects is serving as a type of representational device.
</p>
</blockquote>

<ul>
<li>
<p><strong>Conclusion</strong></p>
<ul>
<li>
<p>If we equate <em>being an indicator</em> to <em>being a nomic dependant</em>, then Dretske cannot establish that a strcture is a representation by showing that it functions as an indicator because, <em>trivially</em>, functioning as an indicator just means functioning as a nomic dependant</p>
<ul>
<li>
<strong>And</strong>: There are all sorts of ways to function as a nomic dependant without being a representation.
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Further dimensions of the receptor notion
</h4>

<blockquote>
  <p>
<strong>Question</strong>: How do we demarcate between:
</p>
  
<ol>
  <li>
Cases where the nomic regularity is relevant to a non-representational function
</li>
  <li>
Cases wehere it makes sense to say that nomic dependency helps serve as a representation.
</li>
  </ol>
</blockquote>

<ul>
<li><p>
<strong>Contast</strong>:
</p>

<ol>
<li><p>
The firing pin in a gun
</p>

<ul>
<li>
There is no sense in which the information carried by the firing pin is exploited in its normal functioning.
</li>
</ul></li>
<li><p>
The mercury in a thermometer
</p>

<ul>
<li>
There is a <em>clear</em> sense in which it serves to inform people who want to learn about the temperature.
</li>
</ul></li>
</ol></li>
</ul>

<h4>
Does it really matter?
</h4>

<ul>
<li><p>
People often compain that Ramsey is just being stingy with the word &quot;representation.&quot;
</p>

<ul>
<li>
Why not treat receptor states as some low-level type of representation?
</li>
</ul></li>
<li><p>
Taken as a point about langauge and our ability to call things what we want, it's correct, but silly.
</p></li>
</ul>

<blockquote>
  <p>
<strong>Question</strong>: If we <em>are</em> so inclined, and nothing is really at stake, then why not go ahead and do so?
</p>
</blockquote>

<ul>
<li><p>
<strong>Answer</strong>: There's a fair bit at stake.
</p>

<ul>
<li>
&quot;Representation&quot; and &quot;information&quot; is increasingly just being use to mean &quot;reactive neuron&quot; or &quot;causal activity.&quot;
</li>
</ul></li>
<li><p>
Here he paints a picture about a real-world case where misconceptions about representation derailed progressed and blurred understanding in a study by Freeman and Skarda.
</p></li>
</ul>

<h4>
Summary
</h4>

<ul>
<li><p>
<strong>Argued</strong>: One of the most popular ways of thinking about representation in cognitive science is confused and should be discontinued.
</p>

<ul>
<li>
Receptor notions of representation have jobs that have little to do with representation.
</li>
</ul></li>
<li><p>
In an effort to supplment the failure, Ramsey brought in Dretske.
</p>

<ul>
<li>
Dretske focused on on an indication <em>functioning</em> as a representation.
</li>
<li>
<p>He suggest that receptor type states qualify as representation by virtue of the way in which they are incorporated into cognitive architecture.</p>
<ul>
<li>
<strong>But</strong> the functional role is a reliable causal mediator, and not a representation.
</li>
</ul></li>
</ul></li>
<li><p>
If the receptor notions of representation are in fact the right account of representation in connectionism, then <abbr title="Representational theory of mind">RTM</abbr> is simply false.
</p></li>
</ul>

<h3>
Commentary
</h3>

<p>
I want to be a realist about the external world and I have this premonition without proof that my inner representation of the external world reliably covaries with at least those states important for my continued success. If the way I characterize this inner representation is with &quot;indication&quot; and Ramsey's argument that indication isn't represnetation, then I reject indication as being able to tell the whole story about representations in the head.
</p>

<p>
A way that I think indication could be vindicated is by making it a part of inner representation instead of claiming that it's all there is. For instance, perhaps my vision is an indication of what's directly in front of my eyes, my ears indicate what sounds are in my immediate surroundings, etc, and these indications are amalgamated by my mind in higher-order mental processing, and it is from this and these indications that an internal representation emerges.
</p>

<h2>
February 25th, 2014 <small>Seminar</small>
</h2>

<h3>
S-Representation
</h3>

<ul>
<li><p>
Two key ideas:
</p>

<ol>
<li>
<strong>Structural similarity</strong> between the vehicles of representation and the things that they're about, the represented domain.
</li>
<li>
<p>The <strong>use</strong> of the representation.</p>
<ul>
<li>
Why is this important?
</li>
<li>
If we don't bring in use, what problem are we left with?
</li>
<li>
The <em>state</em> represents <em>the map</em>, the map is used in certain ways to represent the state.
</li>
<li>
What <em>kind</em> of use? <em>Surrogative reasoning</em>, it's use to <em>say things about</em>, <em>make claims about</em> the <em>domain</em>. Doormat not relevant cognitive use.
</li>
<li>
<strong>But</strong>, you <em>could</em> use the landscape of New Jersey to reason about the map. <strong>But</strong> it does break the symettry because you're using the one about the other. <em>Minor bullet-biting</em>.
</li>
</ul></li>
</ol></li>
<li><p>
The <strong>map example</strong>, paradigm of this representation
</p>

<ul>
<li><p>
A map of New Jersey represents some properties and relations, dots represent cities, some features of the dots represent relative size.
</p>

<ul>
<li>
There are other features of the map that don't have any function at all, like perhaps colors.
</li>
<li>
Colors might make it more salient to see the county divisions, etc.
</li>
<li>
It's not representing any &quot;shared feature&quot; of these counties.
</li>
<li>
Not <em>every</em> feature maps to a representation, morphism and isomorphism.
</li>
</ul></li>
<li><p>
Isomorphic
</p>

<ul>
<li>
1 to 1
</li>
<li>
Bijection
</li>
</ul></li>
<li><p>
Representatino on this model is <em>holistic</em>.
</p></li>
</ul></li>
</ul>

<p>
<a href="">./img/family-tree.png</a>
</p>

<ul>
<li>
<p>When Bob is using this, he has to use the connections correctly for surrogatively reasoning.</p>
<ul>
<li>
The map is the clear case, but the other examples include more and more levels of abstraction. The <em>interpretation</em> of the <em>representation</em> needs more levels of <em>abstract</em>.
</li>
</ul></li>
</ul>

<p>
<a href="">./img/broken-tree.png</a>
</p>

<blockquote>
  <p>
How is this going to work for <em>mental</em> representation? What would <em>justify</em> using &quot;AAAAAA&quot; to mean &quot;John&quot;?
</p>
</blockquote>

<ul>
<li>
<strong>Use</strong>.
</li>
</ul>

<h4>
Shank and Abelson
</h4>

<ul>
<li><p>
Given very general information, and a story.
</p>

<ul>
<li>
Like how to take a bus to the city, how to get food in a restaurant.
</li>
<li>
Lots of information about the situation.
</li>
<li>
Then, a story about John and Mary, and the program is asked about the specific story given the general facts.
</li>
</ul>

<blockquote>
  <p>
John and Mary when to some resaturant and they both ordered them medium-rare and got burnt burgers and so they stood up and walked out. <strong>To the machine</strong>: <em>Did they leave a tip?</em>
</p>
</blockquote></li>
</ul>

<blockquote>
  <p>
If the symbols in the Chinese Room constitute scripts of this sort, then they serve as representations, not because there is some conscious interpreter who understands them as such, or because the people who designed the system intended them that way, but because the overall system succeeds by exploiting the organizational symmetry that exists between its internal states and some chunk of the real world.
</p>
</blockquote>

<ul>
<li>
<p>&quot;All Ramsey needs&quot; is <em>some</em> morphism that is exploited<sup id="fnref:11"><a href="#fn:11" class="footnote-ref">11</a></sup>.</p>
<ul>
<li>
This isomorophism idea isn't <em>as general</em> as Ramsey would like to claim.
</li>
</ul></li>
</ul>

<h3>
Two Objections and Their Replies
</h3>

<h4>
Challenge 1: indeterminacy in S-representation (and <abbr title="Input-output">IO</abbr>-representation) content
</h4>

<ul>
<li><p>
Pragmatic considerations will come out to privelege on interpretation over the other.
</p>

<ul>
<li><p>
Ramsey's constraints are <em>doing a lot of work</em>,
</p>

<ol>
<li>
that is the explantory agenda and
</li>
<li>
how the system is embedded in the environment.
</li>
</ol></li>
<li><p>
Having two or three candidates left is <em>pretty good</em>.
</p></li>
</ul></li>
<li><p>
It's often said that &quot;task description&quot; are indeterminate.
</p>

<ul>
<li>
The first thing you have to do is consider how the system is working it it's environment.
</li>
<li>
<p>Then working out <em>what it succesful</em> at.</p>
<ul>
<li>
&quot;All it has to do is get the unique <em>rigid</em> interpretation of the sense data.&quot;
</li>
</ul></li>
</ul></li>
<li><p>
It's not clear how this account will get misrepresentation.
</p></li>
<li><p>
There's a distinction between theorists <em>attributing incorrectly</em> and agents <em>mispresenting radically</em>.
</p>

<ul>
<li><p>
Egan is a theorist of vision, she knows we're pretty good at getting a 3-D representation of a scene, and except for the dark, we're very good at succeeding at this task.
</p>

<ul>
<li>
This is compatible with making <em>some</em> mistakes, it's not easily compatible with making <em>systematic</em> mistakes.
</li>
<li>
But this is going to be a problem regardless of whether you assume success or not (disputed).
</li>
<li>
People <em>are</em> representing things when they make systematic errors, but how would this explantory account explain this possible phenomena?
</li>
</ul></li>
<li><p>
<em>The project</em> is to take a phenomena, human conciousness, and explain it. If there is systematic failure, then there is nothing to explain (???). It's not performing a task (???). It's not <em>succeeding</em> at a task.
</p>

<ul>
<li>
Success <em>is a motivation</em> for looking for representation.
</li>
<li>
<strong>Endgoal</strong>: Psychological capacties explanation.
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Challenge 2: <abbr title="Input-output">IO</abbr>-representation and S-representation aren’t sufficiently real
</h4>

<ul>
<li><p>
&quot;You're <em>just imposing</em> your way of understanding on the machine or person, it isn't that <em>it's there</em>, you're <em>assigning</em> it to physics.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref">12</a></sup>&quot;
</p>

<ul>
<li>
Grouping together physical states is interest-relative, but it is objective that <em>the states are there</em>.
</li>
</ul></li>
<li><p>
Identifying real properties of the device, just a real as genes, but if we don't care about diseases then we don't care about biochemical engineering (or similar statements about other fields).
</p>

<ul>
<li>
So atoms <em>are just not relevant</em> to what we're trying to explain.
</li>
<li>
Unless you're also instrumentalist about gene descriptions and genetics, then you should be a realist about representation in the way it's been built.
</li>
</ul></li>
</ul>

<h3>
Presentation <small>by Ben</small>
</h3>

<h4>
Background: Ramsey's Question
</h4>

<ul>
<li><p>
The original question: &quot;Is this thing reallyu functioning in a way that is reconizably representation in nature?&quot;
</p>

<ul>
<li>
Ramsey asks if various theoretical posits called &quot;representations&quot; accords with out <em>pre-theoretical</em> concept of a representation.
</li>
<li><p>
The <strong>worry</strong>: Science might start out with commonsense notions (mass, energy, representation), but science quickly leaves common sense behind.
</p>

<ul>
<li>
Compare Ramsey's question to the following: &quot;Is the physicist's energy really functioning in a recognizably energy-like in nature?&quot;
</li>
<li>
&quot;Matter, motion, energy, work, liquid, and other common sense notions ... are abandoned as naturalistic inquiry proceeds; a physicist asking whether a pile of sand is a solid, liquid, or gas ... spends no time asking how the terms are used in ordinary discourse, and would not expect the answer to the latter question to have anything to do with natural kinds, if these are kinds in nature&quot; (Noam Chomsky, <em>New Horizons in the Study of Language and Mind</em>, pg. 21).
</li>
<li>
So if our goal is to understand the mind, why worry about Ramsey's question?
</li>
</ul></li>
<li><p>
A slightly modified question: How do mental representation compare to paradigm representations such as sentences and maps.
</p>

<ul>
<li>
Answer this question might help us to better understand the nature of the mental representation posited by our theories.
</li>
<li>
Ramsey can be read as answering this question.
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Ramsey on Dretske's receptor representations
</h4>

<ul>
<li>
Ramsey argues that neither Dretske's account of <em>misrepresentation</em> nor his account of representational <em>function</em> gives us reason to believe that receptor &quot;representations&quot; are representations in any interesting sense.
</li>
<li><p>
Misrepresentation
</p>

<ul>
<li>
The <strong>problem</strong>: When a cell in the frog's eye respond to a flying BB, why is it misrepresenting that there is fly rather than accuratly representing that there is <em>either</em> a fly <em>or</em> a BB?
</li>
<li>
Dretske's <strong>answer</strong>: The <em>function</em> of the cell is to activate in the presence of flies. So the cell (mis)represents that there is a fly.
</li>
<li>
Ramsey's <strong>response</strong>: Dretske's answer only work if we already know that the cell in question representation <em>something or other</em> and we're just trying to figure out <em>what</em> it represents.
</li>
</ul></li>
<li><p>
Representational function
</p>

<ul>
<li>
Roughly, according to Dretske a representation is a structure
</li>
</ul></li>
</ul>

<h2>
February 27th, 2014 <small>Office hourse</small>
</h2>

<ol>
<li>
Why is it that with regards to content, that it is implicit that if you can get some sort of notational &quot;picking out&quot; of the unique object in the world you get &quot;objective/naturalistic/determinant&quot; content? Specifically, with regards to representation, why is it important that we &quot;pick out&quot; the unique interpretation of a representation? I'm reminded of Frege trying to uniquely pick out zero for his foundation of algebra. More pointedly: Why is content determinancy content important? We seem to have some algorithm which is more or less succesful.
</li>
<li>
Indication <em>must</em> be <em>some</em> part of representation. For instance, my vision is undoutedly <em>indicates</em> to me what is directly in from of me. It's what happens after or with the indication, however. So: <em>How does inditication fit in with a full story about cognition?</em>
</li>
<li>
On the scripts and Shank and Abelson. The discussion during seminar made it seem like people were searching between a morphism between &quot;the code&quot; and human norms about restaurants. But I think that this is confusing for a number of reasons. First, in naturalistic terms, these human norms are &quot;located&quot; in our shared, repeated experience over time, but more crucially they're not a physical or fixed object that we can point to. Second, the morphism we're really looking for is with &quot;the code&quot; and whatever the scientists decided were the present norms, or present norms sufficiently in-stone to merit inclusion in their script, perhaps their in the form of sentences like &quot;first appetizers, second entree, third dessert, etc.&quot; Third, there is a level of abstraction between those sorts of sentences and the running machine, namely that just like &quot;4&quot; is translate to &quot;100&quot;, &quot;first appetizers&quot; will have some bit-code representation. The relevant morphism, I think, is between the machine's code and the written down norms, the written down norms and the &quot;actual norms.&quot;
</li>
</ol>

<h2>
March 4th, 2014 <small>Reading</small>
</h2>

<h3>
Brooks, &quot;Intelligence without Representation&quot;
</h3>

<h4>
Introduction
</h4>

<ul>
<li>
Artificial intellgience started as a field whose <strong>goal</strong> was to replicate human-level intelligence <em>in a machine</em>.
</li>
<li>
Early <strong>hopes</strong> fell as the magnitiude of the <strong>problem</strong> came to bear.
</li>
<li><p>
No one tries to replicate the full gamut of human intelligence anymore.
</p>

<ul>
<li><p>
&quot;Specialized subproblems.&quot;
</p>

<ul>
<li>
Representing knowledge
</li>
<li>
Natural language processing
</li>
<li>
Vision
</li>
<li>
Truth maintenance
</li>
<li>
Plan verification
</li>
</ul></li>
<li><p>
The <strong>hope</strong> is that these systems &quot;all fall in to place&quot; to see a truly intelligent system <strong>emerge</strong>.
</p></li>
</ul></li>
<li><p>
Brooks <strong>believes</strong> that human-level intelligence is too complex and little understood to be correctly decomposed into subpieces <em>at the moment</em>.
</p>

<ul>
<li>
<strong>And</strong> even if we knew the subpieces we still wouldn't know the <em>interface</em> between them.
</li>
<li>
<strong>And</strong> we will never understand how to decompose human intelligence until we practice on lesser intelligences.
</li>
</ul></li>
<li><p>
In this paper, Brookes <strong>argues</strong> for a different approach to creating <abbr title="Artificial intelligence">AI</abbr>:
</p>

<ul>
<li><p>
We must incrementally build up capabilities of intelligent systems, having <em>complete systems</em> at each step of the way.
</p>

<ul>
<li>
<em>Thus</em> automatically ensure that the pieces and their interfaces are <em>valid</em>.
</li>
</ul></li>
<li><p>
At each step we should build <em>complete</em> intelligent system that we let loose <em>in the real world</em> with <em>real sensing</em> and <em>real action</em>.
</p>

<ul>
<li>
Anything less provides a candidate with which we can delude ourselves.
</li>
</ul></li>
</ul></li>
<li><p>
Using this approach, they have come to an unexpected <strong>conclusion</strong> (C) and a radical <strong>hypothesis</strong> (H):
</p>

<blockquote>
  <p>
<strong>(C)</strong>: When we examine simple level intelligence we find that <em>explicit representations</em> and models of the world <em>simply get in the way</em>. It turns out to be better to use the world <em>as its own model</em>.
</p>
  
<p>
<strong>(H)</strong>: <em>Representation</em> is the <em>wrong unit</em> of abstraction in building the <em>bulkiest parts</em> of intelligent systems.
</p>
</blockquote></li>
<li><p>
Representatino has been the <strong>central ussue</strong> in <abbr title="Artificial intelligence">AI</abbr> for the last 15 years only because it has provided an <em>interface</em> between otherwise isolated modules and conference papers.
</p></li>
</ul>

<h4>
The evolutino of intelligence
</h4>

<ul>
<li><p>
We already have the proof of intelligent beings: <em>human beings</em>.
</p>

<ul>
<li>
Some animals are intelligent to some degree.
</li>
<li>
It's taken 4.6 billion year history.
</li>
</ul></li>
<li><p>
It in <em>instructive</em> to <em>reflect</em> on the way in which earth-based biological evoltuion spent its time.
</p>

<table>
<thead>
<tr>
  <th>
Timeline
</th>
  <th>
Evolutionary event
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
3.5 <abbr title="Billion years ago">BYA</abbr>
</td>
  <td>
Single-cell entities arose out of primordial soup.
</td>
</tr>
<tr>
  <td>
2.5 <abbr title="Billion years ago">BYA</abbr>
</td>
  <td>
First photosynthetic cells appeared.
</td>
</tr>
<tr>
  <td>
550 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
The first fish a vertabrates.
</td>
</tr>
<tr>
  <td>
450 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Insects
</td>
</tr>
<tr>
  <td>
370 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Reptiles
</td>
</tr>
<tr>
  <td>
330 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Dinosaurs
</td>
</tr>
<tr>
  <td>
250 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Mammals
</td>
</tr>
<tr>
  <td>
120 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Primates
</td>
</tr>
<tr>
  <td>
18 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Great apes
</td>
</tr>
<tr>
  <td>
2.5 <abbr title="Million years ago">MYA</abbr>
</td>
  <td>
Humans in current form
</td>
</tr>
<tr>
  <td>
10,000 years
</td>
  <td>
Agricultures
</td>
</tr>
<tr>
  <td>
5000 years
</td>
  <td>
Writing
</td>
</tr>
<tr>
  <td>
100 years
</td>
  <td>
&quot;Expert&quot; knowledge
</td>
</tr>
</tbody>
</table></li>
<li><p>
This <strong>suggests</strong> that problem solving behavior, language, expert knowledge, application, and reason are all pretty simple once the essence of <strong>being</strong> and <strong>reacting</strong> are available.
</p>

<ul>
<li>
This part of intelligence took evolution much longer, so it is much harder.
</li>
</ul></li>
<li><p>
Brooks <strong>believes</strong> that mobility, acute vision, and the ability to carry out survival related tasks in a dynamic environemtn provide a <em>necessary basis</em> for the <em>development of true intelligence</em>.
</p></li>
</ul>

<h5>
A story
</h5>

<ul>
<li><p>
Suppose it is the 1890s, where artificial flight is a glamor topic in science.
</p>

<ul>
<li>
A few artificial flight researches are magically transported to the 1980s for a medium duration flight on a Boeing 747.
</li>
</ul></li>
<li><p>
Returning to the 1980s, they feel vigorated, knowing that flight is possible on a grand scale.
</p>

<ul>
<li>
<p>They immediately set to work on duplicate what they have seen.</p>
<ul>
<li>
Pitched seats
</li>
<li>
Double-pane windows
</li>
<li>
&quot;Plastics&quot;
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Abstraction as a dengerous weapon
</h4>

<ul>
<li><p>
<abbr title="Artificial intelligence">AI</abbr> researchers are fond of pointing out that <abbr title="Artificial intelligence">AI</abbr> is denied its rightful successes.
</p>

<ul>
<li>
<p>If nobody has any good idea of how to solve a particular problem, it becomes known as an <abbr title="Artificial intelligence">AI</abbr> problem.</p>
<ul>
<li>
When an algorithm developed by <abbr title="Artificial intelligence">AI</abbr> researches succesfully tackles the problem, <abbr title="Artificial intelligence">AI</abbr> detractors claim that since it was solvable by algorithm, it isn't an <abbr title="Artificial intelligence">AI</abbr> problem.
</li>
</ul></li>
</ul></li>
<li><p>
Brooks <strong>claims</strong> that <abbr title="Artificial intelligence">AI</abbr> researchers are guilt of the same self-deception.
</p>

<ul>
<li><p>
They partition the problems into two parts:
</p>

<ul>
<li>
The <abbr title="Artificial intelligence">AI</abbr> problems, which they solve.
</li>
<li>
The non-<abbr title="Artificial intelligence">AI</abbr> problems, which they don't.
</li>
</ul></li>
<li><p>
Typically, <abbr title="Artificial intelligence">AI</abbr> &quot;succeeds&quot; by labelling the parts of problem they solve as &quot;<abbr title="Artificial intelligence">AI</abbr> problems.&quot;
</p></li>
<li>
&quot;Abstraction&quot; is used to discount problems of perception and motor skills.
</li>
</ul></li>
<li><p>
Early work on <abbr title="Artificial intelligence">AI</abbr> concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems.
</p>

<ul>
<li>
In each case, the semantics of the domains were fairly simple.
</li>
</ul></li>
<li><p>
<strong>Thus</strong>, because we perform all the abstractions for our programs, <abbr title="Artificial intelligence">AI</abbr> work is still being done in block world.
</p>

<ul>
<li>
The blocks have slightly different shapes and colors.
</li>
</ul></li>
<li><p>
<em>It could be</em> <strong>argued</strong> that performing perceptual abstraction is <em>merely</em> the normal reductionist use of abstraction common in all good science. Two <strong>objections</strong>:
</p>

<ol>
<li><p>
Each animal species will have different <em>Merkwelt</em>.
</p>

<ul>
<li>
The human-assumed <em>Merkwelt</em> may not be valid.
</li>
</ul>

<dl>
<dt>
Merkwelt
</dt>
<dd>
The merkwelt is a concept in robotics, ethology and biology that describes a creature or android's capacity to view things, manipulate information and synthesize to make meaning out of the universe. In biology, for example, a shark's merkwelt for instance is dominated by smell due to its enlarged olfactory lobes whilst a bat's is dominated by its hearing, especially at ultrasonic frequencies.
</dd>
</dl></li>
<li><p>
It is by no means clear that such a <em>Merkwelt</em> is anything like what we actually use internally.
</p></li>
</ol></li>
</ul>

<h4>
Incremental intelligence
</h4>

<ul>
<li>
<p>Requirements on Creatures:</p>
<ul>
<li>
Must <em>cope appropriately</em> to a <em>dynamic environment</em>.
</li>
<li>
Must <em>be robust</em> (minor change in the world doesn't lead to total collapse).
</li>
<li>
Must <em>maintain multiple goals</em>.
</li>
<li>
Must <em>do something</em>, purpose in being.
</li>
</ul></li>
</ul>

<h5>
Decomposition by function
</h5>

<ul>
<li>
Hardly anyone has ever connected a vision system to an intelligent central system.
</li>
<li>
One needs a long chain of modules to connect perception to action.
</li>
</ul>

<h5>
Decomposition by activity
</h5>

<ul>
<li>
<p>Makes no distinction between peripheral and central systems.</p>
<ul>
<li>
The fundamental slicing is in the &quot;orthogonal direction&quot; dividing it into <em>activity producing subsystems</em>.
</li>
</ul></li>
</ul>

<h4>
The methodology, in practice
</h4>

<ul>
<li>
In order to build systems based on an activity decomposition, we must folow a careful methodology.
</li>
</ul>

<h5>
Methodological maxim
</h5>

<ul>
<li><p>
<strong>First</strong>, test the Creatures in the real world.
</p>

<ul>
<li>
It is very easy to accidentally build a submodile of the system which happens to rely on some of those simplified properties.
</li>
</ul></li>
<li><p>
<strong>Second</strong>, the system must interact with the real world over extended period.
</p></li>
</ul>

<h5>
An instatiation of the methodology
</h5>

<ul>
<li><p>
Layers
</p>

<ol>
<li><p>
Avoid hitting objections
</p>

<ul>
<li>
Sonar
</li>
<li>
Collide
</li>
<li>
Feelforce
</li>
<li>
Runaway
</li>
<li>
Turn
</li>
<li>
Forward
</li>
</ul></li>
<li><p>
Wander
</p></li>
<li>
<p>Try to explore, distant places</p>
<ul>
<li>
Whenlook
</li>
<li>
Pathplan
</li>
<li>
Integrate
</li>
</ul></li>
</ol></li>
</ul>

<h4>
What this is not
</h4>

<ol>
<li>
Connectionism
</li>
<li>
Neural networks
</li>
<li>
Production rules
</li>
<li>
Blackboard
</li>
<li>
German philosophy
</li>
</ol>

<h4>
Limits to growth
</h4>

<ul>
<li><p>
These machines operate completely autonomously in complex dynamic environments at the flick of their on switches, and continue until the batteries are drained.
</p>

<ul>
<li>
We <strong>believe</strong> they operate at alevel closer to simple insect level intelligence than to bacteria level intelligence.
</li>
</ul></li>
<li><p>
Serious <strong>questions</strong>
</p>

<ol>
<li>
How many layers can be built in the subsumption architecture before the interactions between layers become too complex to continue?
</li>
<li>
How complex can the behaviors be that are developed without the aid of central representation?
</li>
<li>
Can higher-level functions such as learning occur in the fixed topology networks of simple FSM.
</li>
</ol></li>
</ul>

<!-- ### Haugeland, "Mind Embodied and Embedded" -->

<!-- ### Morgan, "Information Processing in Connectionist Systems" -->

<h2>
March 4th, 2014 <small>Seminar</small>
</h2>

<h3>
Wrapping up R-Representation
</h3>

<ul>
<li><p>
These are some examples of receptors that he mentions
</p>

<ul>
<li><p>
Structures in the frog's visual systems
</p>

<ul>
<li>
Bugs
</li>
<li>
Flies
</li>
<li>
Food
</li>
<li>
BBs?
</li>
</ul></li>
<li><p>
These are nomically dependant on flies.
</p>

<ul>
<li>
When they're &quot;triggered&quot;, it <em>causes</em> the flies tounge to pop out.
</li>
</ul></li>
<li><p>
Assuming they are representation, a good candidate for the content of the representation is <em>fly</em>.
</p>

<ul>
<li>
But any small dark and moving spot will cause this same effect.
</li>
</ul></li>
<li><p>
Also, edge-detectors in the visual system.
</p></li>
<li><p>
Magnetosomes in the bacteria represent maybe:
</p>

<ul>
<li>
Magentic north
</li>
<li>
Direction of oxygen-free water
</li>
<li>
Direction of the nearest magent
</li>
</ul></li>
<li><p>
It's not clear what these represent because of content-indeterminancy.
</p>

<ul>
<li>
Usually theorists of this kind want to privilege a special kind of content.
</li>
<li>
What's really important is that <em>food gets into the frog's stomach</em>.
</li>
<li>
Maybe it's a fly, but it doesn't <em>really</em> matter so long as it's nutritious.
</li>
</ul></li>
<li><p>
<em>What's the biological function?</em>
</p>

<ul>
<li>
This doesn't seem to cut any more finely.
</li>
</ul></li>
</ul></li>
<li><p>
Suppose that the idea is you get food, there are many paths to getting food.
</p>

<ul>
<li>
People talking about getting the trout shodows, which, low and behold, you get the trout.
</li>
<li>
<em>You get the food by representing BBs</em>.
</li>
</ul></li>
<li><p>
There are all these issues about content.
</p>

<ul>
<li>
Are these even representatinos at all?
</li>
</ul></li>
<li><p>
Dretske is trying to narrow down the content,
</p></li>
</ul>

<h3>
On Ben's presentation
</h3>

<ul>
<li><p>
The idea is our receptor representation and structural representation aren't very different, if one is a representation then why can't the other?
</p>

<ul>
<li><p>
S-representation is representation because:
</p>

<ol>
<li>
Structural similarity, &quot;isomorphism&quot;
</li>
<li>
Use in <em>doing something cognitive</em>
</li>
</ol></li>
<li><p>
What are R-representations?
</p>

<ul>
<li>
Ramsey points out that these are <em>mere relays</em> and they're serving in the system <em>only as relays</em>, nomic dependancy.
</li>
</ul></li>
</ul></li>
<li><p>
There's no reason for these structures in the frog's brain or the magentosome's magnets, in these theories, these are playing an important role in <em>doing something cognitive</em>, something interpretation, it's &quot;use that picks up the slack.&quot;
</p>

<ul>
<li>
<p>Is this difference on the basis of content, is this sufficient to make on representatino and the other <em>not</em>. Couple of things to notice:</p>
<ol>
<li>
<p>Are these really so different?</p>
<ul>
<li>
Structural similarity vs. nomic dependancy.
</li>
<li>
Take an internal map. Doesn't it <em>have to be some sort of nomic dependancy?</em> Various uses by the intelligent agent. <em>It can't be an accident that some structure is structually similar to the thing it represents.</em> That &quot;causal embedding&quot; is going to favor one interpretation over another.
</li>
<li>
If you think about edge detectors, these are not attributed <em>singularly</em>, they're in systems. Typically, they'll be <em>systems</em> of representatinos underly <em>systems</em> of (something).
</li>
</ul></li>
</ol></li>
</ul></li>
<li><p>
If you remember Ramsey's argument, a lot of his examples are mechanical causal relays.
</p>

<ul>
<li><p>
If we <em>do</em> focus on mental representation cases, if we say that the receptor plays a role in a representation cognitive system, then it's playing a representation causal role.
</p>

<ul>
<li>
Ramsey's account of mental represnetation, to do something cognitive <em>just is</em> to interpret its inputs and outs as mental representations.
</li>
</ul></li>
<li><p>
Notice that this move <em>takes</em> <abbr title="Input-output">IO</abbr>-representation as <em>basic</em>.
</p>

<ul>
<li>
Then internal states that play a nomic dependancy role play a representational role.
</li>
<li>
Nothing in the stomach or liver is doing something <em>recognizably cognitive</em>. This just <em>isn't</em> to treat as a cognitive system.
</li>
</ul></li>
<li><p>
This would be one way to go, piggyback R-representation on <abbr title="Input-output">IO</abbr>-representation.
</p></li>
</ul></li>
</ul>

<blockquote>
  <p>
What would differentiate a cognitive system from a non-cognitive system, perhaps some criteria? A liver can be treated as having <abbr title="Input-output">IO</abbr>, etc.
</p>
</blockquote>

<ul>
<li>
If the thing is an adder, then it <em>has to be</em> that it's inputs are addends and outputs are sums.
</li>
</ul>

<blockquote>
  <p>
Liver couldn't be interpreted as cognitive, because it doesn't have structure or use.
</p>
</blockquote>

<ul>
<li>
There's <em>very little conceptual space</em> as saying it is doing something cognitive and that's its inputs and outputs are interprebable.
</li>
</ul>

<blockquote>
  <p>
If you watch a bumblebee, you'll see it going about it's business, flowers, but the outputs are movements - how are these outputs cognitive? It seems like one explanation, maybe the best, is that it's computing 3-D space, but when you look at what's going on is that it's learning, avoid predators, just to characterizing it's behavior as cogntive might not be to interpret it.
</p>
</blockquote>

<ul>
<li>
When I say I'm characterzing it as doing something cognitive, I'm talking precisely, it's able to interpret speech, interpret 3-D scene, division, add.
</li>
</ul>

<blockquote>
  <p>
If we say that the bee can perceive, would you say that this getting the inputs of representatinos and outputs of represnetation?
</p>
</blockquote>

<ul>
<li>
<p>If you characterize exactly what the system is, you'll get representation that just are inputs and outputs of representations that are interpreble.</p>
<ul>
<li>
<p>You see some system doing something fairly succesfully, the first job of the theorist is to characterize the competence.</p>
<ul>
<li>
You have to see under what condition <em>it fails</em>. What <em>exactly</em> is the function it is computing.
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
How is this different from Dennet's pragmatism? What's the idea that for any system we can characterize it as intentional or we don't and whether we do or don't is just about it being pragmatic to predict behavior. When you say, &quot;The difference between the liver and the honeybee is that it isn't helpful to call it cognitive because there's no structure we look at it as an adder.&quot; Do you think there are objective facts about the capabilities of a cognitive (or non-cognitive) system. A few weeks ago we have a conversation where you (Egan) if you gerrymander things you can get something that looks like an adder.
</p>
</blockquote>

<ul>
<li>
We attribute capacties to systems
</li>
<li>
Rocks and walls do not have the necessary structure to represente addition, <em>unless</em> you add indices for time.
</li>
<li>
Another way, go to the quantum level, there's lots of structure there.
</li>
<li>
How to understand the realization function, the way to think about that is specifying to causal
</li>
<li>
To push this a bit further, remember that a gene is certain physical structure in the body and what's crucial is that it <em>causes</em> certain phenotypical effects, they are responsible for producing the realization's function.
</li>
</ul>

<blockquote>
  <p>
So you diverge from Dennet in that you don't think that Dennet has some underlying ... Two position:
</p>
  
<ol>
  <li>
<p>There are objective facts about a systeming <em>doing something cogntive</em>?</p>
<ul>
  <li>
Pretty much any system can be contrived to have systems which are doing something cognitive.
</li>
  </ul></li>
  </ol>
</blockquote>

<ul>
<li>
You slipped in pragmatic, there are the phenomena we choose to explain, it seems to be doing something cogntive, I'm trying to interpret it, Searle's given me a big book on how to do this.
</li>
<li>
<p>It's important for us to develop a theory about what's cognitive, that's pragmatic.</p>
<ul>
<li>
But what isn't is what a system needs to have the relevant features.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
Facts are relevant once we fix interests.
</p>
</blockquote>

<ul>
<li>
<p>Dennet doesn't think this. The Dennet of the 70s is to systematically predict a system's behavior on the basis of intetion.</p>
<ul>
<li>
<p>The Dennet of the 80s and 90s has patterns in behavior, not patterns in the phenonmena.</p>
<ul>
<li>
He thinks that beliefs and desires are abstracta.
</li>
<li>
He denies the instrumentalist label, he rejects it.
</li>
<li>
This is what the computational theory is c
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
The facts are fixed once you have interests, and the relevant facts are causal structure.
</p>
</blockquote>

<ul>
<li>
Physics describes the fundamental way of things, but it cannot distinguish between those things that <em>can think and *cannot thing</em>.
</li>
</ul>

<h3>
Niko's presentation
</h3>

<h4>
The Polemical Prologue
</h4>

<ul>
<li><p>
The general point is that Brooks believes that something has gone very wrong in the research of <abbr title="Artificial intelligence">AI</abbr>.
</p>

<ul>
<li>
It might have had more success if it didn't do this.
</li>
<li>
He's discovered a better way.
</li>
</ul></li>
<li><p>
In making in his case, he does a lot of things, he:
</p>

<ul>
<li>
Describes his robot
</li>
<li>
German words
</li>
<li>
Ant parable
</li>
</ul></li>
</ul>

<h5>
A Dubious Parable
</h5>

<ul>
<li>
There are these artificial flight researches from the 1890s and their transported to the 1980s in a commercil airliner for the duration of a flight.
</li>
<li><p>
Upon returning to the 1980s, they focus their efforts on replicatin the seats and windows they observed on the airliner.
</p>

<ul>
<li>
Because the task of replicating the airliner seems so overwhelming, they become specialists in different areas.
</li>
</ul></li>
<li><p>
The groups of researchers do not communcat well.
</p>

<ul>
<li>
They give up the study of aerodynamics entirely. Their propject is doomed to fail - they will never replicate the airliner.
</li>
</ul></li>
</ul>

<h5>
A Potted History
</h5>

<ul>
<li><p>
We have this <em>proof of concept</em>, namely ourselves.
</p></li>
<li><p>
But this kind of specialiation, because it requires abstraction, is a form of &quot;self-delusion&quot;
</p>

<ul>
<li>
<abbr title="Artificial intelligence">AI</abbr> researchers design program swhich can only operate on highly abstracted input represnetations.
</li>
<li>
This approach ignores the really difficult problem of perfomring the abstractions in the first place.
</li>
<li>
Brooks illustrates the failre of this kind of approach with the story of work on the &quot;blocks world&quot; in the 1960s and 1970s.
</li>
<li>
This block world made it much easier for the machines to move around in this world, these robots would not function in real-life situations.
</li>
</ul></li>
<li><p>
There is, moreover, an in-principle objection to abstraction: we have no reason to assume that the abstraction which seem most natural to us are anything like abstractions twhcih will be useful for a robotic system with very different sensory modaility; indeed, we have little reason to assume that our own cognition employs just the abstraction which seem most natural to us.
</p></li>
</ul>

<blockquote>
  <p>
What's the relatinship between theorizing and abstraction?
</p>
</blockquote>

<ul>
<li>
<p>The way we come to abstractions we use is in designing these modules, he thinks basically &quot;What data is salient to us&quot; is a raw visual data stream, we think, &quot;Oh, it's salient that there's a chair and it's folded.&quot;</p>
<ul>
<li>
That this is introspectivesly relevant is important.
</li>
</ul></li>
</ul>

<h5>
An Evolutionary Argument
</h5>

<ul>
<li>
<strong>Premise</strong>: Human-level intelligences have existed for a comparatively small proposition of the history of life, whereas simpler intelligences have not.
</li>
<li>
<strong>Conclusion</strong>: &quot;This suggests that problem solving behavior, lagnauge, expert knowloedge and application, and reason, are all pretty simple once the essence of being and reacting are available.&quot;
</li>
</ul>

<blockquote>
  <p>
This is supposed to be relevant because designing low-level intelligence is important for investigating high-level systems.
</p>
</blockquote>

<h5>
A Diagnoises
</h5>

<ul>
<li>
<p>The fundamental problem, Brooks suggests, is the approach to <abbr title="Artificial intelligence">AI</abbr> research by decomposition by function.</p>
<ul>
<li>
This apprach is characterized by the &quot;traditional notion of ... a central system, with perceptual modules as inputs and action modules as outputs. The perceptual modules deliver a symbolic description of the world and the action modules take a symbolic descrption of deisred acition and make sure they happen in the world. The central system is then a symbolic information processor.&quot;
</li>
</ul></li>
</ul>

<blockquote>
  <p>
What's a decomposition <em>supposed to do</em>? This is supposed to be a better way to do.
</p>
</blockquote>

<ul>
<li><p>
<strong>Ben</strong>: For him, isn't he just trying to build intelligent creatures in building creatures that go around the office.
</p>

<ul>
<li>
His <em>goal</em> in finding a good decomposition, he's not trying to illuminate, he's trying to build a system that is intelligent.
</li>
</ul></li>
<li><p>
Ths model allows groups working of differnt problems to make differnt assumption about the &quot;shape of the symbolic interfaces&quot;, which in teun makes those inferfaces &quot;subject to intellectual abuse&quot;. Brooks think this apporach is unlikely to result in the development of integrated intelligent systems.
</p></li>
</ul>

<h5>
Questions
</h5>

<ol>
<li><p>
Does Brook's history of <abbr title="Artificial intelligence">AI</abbr> research point on <em>in-principle</em> problem with the decomposition by function approach? If not, does it support his case? Is his parable, which seems almost fatalistic, entirely fair?
</p>

<blockquote>
  <p>
How is it not going to scale up? It's not going to scale up to the messiness of the dynamic, changing world. This is the problem with traiditional <abbr title="Artificial intelligence">AI</abbr>. So it's a disanalogy with the <abbr title="Artificial intelligence">AI</abbr> researchers.
</p>
  
<p>
Maybe these guys a decomposing the functional subsystems badly.
</p>
</blockquote></li>
<li><p>
What of the alleged in-principle objection to the decomposition by function approach? Whose cares whether there might be some intelligent systems which couldn't function using the abstractions most natural to us? Might it still be the case fhat some intelligent system can, and that the decomposition by function apporach could allows us to deisgn them? Why think there might be some tight connection between senory modalities useful abstraction to the first place?
</p></li>
<li>
Is the evolutionary argument any good? <strong>Compare</strong>: Space flight has existed for compataively small proportion of the history of <em>homo sapiens</em>, whereas agriculture has not. This suggest spaceflight is easy once you have agriculture.
</li>
<li>
What does any of this have to do with prescnec or absence of representation?
</li>
</ol>

<h4>
Meet the Robots
</h4>

<h5>
Decomposition by Activity
</h5>

<blockquote>
  <p>
This is what Brooks proposes. The basic idea of this is unit of investigation is a capacity of the cognitive system. He calls the capacities <em>layers</em>, which would allow the creature to avoid objects in its vicinity.
</p>
  
<p>
Instead of having a representations sent to a <abbr title="Central processing unit">CPU</abbr>, you have a parallel architecture where there are no central computational system, just different capacities which interact with one another to some extent.
</p>
  
<p>
This hope is that you can implement one layer and continue to implement layers so it can do more and more complicated behavior without a <abbr title="Central processing unit">CPU</abbr>.
</p>
</blockquote>

<ul>
<li><p>
To replace the problmatic apporach of decomposition by function, Brooks suggests an approach by decompistion by activity.
</p>

<ul>
<li>
The idea is that each activity of a system should be controlled by an indepedant subsystem.
</li>
<li>
One such layer might cause a robot to avoid colliding with objects.
</li>
</ul></li>
<li><p>
Once one system is in place, further, indepedant systems are added which interact in such a way as to produce desirable behavior.
</p>

<ul>
<li>
The systems are not entirely autonomous, since theye have to coordinate the robot's overall behavior, but there is no central control system.
</li>
</ul></li>
</ul>

<h5>
No Representations
</h5>

<ul>
<li><p>
Brooks claims that a robot designed according to his principles does not need to employ any representations
</p>

<blockquote>
  <p>
We do claim however, that there need be no explicit representation of either the world or the intentions of the system to generate intelligent behaviors for a Creature. Without such explicit representations, and when viewed locally, the interactions may indeed seem chaotic and without purpose.
</p>
</blockquote></li>
</ul>

<h5>
Description of a Model Robot
</h5>

<blockquote>
  <p>
Each layer is made of a bunch of FSMs which each operate semi-indepedantly and has access to a central processor, and it's arranged of three layers.
</p>
  
<ol>
  <li><p>
The simplest
</p>
  
<ul>
  <li>
Avoids colliding with things
</li>
  </ul></li>
  <li><p>
Also looks at the data (actually not sure), but its purpose is to make the robot wander aound.
</p></li>
  <li>
Seems to always override the second layer, and this layer takes in the data from the sonar and finds places that are distant from the robot and finds the path from the place, but when there is an object, the third layer recalculates a new path, the capacity of the third level is to <em>explore</em>.
</li>
  </ol>
</blockquote>

<ul>
<li><p>
The different layers of the model robot are composed of networks of FSM.
</p>

<ul>
<li>
Each such FSM has timers and access to computational machines &quot;which can compute things such a vector sums.&quot;
</li>
</ul></li>
<li><p>
The model robot has a ring of sonars around its circuference which serve as it sensors.
</p>

<ul>
<li><p>
The lowest layer of the robot makes it avoid hitting objects.
</p>

<ul>
<li>
It does this by running its sonors &quot;and every second emitting an instantaneous map with the readings converted to polar coordinates.&quot;
</li>
</ul></li>
<li><p>
The map gets passed on to some other machines, which determine whether an object is too close, and if so, cause the robot to move in the opposite direction
</p></li>
</ul></li>
<li><p>
The second layer of the robot makes it wander waround by generating random headings and interfacing with the first layer.
</p>

<ul>
<li>
The third layer makes the robot try to reach distant places by choosing locations and continually calculating paths for the robot it is forced to avoid an obstacle.
</li>
</ul></li>
</ul>

<h5>
Questions
</h5>

<ul>
<li>
Is Brook's claim that his robots do not employ any explicit representations plausible? If he is correct, how can he help himself to descriptions of the layers as computing vecto sums, emitting a map witht e readins converted to polar coordinates, etc.?
</li>
</ul>

<h3>
Response
</h3>

<ul>
<li>
There as to be enviromental richness.
</li>
<li><p>
There were two problem that stymied research:
</p>

<ol>
<li><dl>
<dt>
Knowledge-representation problem
</dt>
<dd>
If you're building a system for the real-world, it's got to <em>know a lot</em>.
</dd>
</dl>

<ul>
<li>
Last time we talked about about Shank scripts, the device has a paradigm of what it's like to go to a restaurant, and it's supposed to answer questions about people in this scenario, and there was a lot that it couldn't answer, like people sit on chairs and not on the floor, that wasn't in it's knowledge base, and of course we know that.
</li>
<li>
The problem of structure or representing in some way all of the knowledge that the system has to have to get around in the world and the knowledge has to be represented in a way the system can get it.
</li>
<li>
This is related to the problem of designing a machine that can pass the Turing-test, and any bit of esoteric information can become relevant by a shift in context, and this machine has to have this information accesible.
</li>
<li>
If you got an artificial, well-defined, then scaling up to the real world requires a lot of information.
</li>
</ul></li>
<li><dl>
<dt>
Frame or relevance problem
</dt>
<dd>
<p>
The world changes from one minute to the next, but it really only has to notice the relvant changes, and the system would be overwhelmed if it took account for <em>every possible change</em>.
</p>

<p>
For instance, the lighting in the room is affecting our shadows and were affecting the ambient tempurature, and the system has to represent the relevance from irrelevant changes. We do this.
</p>

<p>
Combinatorial explosion.
</p>
</dd>
</dl></li>
</ol></li>
<li><p>
Brooks has some suggestions for building things in the real world. He does say that his robots have no representations at all. What seems to be the case is theres no explicit, general, context-free representation, that's the kind of representation that it <em>doesn't have</em>.
</p>

<ul>
<li>
<p>What are perceived, but doesn't stress to much in this paper, because each layer is intended to have it's own proprietary sensory mechanisms, these are tied to the goals of the system, what's perceived by each layer are &quot;oppurtunities for action.&quot;</p>
<ul>
<li>
The analogy here, the father here of what the system perceives things which <em>invite doing certain things</em>, this idea comes from JJ Gibson's Affordances.
</li>
<li>
He thought what the visual system detects are complex properties which are salient to speicific kinds of organisms.
</li>
<li>
When you perceive a knife, you see something which can cut. Food, we see it as something which is <em>ripe for eating</em>.
</li>
<li>
Gibson thought that these were the objects for perception.
</li>
<li>
This idea is going to be pretty influential in embodied cognition, people talking about representations that are action-oriented, not a passive conditino of the world but as serving some sort of goal.
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
To be clear, we have this perceptual system and one reason it makes the disctinctions it does make is that it partitions the world in such a way that is useful, or, that we perceive things as purposeful, which is <em>adding content to the perception</em>. It's like sayin &quot;Why do we lump objects in the ways we do?&quot;
</p>
</blockquote>

<ul>
<li><p>
Gibson thought that properties like affording cutting, like the knife, is that <em>what we see</em> is an object for cutting, and this structures the light in certain ways. We <em>literraly perceive</em> them, it isn't <em>categorizing</em>.
</p>

<ul>
<li>
Food, for humans, there are higher-order invariance in the light, and that food strctures the light in a certain way that our visual system was built to detect.
</li>
</ul></li>
<li><p>
The idea is then that this is how these layers &quot;cut things differently&quot; and it's partly perceptual, partly cognitive.
</p>

<ul>
<li>
What it's perceiving affordances in the environment, oppurtunities to do something in the world.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
How does this play out in terms in human being using their fingers? Does this come up in Herbert, what in the machine which he's talking about has both the perception and the cognition?
</p>
</blockquote>

<ul>
<li>
The lowest level
</li>
</ul>

<blockquote>
  <p>
They have sensors which are &quot;relvant for action&quot;, this seems like too much of a disconnect, the way we're talking about things being intermingled.
</p>
</blockquote>

<ul>
<li>
They're sensing properties which are then processing in the FSMs, and this is directing behavior by the things detecting things by those sensors.
</li>
</ul>

<blockquote>
  <p>
How tight and how exactly?
</p>
</blockquote>

<ul>
<li>
All I can do is repeat.
</li>
<li><p>
If you think about what is being perceived by these various sensors are properties of this thing that is relevant to the action by the layer that this action is going to produce.
</p>

<ul>
<li>
The bottom layer sensors are not going to sense the coke cans, because that's not relevant to the activity of avoiding obstacles.
</li>
<li>
Each sense organ at a particular layer is going to detect things which are relevant to <em>that layer</em>.
</li>
</ul></li>
<li><p>
The <strong>take-away</strong> message is that what is perceived is not bare properties of things but stuff that is instricinally linked to oppurtunties for behavior.
</p></li>
<li>
Using the world as its own model (third point, unexpected conclusion)
</li>
</ul>

<h2>
March 12th, 2014 <small>Reading</small>
</h2>

<blockquote>
  <p>
Notions of components, systems, and interfaces as a starting framework to discuss. These definitions specifies a decomposition:
</p>
  
<ol>
  <li>
What are the components?
</li>
  <li>
What are the interfaces?
</li>
  <li>
What are the differences between everyone's conceptions.
</li>
  </ol>
</blockquote>

<h3>
Clark and Chalmers, &quot;The Extended Mind&quot;;
</h3>

<h4>
Introduction
</h4>

<blockquote>
  <p>
Where does the mind stop and rest of the world begin?
</p>
</blockquote>

<ul>
<li><p>
There are two replies
</p>

<ul>
<li>
The &quot;skin and skull.&quot;
</li>
<li>
It &quot;just ain't in the head.&quot;
</li>
</ul></li>
<li><p>
Clark and Chalmers propose a very different sort of externalism, <em>active externalism</em>
</p>

<ul>
<li>
It's &quot;based on the active role of the environment in driving cognitive processes.&quot;
</li>
</ul></li>
</ul>

<h4>
Extended Cognitive
</h4>

<ul>
<li><p>
Consider three cases of human problem solving:
</p>

<ol>
<li>
A person mentally rotates a two-dimensional shapes and is asked question concerning fit.
</li>
<li>
A person can mentally rotate the shape <em>or</em> press the rotate button.
</li>
<li>
A person can mentally rotate the shape or let their cognitive implant do it.
</li>
</ol></li>
<li><p>
How much <em>cognition</em> is present in these case?
</p>

<ul>
<li>
If the rotation in case (3) is cognitive, by what right do we count case (2) as fundamentally different?
</li>
</ul></li>
<li><p>
We often rely on external computing resources.
</p>

<ul>
<li>
Pen and paper long multiplication.
</li>
<li>
Physical re-arragnments of letter tiles to prompt word recall.
</li>
<li>
Instruments such as the nautical slide rule.
</li>
<li>
<p>General paraphenrnalia of:</p>
<ul>
<li>
Language
</li>
<li>
Books
</li>
<li>
Diagrams
</li>
<li>
Culture
</li>
</ul></li>
</ul></li>
<li><p>
In fact, these cases can be very real. (1) and (2) are options in the computer game Tetris.
</p>

<ul>
<li><p>
Two types of actions
</p>

<dl>
<dt>
Epistemic action
</dt>
<dd>
Alter the world so as to aid and augment cognitive processes such as recognition and search.
</dd>

<dd>
<p>
Demands spread of <em>epistemic credit</em>.
</p>
</dd>

<dt>
Pragmatic action
</dt>
<dd>
Alter the world because some physical change is desirable for
</dd>
</dl></li>
</ul></li>
</ul>

<h4>
Active Externalism
</h4>

<ul>
<li>
<p>The human organism is linked with an external entity in a two-way interaction, creating a <em>coupled system</em> that can be seen as a cogntive system in its own right.</p>
<ul>
<li>
Al the components in the system play an active causal role.
</li>
</ul></li>
</ul>

<h3>
Aizawa, &quot;Extended Cognition&quot;
</h3>

<h2>
March 11th, 2014 <small>Seminar</small>
</h2>

<h3>
Set up of Clark &amp; Chalmers
</h3>

<ul>
<li>
A lot of diversity, but this is only on Clark and Chalmers.
</li>
</ul>

<dl>
<dt>
<abbr title="Hypothesis of Extended Cognition">HEC</abbr>
</dt>
<dd>
<p>
Cognitive processes <em>litterally extends</em> into the enviroment surrounding the organism, and human cognitive states literally comporse.
</p>
</dd>

<dd>
<p>
Cognitive processes are implemented/realized/constituted by processes in the brain, body, and world. (Note not <em>can be</em>.)
</p>
</dd>

<dt>
<abbr title="Hypothese of Embedded Cognition">HEMC</abbr>
</dt>
<dd>
<p>
Cognitive processes depend on external ...
</p>
</dd>
</dl>

<blockquote>
  <p>
There is a difference between human cognition and cognition <em>simpliciter</em>.
</p>
</blockquote>

<ul>
<li><p>
Two things we need for discussion:
</p>

<ol>
<li>
A distinction between causation and constitution.
</li>
<li>
Some understanding of &quot;cognitive processes.&quot;
</li>
</ol></li>
<li><p>
A standard picture of contiion is that it's <em>among the factors</em> that <em>drive</em> behavior.
</p>

<ul>
<li>
Another part is <em>physical capacities</em>.
</li>
</ul></li>
<li><p>
These guys are calling <em>behavior</em> is extended, but of course everyone thinks that.
</p>

<ul>
<li>
We want to know wehther <em>cognition</em> is extended.
</li>
</ul></li>
<li><p>
If you mean by cognitive the term &quot;information processing&quot;, then of course its extended as well, because you can gather information from your environment.
</p></li>
</ul>

<blockquote>
  <p>
Why do you think that the game is over when cognition becomes behavior?
</p>
</blockquote>

<ul>
<li><p>
<strong>Take-away</strong>: This is not old-fashioned, bad methodology, this is just, what do you mean when you talk about cognition.
</p></li>
<li><p>
There are two kinds of arguments:
</p>

<ol>
<li>
Cogntive equivilence arguments
</li>
<li>
Coupling arguments
</li>
</ol></li>
</ul>

<blockquote>
  <ol>
  <li><p>
What are psychologists doing?
</p>
  
<ul>
  <li>
Should they be doing that?
</li>
  </ul></li>
  <li><p>
What do the folk think?
</p></li>
  <li>
What are the metaphysical joints?
</li>
  </ol>
</blockquote>

<!-- Abbreviations -->

<!-- Footnote -->

<div class="footnotes">
<hr />
<ol>

<li id="fn:4">
<p>
Are the rules of logic the norms of reasoning? Are the rules of logic pre-existing intuitions systemitized? <a href="#fnref:4" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:3">
<p>
If we map the structural, syntatical, and semantical features of the way we <em>talk about</em> beliefs states and agents and doxastic attitudes, the former is accesible to us and the latter is inaccesible. But if we find a feature of the former we can reasonable map it to the latter.
</p>

<p>
Often logic is called &quot;the norms of believing.&quot; Does this pose a problem because logic is a norm of reasoning and we commonly don't reason well, via the affirming the consequent result, then maybe we can't move from one to the other.
</p>

<p>
If we discover a mental structure which <em>conflicts</em> with logical structure, like for instance affirming the consequent being so common, should we affirm the consequent <em>more often</em> as a result? Alternatively, does it raise the perceived reliability of the seeming invalid (from a logical point-of-view) mental structure? If folk psychology is <em>generally valid</em> and we want to <em>vindicate</em> it, and actual, empirical mental structures follow invalid logical syntax, does that mean we want to vindicate invalid logical syntax too? <a href="#fnref:3" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:1">
<p>
I don't understand when you're &quot;supposed to stop describing&quot; for a closed system. I see that account (4) stops describing when the internal state of the computer comes to be required to continue explaining, but I don't understand why in account (3), supposedly &quot;closed&quot;, you don't say that bits come to be charged via electricity, for instance.
</p>

<p>
I think that even account (1), (2), and (3) really require an arbitrarily large conjunction of state description to be &quot;closed.&quot; <a href="#fnref:1" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:2">
<p>
I want to know what a biological or neurological taxonomy of types would look like -- our ability to type artifacts and concepts seems sufficiently general that &quot;anything can fit in the bin&quot;, that is we can store tokens of mountains and transcendental idealism -- if token couldn't be contained in any of our brain's possible types, would we be in a position to know? <a href="#fnref:2" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:10">
<p>
&quot;We don't care what if you understand what representation <em>is</em>, we do, and that's one.&quot; When scientists do this, or where scientists could possibly do this in other fields, you can still ask them where the representation <em>is</em> and <em>how</em> it represents.
</p>

<p>
The computer scientist can tell you that the letter on a screen is represented by the computer's software as an ASCII character with the numbers 1 through 64 (or something) and that those numbers are represented on the computers hardware with many, many on-off switches. On-off switches do no equal represented numbers and represented numbers are not represented letters, but they all share an resemblance that can be exploited.
</p>

<p>
What <em>I</em> want to know is can the cognitive scientist take a cognizers utterance of some state, say &quot;I am hungry&quot;, and use folk psychology to say that the utterance is a represents the utterer's inner state, and if you look at the inner state you'll find the processes by which the brain finds out and stores facts about the body (but more generally the non-mind world). <a href="#fnref:10" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:9">
<p>
There's a paradox of analysis here. Where on one hand, cogntive theorists and any competent user of English knows what a representation <em>is</em>, we use them all the time. There are only two possibilities: We know what representation is or we do not. If we do, then we need to &quot;check what's in the head&quot; and see if it's a representation. If we don't, what's the use of writing a book on it? To clarify what we could possibly mean by a &quot;mental model&quot; or &quot;mental representation&quot;? We don't know what representation is <em>in general</em>, nevermind whether they can be in the head. <a href="#fnref:9" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:6">
<p>
There is no &quot;primtive&quot;, I think, in geometry. A point can be expressed in terms of a line or a plain. A line can be expressed in terms of two points or a part of a plain. A plain can be expressed in terms of 3 lines or 3 points. <a href="#fnref:6" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:5">
<p>
Only with regards <em>to fitness</em>. <a href="#fnref:5" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:7">
<p>
I think that this may relate to debates in epistemology about brains in vats. So, here's an argument for thinking that they are related issues, and then I'll give an additional argument for how one might give insight to the other.
</p>

<p>
Assume that the brain represents its environment. Cummins thinks that a representation can represent all content with which it is isomorphic. Ramsey thinks that the content that matters is what <em>this</em> brain is using <em>now</em>. What's at issue in brain-in-vat skeptical arguments is that the brain's internal representation is isomorphic with both what the realist wants to call objective reality and what the skeptic wants to call &quot;brain-in-vat land&quot; (or the images that the vat is giving the brain or something). What's problematic about the brain-in-vat skeptical concerns is that the internal represnetation is isomorophic with both pictures.
</p>

<p>
So how is this insightful? Well. Actually. In hindsight, I'm not sure, I'd have to think about it. I think this succesfully relates the issues. Ramsey's solution to BIV challenges might be that the most explantory useful theory, because objective reality is what <em>this</em> brain is using <em>now</em> or something ... <a href="#fnref:7" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:8">
<p>
It's not that we don't know what representation is or that there is no representation, but rather, it's that representation with regards to cognizing if a fuzzy and poorly understood. <a href="#fnref:8" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:11">
<p>
I think what's confusing here is that the isomorphism being exploited is not a relation between the data structures and human norms about restauraunts, but rather between the input stories and the &quot;background knowledge&quot; data structures. <a href="#fnref:11" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:12">
<p>
<em>Of course</em> (I think) it's real and we're not just assigning something to physics. You can raise skeptical or linguistic or Wittgensteinian worries about words and how we use them, and in fact I think within the realm of those worries you might be right. But the present project is not trying to resolve worries about reductionism or scientism or realism - I think that just as someone who studies grains of sand can grant that there is some ambiguity about heaps, but when you see a heap of sand you very well know it's a heap of sand. By the same reasoning, I'm okay with being unsure if I can escape that with regards to the ontology of conciousness whether or not there is a brain with a mind or just a series of atoms, but when I'm reasoning about the mind I can put that worry aside. &quot;Keep your metaphysics and linguistics out of my philosophy of mind.&quot;
</p>

<p>
(Doesn't sound very strong, I know.) <a href="#fnref:12" class="footnote-backref">↩</a>
</p>
</li>

</ol>
</div> 
{% endraw %}