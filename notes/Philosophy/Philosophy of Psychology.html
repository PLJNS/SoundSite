---
layout: page
title: Notes
subtitle: From Paul Jones at Rutgers University
---
{% raw %}
<h1>
Topics in Philosophy of Psychology <small>with Professor Frances Egan</small>
</h1>

<h2>
Syllabus
</h2>

<h3>
Course Description
</h3>

<p>
The topic of the seminar is psychological explanation. We will focus on two issues: (1) The role of representation in psychological explanation, considering recent challenges – by (among others) proponents of extended, embodied, and enactive cognition – to the traditional view that psychological processes are to be understood as operations on symbol structures. We will consider the requirements for a theoretical construct to count as a representation, and whether there are any distinctively mental representations. (2) The relation of psychology to neuroscience, considering the recent challenge – by proponents of the so-called 'new mechanism' view in philosophy of science – to the view that psychological explanation of human cognitive capacities (in particular, computational explanations of cognitive capacities) can be constructed and confirmed independently of an account of how these capacities are realized in the brain. The new mechanists argue that genuine explanations of cognition are mechanistic explanations – they must bear a transparent relationship to accounts of realizing neural mechanisms.
</p>

<h3>
Readings
</h3>

<h4>
The role of representation in psychological explanation
</h4>

<ul>
<li><p>
The traditional view – strong representationalism
</p>

<ul>
<li>
Jerry Fodor. <em>Psychosemantics</em>, ch.1, Appendix
</li>
<li>
Zenon Pylyshyn. &quot;The Explanatory Role of Representation&quot;
</li>
</ul></li>
<li><p>
Implications and challenges
</p>

<ul>
<li>
David Kirsh. &quot;When is Information Explicitly Represented?&quot;
</li>
<li>
William Ramsey. <em>Representation Reconsidered</em> (ch.3 and ch.4)
</li>
</ul></li>
<li><p>
The challenge from extended, embodied, and enactive cognition
</p>

<ul>
<li>
Rodney Brooks. &quot;Intelligence without Representation&quot;
</li>
<li>
John Haugeland. &quot;Mind Embodied and Embedded&quot;
</li>
<li>
Clark &amp; Chalmers. &quot;The Extended Mind&quot;
</li>
<li>
Adams &amp; Aizawa. &quot;Defending the Bounds of Cognition&quot;
</li>
<li>
Robert Rupert. &quot;Challenges to the Hypothesis of Extended Cognition&quot;
</li>
<li>
Lawrence Shapiro. <em>Embodied Cognition</em> (excerpt)
</li>
<li>
Andy Clark. &quot;An Embodied Cognitive Science?&quot;
</li>
<li>
Clark &amp; Toribio. &quot;Doing without Representing?&quot;
</li>
<li>
Anthony Chemero. &quot;Anti-representationalism and the Dynamical Stance&quot;
</li>
<li>
Shaun Gallagher. &quot;Are Minimal Representations Still Representations?&quot;
</li>
<li>
Daniel Hutto. &quot;Radically Enactive Cognition in our Grasp&quot;
</li>
<li>
Mark Sprevak. &quot;Fictionalism about Neural Representations&quot;
</li>
</ul></li>
</ul>

<h4>
The autonomy of psychology
</h4>

<ul>
<li><p>
The traditional view
</p>

<ul>
<li>
Jerry Fodor. &quot;Special Sciences&quot;
</li>
<li>
Robert Cummins. &quot;Functional Analysis&quot;
</li>
<li>
John Haugeland. &quot;The Nature and Plausibility of Cognitivism&quot;
</li>
</ul></li>
<li><p>
The 'new mechanist' challenge
</p>

<ul>
<li>
David Michael Kaplan. &quot;Explanation and Description in Computational Neuroscience&quot;
</li>
<li>
Piccinini &amp; Craver. &quot;Integrating Psychology and Neuroscience: Functional Analyses as Mechanism Sketches&quot;
</li>
<li>
Daniel Weiskopf. &quot;Models and Mechanisms in Psychological Explanation&quot;
</li>
<li>
Frances Egan. &quot;Function-Theoretic Explanation and Neural Mechanisms&quot;
</li>
</ul></li>
</ul>

<h3>
Course Requirements
</h3>

<p>
Students taking the course for credit will be expected to write a paper due at the end of the semester. Papers must be on topics related to the course materials; topics should be cleared with me beforehand. Graduate students enrolled in the course will be expected to lead the discussion of assigned materials for one session.
</p>

<h3>
Attendance
</h3>

<p>
Attendance at the seminar is mandatory. You should let me know if you have to miss a class.
</p>

<h3>
Course Objectives
</h3>

<p>
Like any advanced philosophy graduate seminar this seminar aims to give students an opportunity to explore a set of fundamental issues in depth give students opportunities to further develop and hone their analytical skills give students a chance to write and revise a major research paper that will hopefully be interesting, original, and important provide ample opportunities to further develop one's oral skills during discussions
</p>

<h2>
January 28th, 2013 <small>Reading</small>
</h2>

<h3>
Psychosemantics <small>Jerry Fodor</small>
</h3>

<h4>
Chapter 1 <small>The Persistence of Attitudes</small>
</h4>

<h5>
Introduction
</h5>

<ul>
<li>
Quotes <em>A Midsummer Night's Dream</em> for an example of implicit, nondemonstrative, theoretical evidence.
</li>
<li><p>
Here is how the inference must have gone:
</p>

<ul>
<li><p>
Hermia had reason to believe herself beloved of Lysander.
</p>

<ul>
<li>
Because he told her.
</li>
</ul></li>
<li><p>
If Lysander loves Hermia, then Lysander wishes Hermia well.
</p></li>
<li>
If Lysander wishes Hermia well, then Lysander does not voluntarily desert Hermia in the night in the woods.
</li>
<li>
But Hermia was deserted by Lysander.
</li>
<li>
Therefore, not voluntarily.
</li>
<li>
Therefore, it is plausible Lysander has come to harm, and plausibly by Demetrius's hands, for Demetrius is Lysander's rival for the love of Hermia.
</li>
</ul></li>
<li><p>
Hermia believes (correctly) that: If <em>x</em> wants that <em>P</em>, and <em>x</em> believes that not-<em>P</em> unless <em>Q</em>, and <em>x</em> believes that <em>x</em> can bring about <em>Q</em>, then (ceteris paribus) <em>x</em> tries to being it about that <em>Q</em>.
</p></li>
<li><p>
But Hermia has it wrong about Demetrius.
</p>

<ul>
<li>
The intricate theory that she relies on to make sense of her peers, that we rely on to make sense of Hermia, and what Shakespeare relies on to predict and manipulat our sympathies.
</li>
</ul></li>
<li><p>
This theory, Fodor wants to emphasize:
</p>

<ol>
<li>
How often it goes right,
</li>
<li>
How deep it is,
</li>
<li>
How much we do depend on it.
</li>
</ol></li>
</ul>

<h5>
How often it works
</h5>

<ul>
<li><p>
Applications of commonsense mediate our relations with one another, and when its predictions fail these relations break down.
</p>

<ul>
<li>
Failures make for great theater.
</li>
<li>
Succeses are practically invisible and ubiquitous.
</li>
</ul></li>
<li><p>
Commonsense psychology is like those mythical Rolls Royce cars whose engines are seals when they leave the factory.
</p>

<ul>
<li>
Someone I don't know calls me and asks me to lecture in Arizone on Tuesday.
</li>
<li>
Fodor responds, &quot;Yes, thank, I'll be at your airport on the 3 p.m. flight.&quot;
</li>
<li>
<p>That's <em>all</em> that happens.</p>
<ul>
<li>
The theory is used to gap the bridge betwen utterances and actions.
</li>
<li>
If Fodor doesn't show it, the theory makes predicts about the likelihood of why.
</li>
</ul></li>
</ul></li>
<li><p>
The point is that the theory from which we get this extrodinary predictive power is just good old commonsense belief/desire psychology.
</p>

<ul>
<li>
It tells us how to infer people's intentions from the sounds they make.
</li>
<li>
It tells us how to infer people's behavior from their intentions.
</li>
<li>
And all of this works on your friends and your spouses and absolute strangers.
</li>
</ul></li>
<li><p>
But what about all those ceteris paribuses?
</p>

<ul>
<li>
Philosophers love the &quot;false or vacuous&quot; dilemma.
</li>
</ul></li>
<li><p>
Consider the defeasibility of &quot;if someone utters the form of words, 'I'll be at your airport on the 3 p.m. flight,' then he intends to be at your aiport on the 3 p.m. flight.&quot;
</p>

<ul>
<li><p>
This generalization does <em>not</em> hold if:
</p>

<ul>
<li>
The speaker is lying,
</li>
<li>
Monoligual speaker of Urdu who uttered by accident,
</li>
<li>
If the speaker is talking in his sleep,
</li>
<li>
Or ... whatever.
</li>
</ul></li>
<li><p>
Perhaps all that this means with a <em>ceteris paribus</em> clause is, if it doesn't happen, then it still true because <em>ceteris paribus</em>.
</p></li>
</ul></li>
<li><p>
A lot of philosophers are moved by this.
</p>

<ul>
<li>
<p>But the predictions often come out true!</p>
<ul>
<li>
So how could it be <em>empty</em>?
</li>
</ul></li>
</ul></li>
<li><p>
The reliance on uncashed <em>ceteris paribus</em> clauses is a general property of the <em>explicit</em> generalizations in <em>all</em> the special sciences.
</p>

<ul>
<li><p>
For example, &quot;A meandering river erodes its outside bank&quot;
</p>

<ul>
<li>
False or vacuous a philosopher may complain.
</li>
</ul></li>
<li><p>
A <em>ceteris paribus</em> clause here cause &quot;If <em>p</em> then <em>q</em> unless not <em>p</em> then <em>q</em>.&quot;
</p></li>
</ul></li>
<li><p>
Something must have gone wrong, surely this explanation is meaningful despite the big <em>ceteris paribus</em> clause.
</p>

<ul>
<li>
Surely these statements are stronger than &quot;<em>P</em> in any world where not not-<em>P</em>.&quot;
</li>
</ul></li>
<li><p>
There is a face similarity between implicit generalizations and commonsense psychology and explicity generalization in special sciences.
</p>

<ul>
<li>
We can get rid of the <em>ceteris paribus</em> clauses by <em>actually enumerating</em> the conditions.
</li>
</ul></li>
<li><p>
By this criterion, the only real science is basic physics.
</p>

<ul>
<li>
We can't enumarate all the conditions on geology by sticking to the vocabulary of geology.
</li>
<li>
The events in the <em>ceteris paribus</em> clause about the rivers aren't geological events, it's outside the domain. (Also hard.)
</li>
</ul></li>
<li><p>
Exceptions to generalizations of a special science are typically inexplicable from the point of view of that science.
</p>

<ul>
<li>
This is what makes it <em>special</em>.
</li>
<li>
<p>It's possible to enumerate them in the vocaubalry of another science.</p>
<ul>
<li>
You go &quot;down&quot; one or more levels and use the vocabulary of the more &quot;basic&quot; science.
</li>
</ul></li>
</ul></li>
<li><p>
If the world is describable at all in a closed causal system, it is with vocabulary of the most basic science.
</p>

<ul>
<li>
But the psychologist and the geologist needn't worry about this.
</li>
<li>
If you want to know where Fodor will be next Thursday, mechancins is <em>no use to you at all</em>.
</li>
</ul></li>
</ul>

<h5>
The depth of the theory
</h5>

<ul>
<li><p>
It's tempting to think that commonsense psychology is a toolkit of truisms one learns on Granny's knee.
</p>

<ul>
<li><p>
Like,
</p>

<ul>
<li>
The burnt child fears the fire.
</li>
<li>
Money cannot buy happiness.
</li>
<li>
Reinforcment affect response rate.
</li>
<li>
A way to a man's heart is through his stomach.
</li>
</ul></li>
<li><p>
None of these are worth save, but commonsense psychology is not like this.
</p></li>
<li>
<p>There are two parts to this:</p>
<ol>
<li>
The theory's underlying generalizations are defined over unobservables.
</li>
<li>
They lead to its predictions by iterating and interacting rather than being directly instantiated.
</li>
</ol></li>
</ul></li>
<li><p>
Behavior is casued by mental states and this causation is intricate.
</p>

<ul>
<li>
<p>Roughly, <em>If x is y's rival, the x prefers y's discomfiture, all else being equal</em>.</p>
<ul>
<li>
Doesn't mention behavior.
</li>
<li>
It <em>leads to</em> behvaioral predictions.
</li>
</ul></li>
</ul></li>
<li><p>
It is a deep fact about the world that the most powerful etiological generalizations hold of unobservable causes.
</p>

<ul>
<li>
Meterology is <em>not</em> deep because it's genralization are of the form, &quot;Red at night, sailors delight.&quot;
</li>
<li>
<p>Psychology <em>is</em> a deep theory, because we do not have access to mental states.</p>
<ul>
<li>
We're born mentalisms and realists, and we stay that way until common sense is driven out by bad philosophy.
</li>
</ul></li>
</ul></li>
</ul>

<h5>
Its indispensability
</h5>

<ul>
<li><p>
We have no alternative to the vocabulary of commonsense psychological explanation.
</p>

<ul>
<li>
There is no other way of describing our behaviors and their causes if we want our behaviors and their causes to be subsumed by any counterfactual-supporting generalizations that we know about.
</li>
</ul></li>
<li><p>
Without commonsense psychological generalizations, we cannot even describe the utterances as forms of words.
</p>

<ul>
<li>
Word is a <em>psychological</em> category.
</li>
<li>
<p>There are non acoustic properties that all and only <em>fully intelligble</em> tokens of the same word type share.</p>
<ul>
<li>
Which is why our best technology cannot build a typewriter you can dictate to.
</li>
</ul></li>
</ul></li>
<li><p>
We have <em>no</em> vocabulary for describing event types with these conditions:
</p>

<ol>
<li>
My behvior in uttering, &quot;I'll be there on Thursday&quot; counts as an event of type <span class="math">\(T_i\)</span>.
</li>
<li>
My arriving there on Thursday counts as an event of type <span class="math">\(T_j\)</span>.
</li>
<li>
'Events of type <span class="math">\(T_j\)</span> are consequent upon events of type <span class="math">\(T_i\)</span>' is even roughly true and counterfactual supporting.
</li>
<li>
Categories <span class="math">\(T_i\)</span> and <span class="math">\(T_j\)</span> are other than irreducibly psychological.
</li>
</ol></li>
<li><p>
Physics describes organisms <em>qua</em> motions, but not <em>qua</em> organismic.
</p>

<ul>
<li>
It dissolves the behav<em>er</em> and the behav<em>ior</em> into atoms in the void.
</li>
</ul></li>
<li><p>
Even if psychology was dispensible <em>in principle</em>, it's not argument for dispensing with it <em>in practice</em>.
</p></li>
</ul>

<h6>
The essence of the attitudes
</h6>

<blockquote>
  <p>
How do we tell whether a psychology <em>is</em> a belief/desire psychology? How, in general, do we know if propositional attitudes are among the entities that the ontology of a theory acknowledges? ... How do you distinguish elimination from reduction and reconstruction?
</p>
</blockquote>

<ul>
<li>
<p>Fodor will view psyhology as being commensensical about the attitudes just in case it postulates states satisfying:</p>
<ol>
<li>
Thy are semnatically evaluable.
</li>
<li>
The have causal powers.
</li>
<li>
The implicit generalizations of commonsense belief/desire psychology are largely true of them.
</li>
</ol></li>
</ul>

<blockquote>
  <p>
Squabbling about intuitions strikes me as vulgar.
</p>
</blockquote>

<h6>
<abbr title="Representational theory of mind">RTM</abbr>
</h6>

<ul>
<li>
<strong>Thesis</strong>: We have no reason to doubt - indeed, we have substantial reason to believe - that it is possible to have a scientific psychology that vindicates commonsense belief/desire explanation.
</li>
<li>
Fodor will argue that the sorts of objections philosophers have recently raised against belief/desire explanation are not conclusive against the best vindicating theory currently available.
</li>
<li>
At the heart of <abbr title="Representational theory of mind">RTM</abbr> is the postulation of a <abbr title="Language of thought">LOT</abbr>, an infinite set of 'mental representations' which function both as immediate objects of propositional attitudes and as the domains of mental processes.
</li>
<li><p>
The two claims he's making:
</p>

<ol>
<li><p>
<strong>The nature of propositional attitudes</strong>:
</p>

<ul>
<li>
<p>For any organism <em>O</em>, and any attitude <em>A</em> towards the proposition <em>P</em>, there is a (computational/functional) relation <em>R</em> and a mental representation <em>MP</em> such that:</p>
<ul>
<li>
<em>MP</em> means that <em>P</em>.
</li>
<li>
<em>O</em> has <em>A</em> just in case <em>O</em> bears <em>R</em> to <em>MP</em>.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>The nature of mental processes</strong>:
</p>

<ul>
<li>
Mental processes are causal sequences of tokenings of mental representations.
</li>
</ul></li>
</ol></li>
<li><p>
A train of thought is a causal sequence of tokenings of mental representations which express the propositions that are the objects of the thoughts.
</p></li>
<li>
<p><abbr title="Representational theory of mind">RTM</abbr> underlies practicaly all current psychological research on mentation, and the best science is ipso fact the best estimate of what there is and what it's made of.</p>
<ul>
<li>
Philosophers do not think this convincing, and Fodor is blushing for them.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
There is a stricking parallelism between the causal relations among mental states, on the one hand, and the semantic relations that hold among propositional objects, on the other.
</p>
</blockquote>

<ul>
<li>
Trains of thought are largely truth preserving<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">1</a></sup>.
</li>
<li>
<p>The &quot;trick&quot; is to combine the postulation of mental representations with the &quot;computer metaphor.&quot;</p>
<ul>
<li>
<p>Computers show us how to connect semantics with causal properties for symbols.</p>
<ul>
<li>
If having propositional attitude involves tokening a symbol, then we can get some leverage on connection semantical properties with causal ones for <em>thoughts</em>.
</li>
</ul></li>
</ul></li>
</ul>

<h2>
January 28th, 2014 <small>Seminar</small>
</h2>

<h3>
On representation
</h3>

<ul>
<li><p>
Advanced &quot;beings&quot; like us have evolved with our environment, this is with representation.
</p>

<ul>
<li>
These somehow causally affect behavior.
</li>
<li>
This is an &quot;inference to the best explanation.&quot;
</li>
</ul></li>
<li><p>
To the degree which organisms can react to a changing circumstance causes the representation to change, and the changed representations change behavior.
</p></li>
<li>
Some people think that dynamic behavior requires representationalism.
</li>
</ul>

<dl>
<dt>
Representation
</dt>
<dd>
A capacity of an organism.
</dd>

<dt>
Representation<em>s</em>
</dt>
<dd>
Concrete objects in the brain.
</dd>

<dd>
<p>
Some properties: - Physically realized. - They have causal powers, can have straightforward causal roles.
</p>

<ul>
<li>
<p>They have content, they are meaningful.</p>
<ul>
<li>
This requires a distinction between a vehicle of representation, they thing physically realized.
</li>
</ul></li>
</ul>
</dd>
</dl>

<ul>
<li>
Vision resonates with the environment like a tuning fork. Gibson's theory of perception.
</li>
<li>
Our brains or us might have the structure for representations, but we can't &quot;poke at&quot; any representation.
</li>
<li><p>
One way that representat<em>ion</em> could get cached out is <em>dispositionally</em>.
</p>

<ul>
<li>
Some theories might posit capacities that are best characterized as <em>representational</em>.
</li>
<li>
<p>There might not be an isolable state at the computational level that's like, &quot;There's the representation!&quot;</p>
<ul>
<li>
A real property of an organism, but diffuse.
</li>
<li>
Like mass.
</li>
</ul></li>
</ul></li>
<li><p>
Many theories posit something analogous to &quot;sentences in the head.&quot;
</p>

<ul>
<li>
<p>But there are other models of cognition.</p>
<ul>
<li>
Connectionist
</li>
<li>
Dynamical
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Vehicle-side
</h3>

<dl>
<dt>
Symbols
</dt>
<dd>
Little hooks on which you can hook meaning. Ripe for having content attributed to them, representation paradigmatically.
</dd>

<dt>
Connectionists models
</dt>
<dd>
Don't posit anything like &quot;sentences in the head.&quot; The carriers of meaning are networks of nodes that are connected in various ways. Do get interpreted, but do individual nodes get interpreted? <em>That</em> node refers to red <em>in a network</em>.
</dd>

<dd>
<p>
Typically are not construed as a strong representational view.
</p>
</dd>

<dt>
Dynamical models
</dt>
<dd>
Model the behavior of the system over time.
</dd>
</dl>

<ul>
<li><p>
What are the <em>vehicles</em> of meaning in non-symbol cognitive models?
</p>

<ul>
<li>
Is there anything in here that could be plausible construed as a representation?
</li>
</ul></li>
<li><p>
In classical models, what the vehicles of representation are, symbols are hooks on which to hang interpretation. Waving red flags, interpret me.
</p></li>
</ul>

<h3>
Content-side
</h3>

<ul>
<li><p>
Content has satisfaction conditions.
</p></li>
<li><p>
The problem of intentionality: How do mental states <em>get</em> their meaning? What is it about mental representations that given them their meaning or content.
</p>

<ul>
<li><p>
Public representation
</p>

<ul>
<li>
Public language content get their meaning by convention.
</li>
<li>
Icons and images get meaning by their resemblance and convention.
</li>
</ul></li>
<li><p>
Private representation
</p>

<ul>
<li>
Some relation that isn't <em>already presuming</em> meaning or intentionality.
</li>
<li>
The &quot;meaning in the head&quot; or &quot;state of affairs&quot; which it representations, <strong>the naturalism semantics project</strong>.
</li>
<li>
&quot;Complex causal relations&quot; represented in my head.
</li>
<li>
Teleological function of the brain, dealing with objects and properties I'm used, and my ancestors are, used to.
</li>
</ul></li>
</ul></li>
<li><p>
No one has be successful in naturalistic conditions on vehicle in the head having determinate content.
</p></li>
<li>
<p>One constraint on all of these account is that mental states can not only <em>represent</em> but also <em>mis</em>represent.</p>
<ul>
<li>
<p>The contrast is with Grice's notion of natural meaning.</p>
<ul>
<li>
The presence of smoke cannot misrepresent the presence of fire <em>unless</em> an agent misinterprets the smoke or something.
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Strong representationalism
</h3>

<dl>
<dt>
Strong representationalism
</dt>
<dd>
Posits structure entities that have content.
</dd>

<dd>
<p>
Mental processes are defined over the structures.
</p>
</dd>
</dl>

<ul>
<li>
An important issue: is there anything in non-representational views that could count as a vehicle.
</li>
</ul>

<dl>
<dt>
Extended cognition
</dt>
<dd>
The mind extends into the environment in some sense.
</dd>

<dd>
<p>
The extended thesis is not centrally a critique of representationalism, but it does have theses that bring to bear on representationalism.
</p>
</dd>

<dt>
Embodied cognition
</dt>
<dd>
Human cognition is necessarily embedded, the mind-brain is the executive that controls the body.
</dd>

<dt>
Dynamical cognition
</dt>
<dd>
Cognition consists in a dynamical interaction between a subject and an environment, and it's wrong to characterize the interaction as contentful.
</dd>
</dl>

<h3>
Jerry Fodor's &quot;Psychosemantics&quot;
</h3>

<ul>
<li><p>
He gives two arguments for the view known as &quot;representational theory of mind.&quot;
</p>

<ul>
<li><p>
Folk psychology is indispendable.
</p>

<ul>
<li>
The only way to vindicate it is if something like <abbr title="Representational theory of mind">RTM</abbr> is true.
</li>
</ul></li>
<li><p>
Eliminativism would be an absolute disaster.
</p></li>
</ul></li>
<li><p>
The second is the striking parrallelism between trains of thought and inferences.
</p></li>
<li>
The argument focuses on prediction, the only way to get around in the world is to ask someone something and predict that, in general, you do what you say and similar elsewhere.
</li>
<li>
Intentional realism is the only appropriate attitude to take towards folk psychology.
</li>
</ul>

<dl>
<dt>
Intentional realism
</dt>
<dd>
<ol style="list-style-type: decimal">
<li>They are semantically evaluable.</li>
<li>They have causal powers.</li>
<li>The implicit generalizations of commonsense belief/desire psychology are largely true of them.
</dd>
</li>
</ol>
<dt>
<abbr title="Representational theory of mind">RTM</abbr>
</dt>
<dd>
The best hope for realizing intentional realism.
</dd>

<dd>
<ol>
<li>
For any organism O, and any attitude A toward the proposition P, there is a ('computational'/'functional') relation R and amental representation MP such that MP means that PI and O has A iff O bears R to MP.
</li>
<li>
Mental processes are causal sequences of tokenings of mental representations.
</li>
</ol>
</dd>

<dd>
<p>
There are representation in a full-blooded sense.
</p>
</dd>
</dl>

<ul>
<li><p>
If the <abbr title="Language of thought">LOT</abbr> is true for propoisitonal attitudes, then the <abbr title="Representational theory of mind">RTM</abbr> is true because it's the weaker view.
</p>

<ul>
<li>
There's a big project on mental logic.
</li>
</ul></li>
<li><p>
Oftentimes, people's reason doesn't follow logic.
</p>

<ul>
<li>
People will more often affirm the consequent, more common in mental reasoning than <em>modus tollens</em>.
</li>
</ul></li>
<li><p>
&quot;I believe there is beer in the refridgerator.&quot;
</p>

<ul>
<li>
What was causally efficaious was that I wanted beer and believed there was beer, so I got up to get beer.
</li>
<li>
This is the &quot;belief box.&quot;<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">2</a></sup>
</li>
</ul></li>
<li><p>
There has to be a pretty good argument for the moving from property of one structure to the structure of the other. Specifically, they share semantics, but we need a good argument about syntax. Moving from the logical scheme to the empirical scheme.
</p>

<ul>
<li>
An argument <em>for this</em> is the tempurature case.
</li>
</ul></li>
<li><p>
<strong>We'll pick up next time with the Dennett counter-example.</strong>
</p></li>
</ul>

<h2>
February 4th, 2014 <small>Seminar</small>
</h2>

<h3>
On Fodor's <em>Psychosemantics</em>, Chapter 1 &quot;The Persistence of Attitudes&quot;
</h3>

<ul>
<li><p>
Fodor is looking for a theory for states that interprets attitudes as states with propositions, beliefs and desires <em>at least</em>.
</p>

<ul>
<li>
Chomsky: &quot;We <em>cognize</em> the grammar of English.&quot;
</li>
<li>
&quot;We <em>believe</em> the grammar of our language.&quot;
</li>
</ul></li>
<li><p>
This is the &quot;last gasp&quot; of narrow content for Fodor.
</p></li>
<li>
<p>On Dennet's counterexample.</p>
<ul>
<li>
<p>There is a distinction between core and derivative.</p>
<ul>
<li>
Those that <em>explicitly</em> represented and those that are <em>implicitly</em> represented.
</li>
<li>
Our heads just aren't big enough.
</li>
</ul></li>
</ul></li>
</ul>

<h2>
February 11th, 2014 <small>Reading</small>
</h2>

<h3>
Zenon Pylyshyn. &quot;The Explanatory Role of Representation&quot;
</h3>

<h4>
Introduction
</h4>

<ul>
<li>
<p>The hardest puzzle is consciousness.</p>
<ul>
<li>
Second hardest is <em>meaning</em>, which this work explains.
</li>
<li>
Does <em>not</em> solve the puzzle of meaning.
</li>
<li>
The author aims to <strong>describe how the idea of the semantic content of representations is implicitly viewed within the field of cognitive science, and discuss why this view is justifiable</strong>.
</li>
</ul></li>
</ul>

<dl>
<dt>
Representations
</dt>
<dd>
Generalizations stated over the contents of representations are not mere functional generalization in the usual sense.
</dd>

<dt>
Function generalizations
</dt>
<dd>
A theory that does not refer to physical properties of the particular system in question, only how it operates.
</dd>
</dl>

<ul>
<li>
There will be a <em>representational level</em> and a <em>symbol-processing level</em>.
</li>
</ul>

<h4>
The Appeal to Representations
</h4>

<ul>
<li><p>
Law-like generalization and explanations can differ in several ways, consider:
</p>

<ol>
<li>
A certain object accelerated at <em>a</em> meters per second per second because a steady force was applied that was equal to <em>ma</em>.
</li>
<li>
A certain neuron fired because a potential of <em>v</em> millivolts was applied along two of its sentries and that it had been inactive during the previous <em>t</em> milliseconds
</li>
<li>
A bit pattern of certain computer register came to have a particular configuration because of the particular contents present in the instruction register and the program counter, and because the system is wired according to a certain transfer protocol.
</li>
<li>
The computer printed numbers 2, 4, 6, because it started with the number 2 and added 2 repeatedly or because it applied the successor function repeatedly and double the value before printing.
</li>
<li>
The pedestrian dialed 911 because he believed it to be the emergency number and had recognized the urgent need for assistance.
</li>
</ol></li>
<li><p>
Accounts (1), (2), and (3), all the terms refer to properties of objects within the closed system.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">3</a></sup>
</p></li>
<li>
Accounts (4) and (6) are different in this important respect: Both make substantive reference to entities or properties that are not an intrinsic part of their state description, that is <em>numbers</em> and <em>need for assistance</em>.
</li>
</ul>

<blockquote>
  <p>
How is it possible for properties of the world to determine behavior when the properties are not causally related in the required sense to to the functional states of the system? --- <strong>Brentano's problem</strong>
</p>
</blockquote>

<ul>
<li>
The notion of representation is necessary only in the context of explanation.
</li>
<li>
Behavior is being caused by certain states of one's brain, and so mental states themselves are related to agent's actions.
</li>
<li><p>
Brain states are not causally connected in appropriate ways to walking or mountains.
</p>

<ul>
<li><p>
The relationship <em>is one of content</em>, a semantic, not causal, relationship.
</p>

<ul>
<li>
The notion of content is roughly that of what the states <em>are about</em>.
</li>
</ul></li>
<li><p>
Brain states cause certain movements. If these movements are view as members of equivalence classes described as &quot;writing a sentence about walking in the Santa Cruz mountains&quot; the brains states must be treated as embodying representations of these codes by certain rules.
</p></li>
</ul></li>
<li><p>
Contrast the brain to a watch -- a watch's &quot;behavior&quot; is considered coextensive with the set of movements corresponding to the physical description of behavior.
</p>

<ul>
<li>
Two ways of explaining human behavior capture extremely different generalizations.
</li>
</ul></li>
</ul>

<h4>
Representational and Functional Levels
</h4>

<ul>
<li>
This shows that <strong>a <em>functional</em> description of mental processes is not enough, there must also be content</strong>.
</li>
</ul>

<blockquote>
  <p>
If the content makes a difference to behavior, is it not also a functional difference?
</p>
</blockquote>

<ul>
<li><p>
To be in a certain representational state is to have a certain symbolic expression in some part of memory.
</p>

<ul>
<li>
The expression <em>encodes</em> the semantic interpretation and the combinatorial structure of the expression encodes the relation among the contents of the subexpressions, much as in the combinatorial system of predicate calculus.
</li>
</ul></li>
<li><p>
The reason there must be symbolic codes is that they can enter in causal relations.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">4</a></sup>
</p></li>
<li>
<p>If there is a unique symbolic expression corresponding to each content, one might expect functional states and representational states to once again be one-to-one relation. Not so, because:</p>
<ol>
<li>
There may be codes with the same semantic content which are functionally but not semantically distinguishable.
</li>
<li>
<p>Merely possessing a certain symbolic expression that encodes semantic content is insufficient to produce behavior.</p>
<ul>
<li>
You need to <em>interpret</em> the symbols.
</li>
</ul></li>
</ol></li>
</ul>

<dl>
<dt>
Semantic-level generalization
</dt>
<dd>
Generalizations expressible in terms of the semantic content of representations.
</dd>

<dd>
<p>
Newell calls this &quot;knowledge-level.&quot;
</p>
</dd>

<dt>
Symbol-level generalizations
</dt>
<dd>
Generalizations expressible in terms of functional properties of the functional architecture.
</dd>
</dl>

<h4>
Representational Content as Defining a Level of Description
</h4>

<ul>
<li><p>
We abandoned a biological vocabulary because of arbitrarily large disjunctions corresponding to processes like &quot;thinking.&quot;
</p>

<ul>
<li>
Functional generalizations cannot be captured in a finite neurophysiological description.
</li>
<li>
<p>There is a vocabulary in between &quot;<em>n</em> fired at <em>t</em> with <em>v</em>&quot; and &quot;He called 911 because he believed he was in an emergency.&quot;</p>
<ul>
<li>
And it's <em>functional</em>.
</li>
<li>
And it's in <em>semantic terms</em>.
</li>
</ul></li>
</ul></li>
<li><p>
&quot;The principal of rationality is a major reason for our belief that a purely functional account will fail to capture certain generalizations, hence, that a distinct new level is required.&quot;
</p></li>
</ul>

<h5>
Levels and Constraints on Realizability
</h5>

<ul>
<li>
<p>For a description, that description might be compatible with other levels.</p>
<ul>
<li>
Newtons laws &quot;are compatible with&quot; biological taxonomy.
</li>
</ul></li>
</ul>

<h3>
Kirsh, &quot;When is Information Explicitly Represented?&quot;
</h3>

<h4>
Introduction
</h4>

<ul>
<li>
Computation is the process of making <em>ex</em>plicit what was <em>im</em>plicit.
</li>
<li>
We know what is explicit, the problem is what information is implicit.
</li>
<li>
<strong>Suppose</strong>: To understand a computation it is necessary to track the trajectory of informational state the computing system follows as it winds its way to an explicit answer.
</li>
<li><p>
Different kinds of computational mechanisms:
</p>

<ul>
<li>
PDP systems
</li>
<li>
Massive cellular automata
</li>
<li>
Analog relaxation systems
</li>
</ul></li>
<li><p>
How far do you have to remove &quot;explicitness&quot; until it's implicit? That is, intuitively.
</p></li>
<li>
Suppose a system has highly ambigious encodings and must deliberate to choose the right interpretation.
</li>
</ul>

<blockquote>
  <p>
Computer and cognitive scientists talk as if they have a precise idea of these concepts (of implicit and explicit,) but that do not.
</p>
</blockquote>

<ul>
<li>
<p><strong>Intent</strong>: Articulate a particular conception of explicit information that at least may serve as a stable base for subsequent inquiries into the meaning of implicit information.</p>
<ol>
<li>
Will show why notions of explicit and implicit need elucidation, that they are not consistent.
</li>
<li>
Will explore efforts to identify explicit information with syntatically and semantically well-defined representations.
</li>
<li>
Will mention some implications of the view.
</li>
</ol></li>
</ul>

<h4>
Our intuitions about explictness are inconsistent
</h4>

<ul>
<li>
<p>Perhaps the intuition is that &quot;if it's there for all to see&quot;, then it's explicit.</p>
<ul>
<li>
<p>Four tempting properties of explicitness:</p>
<ol>
<li>
<strong>Locality</strong>: They are visible structures with a definite defintion.
</li>
<li>
<strong>Movability</strong>: No matter where in a book a word is found or where in the library a book is stored, that word retains its meaning and retains its explicitness.
</li>
<li>
<strong>Meaning</strong>: Words have a definite information content.
</li>
<li>
<strong>Availability</strong>: The information content of a word is directly available to the system reading it, no elaborotate transition or interpretation process is necessary to extract the information it represents.
</li>
</ol></li>
</ul></li>
</ul>

<blockquote>
  <p>
The trouble with using immediate grapability, or better <em>immediate readability</em> as the mark of explicitness is that we run into problems as soon as we ask whether to count accessing time as part of the reading process.
</p>
</blockquote>

<ul>
<li><p>
Are elements in large sets immediate readable?
</p>

<ul>
<li>
This takes <em>computational energy</em> but is somehow <em>constant</em>/
</li>
</ul></li>
<li><p>
From a process perspective information is explicit only when it is <em>ready to be used</em>
</p>

<ul>
<li>
No computation necessary.
</li>
</ul></li>
<li><p>
Explicitness is tied to usability.
</p></li>
</ul>

<h4>
Our intuitions about implcitness are inconsistent
</h4>

<ul>
<li><p>
Our concept of implicit runs into problems when we try to pin down what &quot;in principle, recoverable&quot; means.
</p>

<ul>
<li>
Is it how much effort is required?
</li>
<li>
Our all lemmas, &quot;nearby&quot; or &quot;far&quot;, equally as implicit?
</li>
</ul></li>
<li><p>
To make it more natural, perhaps the conception of <em>that which is not explicit but which could be made so</em>.
</p></li>
</ul>

<h4>
Why it matters whether our intuitions are unsetteled
</h4>

<h4>
Towards a theory of explicitness
</h4>

<h5>
Four condition on explicitness
</h5>

<dl>
<dt>
Locality
</dt>
<dd>
<p>
They states, structures, or processes - henceforth symbols - which explcitly encode information must be easily seperable from each other.
</p>
</dd>

<dt>
Movability
</dt>
<dd>
<p>
An ambigious language may explicitly encode information only if it is trivial to indetify the syntatic and semantic indentity of the symbol.
</p>
</dd>

<dt>
Immediately readable
</dt>
<dd>
<p>
Symbols explicitly encode information if they are either: - Readable in constant time. - Sufficiently small to fall in the attention span of an operator.
</p>
</dd>

<dt>
Meaning
</dt>
<dd>
<p>
The information which a symbol explictly encodes is given by the set of associated states, structures, or processes it activates in constant time.
</p>
</dd>
</dl>

<h4>
Implications
</h4>

<ul>
<li>
<p>One of the following is false:</p>
<ul>
<li>
The <abbr title="Language of thought">LOT</abbr> is the best level of analysis to represent perspicuously the episodes of in our <em>mental life</em>.
</li>
<li>
The events in our mental life are identical with operations on explicit representations.
</li>
<li>
The <abbr title="Language of thought">LOT</abbr> perspicously describes human information processing.
</li>
</ul></li>
</ul>

<h3>
Marr &amp; Nishihara, &quot;Artificial Intelligence and the Sensorium of Sight&quot;
</h3>

<h3>
Ramsey, section 3.1 of <em>Representation Reconsidered</em>
</h3>

<h2>
February 11th, 2013 <small>Seminar</small>
</h2>

<dl>
<dt>
Eliminativism
</dt>
<dd>
<p>
Unless there's a transparent relationship between folk psychology and scientific psychology, then the attitude to hold towards folk psychology is <em>elimination</em>.
</p>
</dd>
</dl>

<ul>
<li>
<p>Ramsey still agrees with Fodor in <em>eliminativism</em>.</p>
<ul>
<li>
Stich, in the 80s and 90s, was an eliminativist.
</li>
<li>
Especially, &quot;The Case Against Belief&quot; didn't think there'd be a transparent relationship.
</li>
<li>
<p>Egan thinks there <em>both</em> wrong, that is the eliminativists.</p>
<ul>
<li>
The paper on this is &quot;Folk Psychology and Cognitive Architechure&quot;
</li>
</ul></li>
</ul></li>
</ul>

<h3>
On Pylyshyn
</h3>

<ul>
<li><p>
He wants to justify and vindicate justification with regards to representation.
</p>

<ul>
<li>
Why content?
</li>
</ul></li>
<li><p>
The argument for a certain way of construing vehicles pervades the whole book.
</p>

<ul>
<li>
<em>We cannot eliminate appeal to content</em>.
</li>
</ul></li>
<li><p>
Stich argued that if we really are symbol systems, like Fodor thinks, then why do we need content?
</p>

<ul>
<li>
Pylyshyn is trying to justify the appeal to content.
</li>
</ul></li>
</ul>

<h4>
The First Puzzle
</h4>

<ul>
<li><p>
Brain states are not causally connected to ...
</p>

<ul>
<li>
&quot;I believe there's beer in the refrigerator.&quot;
</li>
<li><p>
There's not causal connection between the object and my brain states.
</p>

<ul>
<li>
There <em>needn't</em> be a causal connection.
</li>
</ul></li>
<li><p>
I'm thinking about Paris and that might cause me to book a vacation to Paris.
</p></li>
</ul></li>
<li><p>
The solution is how mental states fit into the physical world.
</p></li>
</ul>

<blockquote>
  <p>
Why does he think we need content if it is the physical stuff that is realized?
</p>
</blockquote>

<ul>
<li>
Pylyshyn uses the capturing language as a way of &quot;getting the generalization.&quot;
</li>
<li>
<p>The difference between us and watches.</p>
<ul>
<li>
We get convinience from &quot;thermometer gets the tempurature.&quot;
</li>
<li>
We really need do need generalizations to explain our behavior, it's only convinient for everything else.
</li>
</ul></li>
</ul>

<h3>
The Scheme
</h3>

<p>
<span class="math">\[ \lbrace I_1 ... I_n \rbrace \]</span> <span class="math">\[ f_I \]</span> <span class="math">\[ \lbrace S_1 ... S_n \rbrace \]</span> <span class="math">\[ f_R \]</span> <span class="math">\[ \lbrace P_1 ... P_n \rbrace \]</span>
</p>

<ul>
<li><p>
The three levels:
</p>

<ol>
<li>
Interpretations level
</li>
<li>
Interpretation function
</li>
<li>
Syntatitic level
</li>
<li><p>
Realization function
</p>

<ul>
<li>
Maps physical state to symbol (numeral) 2 or 3 ...
</li>
<li>
Nothing stops it from a bizzare representation.
</li>
<li>
Indepedant from meaning
</li>
</ul></li>
<li><p>
Physical states
</p>

<ul>
<li>
These have <em>many</em> causal functions.
</li>
<li>
These can bring about states, but those states can <em>bring about</em> cognitively important states.
</li>
</ul></li>
</ol></li>
<li><p>
Fodor think that we need an <abbr title="Language of thought">LOT</abbr>, so he wants to identify a level where structures that function as words and they have <em>constituency relationships</em>.
</p></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: If you want to be representational about this, you can say that the mapping is just useful to us ...
</p>
</blockquote>

<pre><code> 2   3     5
 ^   ^     ^
 |   |     |
 |   |     |
S1  S2 -&gt; S3
 ^   ^     ^
 |   |     |
 |   |     |
P1  P2 -&gt; P3

An "adding machine" on this model.
</code></pre>

<h3>
What's the role of content?
</h3>

<ul>
<li>
<p>If the <em>I</em>s and the <em>P</em>s are in a one-one relationship, then you get not explanatory leverage.</p>
<ul>
<li>
Why can't we just explain the systems behvior in terms of the causally efficacious states.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
What is implicit in this is not that there is a one-to-one function, but that there are arbitrarily many disjunctions
</p>
</blockquote>

<ul>
<li>
The rationality principle says that these transition states need to be <em>truth-preserving</em>.
</li>
<li><p>
There is an <em>implicit assumption</em> about explaining cognition that we are doing something inherently rational already.
</p></li>
<li><p>
Here's something that is paradigm of cognitive but not rational:
</p>

<ul>
<li>
If we can succesfully characterize someone in this way, if they have the <em>p</em> and the <em>p</em> to <em>q</em>
</li>
<li>
These models are used to describe data that <em>is</em>. &quot;Cognitive function.&quot;
</li>
</ul></li>
<li><p>
This kind of explantory projects is going to be applied to sub-cognitive functions and phenomena.
</p>

<ul>
<li>
Not just the personal level of behavior.
</li>
<li>
Open up the principle of rationality
</li>
</ul></li>
<li><p>
Fodor in the appendix appeals to theory or processing, these theories are commited to mental representations, so this provides support for the <abbr title="Representational theory of mind">RTM</abbr> and by commitment the <abbr title="Language of thought">LOT</abbr>.
</p>

<ul>
<li>
A lot of these examples, the successes do not fit the model the content of attitudes, beliefs, desires.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Niko</strong>: Is the argument that you want to get rid of the interpretation level but that's silly because that's where we began to explain.
</p>
</blockquote>

<ul>
<li>
<p>No, that's Prof. Egan's argument.</p>
<ul>
<li>
The argument <em>here</em> is that there's going to be a series of unprincipled generalizations without principles of rationality.
</li>
<li>
<p>These are understood as generalizations.</p>
<ul>
<li>
Unless we can understand these truth-preserving.
</li>
</ul></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Ben</strong>: Couldn't you weaken it, the more plausible claim is if we lose the generalizations, we lose these great explanations.
</p>
</blockquote>

<ul>
<li>
<p>Derivability isn't a basic notion.</p>
<ul>
<li>
We can reconstruct it to be truth preserving.
</li>
</ul></li>
</ul>

<dl>
<dt>
Derivability
</dt>
<dd>
<p>
What goes on in the middle level, the higher-level causation.
</p>
</dd>
</dl>

<h4>
Egan's View
</h4>

<ul>
<li><p>
Unless what the system is doing is <em>recognizabily cognitive</em>, this isn't worth doing.
</p>

<ul>
<li><p>
It can add, it can compute, it can speak, it can see, it can recover the 3D structure of the scene.
</p>

<ul>
<li>
To do this, we need content.
</li>
</ul></li>
<li><p>
Unless you can attribute content, you <em>can</em> talk about causal stories, you cannot explain Mary's <em>behavior</em>.
</p>

<ul>
<li>
The folk psychological explaination makes it rational.
</li>
</ul></li>
</ul></li>
<li><p>
This point will come up in Ramsey's IO representations.
</p>

<ul>
<li>
Unless we can construe the inputs and outputs of the system, as cognitive function, then the project <em>doesn't make any sense.</em>
</li>
<li>
We're trying to explain a cognitive competence. We at least need to interpret the inputs and outputs of the system as contentful.
</li>
</ul></li>
</ul>

<h4>
On the third argument, pg. 27
</h4>

<ul>
<li>
My brain states are not causally connected to Paris, or walking, or mountains.
</li>
<li>
The relationship must be in terms in content.
</li>
</ul>

<h3>
On Kirsch
</h3>

<ul>
<li><p>
The main point is that intuitions about explicitness and implicitness are very broken, but that does not stop philosophers and others from using these notions fairly heavily.
</p>

<ul>
<li>
Our concept on explicit representation rests pretty heavily on the printed word.
</li>
<li>
<p>Construing explicit representation on the printed word, litterally sentences in the head, assumes that explicit representation can be understood in purely <em>functional</em> terms.</p>
<ul>
<li>
The big point he wants to get across is that this has to come across in <em>procedural</em> terms.
</li>
</ul></li>
</ul></li>
<li><p>
It's often assumed that we know what <em>explicit</em> is, but not what <em>implicit</em> is, and so implicit is defined in terms of not-explicit.
</p></li>
<li>
He distinguishes between <em>exploiting</em> regularities in the environment and <em>representing</em> regularities in the environment.
</li>
</ul>

<h3>
On Marr
</h3>

<blockquote>
  <p>
So why are we reading Marr? A lot of the work he's done has been falsified.
</p>
</blockquote>

<ul>
<li>
Not interested in the details, interested in the methodology
</li>
</ul>

<h4>
Marr's Methodology
</h4>

<ul>
<li><p>
He thought that an account of a cognitive system has three different levels.
</p>

<ul>
<li>
<p>Those are:</p>
<ol>
<li>
<strong>Theory of computation</strong>: Specification of the function computed. That is, the <em>what</em> that the system is doing. This is the level at which the competence of the system is, knowledge, processing details.
</li>
<li>
<strong>Representation &amp; Algorithm</strong>: This level specific the alogirthm which computes the function speficied in the theory of computation (level 1). And it specifies structures over which the alogirthm which it is defined. <em>Ex</em>. A function can be added, the algorithm can be Roman or Arabic.
</li>
<li>
<strong>Neural interpretation</strong>: This is, in some sense, the <em>how</em>.
</li>
</ol></li>
</ul></li>
<li><p>
The big picture:
</p>

<ol>
<li>
Gray level array
</li>
<li>
Primal sketch
</li>
<li><p>
Has multiple:
</p>

<ul>
<li>
SFM
</li>
<li>
Stereoscopic
</li>
<li>
Texture
</li>
<li>
Shading
</li>
</ul></li>
<li><p>
$ 2  $D
</p></li>
<li>
3D
</li>
</ol></li>
<li><p>
<strong>Next time</strong>: SFM
</p></li>
</ul>

<!-- Accronymns -->

<!-- Footnote -->

<div class="footnotes">
<hr />
<ol>

<li id="fn:4">
<p>
Are the rules of logic the norms of reasoning? Are the rules of logic pre-existing intuitions systemitized? <a href="#fnref:4" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:3">
<p>
If we map the structural, syntatical, and semantical features of the way we <em>talk about</em> beliefs states and agents and doxastic attitudes, the former is accesible to us and the latter is inaccesible. But if we find a feature of the former we can reasonable map it to the latter.
</p>

<p>
Often logic is called &quot;the norms of believing.&quot; Does this pose a problem because logic is a norm of reasoning and we commonly don't reason well, via the affirming the consequent result, then maybe we can't move from one to the other.
</p>

<p>
If we discover a mental structure which <em>conflicts</em> with logical structure, like for instance affirming the consequent being so common, should we affirm the consequent <em>more often</em> as a result? Alternatively, does it raise the perceived reliability of the seeming invalid (from a logical point-of-view) mental structure? If folk psychology is <em>generally valid</em> and we want to <em>vindicate</em> it, and actual, empirical mental structures follow invalid logical syntax, does that mean we want to vindicate invalid logical syntax too? <a href="#fnref:3" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:1">
<p>
I don't understand when you're &quot;supposed to stop describing&quot; for a closed system. I see that account (4) stops describing when the internal state of the computer comes to be required to continue explaining, but I don't understand why in account (3), supposedly &quot;closed&quot;, you don't say that bits come to be charged via electricity, for instance.
</p>

<p>
I think that even account (1), (2), and (3) really require an arbitrarily large conjunction of state description to be &quot;closed.&quot; <a href="#fnref:1" class="footnote-backref">↩</a>
</p>
</li>

<li id="fn:2">
<p>
I want to know what a biological or neurological taxonomy of types would look like -- our ability to type artifacts and concepts seems sufficiently general that &quot;anything can fit in the bin&quot;, that is we can store tokens of mountains and transcendental idealism -- if token couldn't be contained in any of our brain's possible types, would we be in a position to know? <a href="#fnref:2" class="footnote-backref">↩</a>
</p>
</li>

</ol>
</div> 
{% endraw %}