---
layout: page
title: Notes
subtitle: From Paul Jones at Rutgers University
---
{% raw %}
<h1>
Discrete Structures II <small>with Professor David Cash</small>
</h1>

<h2>
Description
</h2>

<p>
Provides the background in combinatorics and probability theory required in design and analysis of algorithms, in system analysis, and in other areas of computer science.
</p>

<ul>
<li><p>
Topics:
</p>

<ul>
<li>
Counting: Binomial Coefficients, Permutations, Combinations, Partitions.
</li>
<li>
Recurrence Relations and Generating Functions.
</li>
<li><p>
Discrete Probability:
</p>

<ul>
<li>
Events and Random Variables;
</li>
<li>
Conditional Probability, Independence;
</li>
<li>
Expectation, Variance, Standard Deviation;
</li>
</ul></li>
<li><p>
Binomial, Poisson and Geometric Distributions;
</p></li>
<li>
law of large numbers.
</li>
<li>
<p>Some Topics from Graph Theory:</p>
<ul>
<li>
Paths,
</li>
<li>
Components,
</li>
<li>
Connectivity,
</li>
<li>
Euler Paths,
</li>
<li>
Hamiltonian Paths,
</li>
<li>
Planar Graphs,
</li>
<li>
Trees.
</li>
</ul></li>
</ul></li>
</ul>

<h2>
Syllabus
</h2>

<h3>
Course Overview
</h3>

<p>
This course is an introduction to <em>probability</em> and <em>combinatorics</em>, including their basic mathematical foundations as well as several non-trivial applications of each. The first topic explores how to think about <em>random processes</em> in a rigorous and sensical way, and second is about techniques for <em>counting</em> the number of objects fitting a given description. As we will see, the techniques involved in both topics are strongly related.
</p>

<p>
Roughly, we expect to cover the following list of topics. Lecture summaries will be posted at the bottom of this page.
</p>

<ul>
<li>
Review of prerequisites, set theory, countability
</li>
<li>
Random experiments, sample spaces, events, probability measures
</li>
<li>
Conditional probability, Bayes' Theorem, Independence
</li>
<li>
Combinatorics and Counting
</li>
<li>
Recurrences, Generating Functions
</li>
<li>
Random Variables
</li>
<li>
Bernoulli Trials
</li>
<li>
Expectation, Variance
</li>
<li>
Markov Chains
</li>
<li>
Applications of Probability and Combinatorics
</li>
</ul>

<h3>
General Information
</h3>

<ul>
<li>
Instructor: <a href="david.cash@cs.rutgers.edu">David Cash</a>
</li>
<li>
<a href="http://www.research.rutgers.edu/~dc789/206-spring-13/index.html">Course website</a>
</li>
<li><p>
Recommended textbooks:
</p>

<ul>
<li>
S. Ross, <em>A First Course in Probability, 8th Edition</em>
</li>
<li>
H. Rosen, <em>Discrete Mathematics and its Applications, 7th edition</em>
</li>
</ul></li>
<li><p>
Lecture meetings: Monday and Wednesday 5:00pm -- 6:20pm in Hill 116
</p></li>
<li><p>
Recitations:
</p>

<ul>
<li>
Section 1: Monday 6:55pm -- 7:50pm in SEC 220
</li>
<li>
Section 2: Wednesday 6:55pm -- 7:50pm in ARC 110
</li>
<li>
Section 3: Monday 8:25pm -- 9:20pm in Hill 120
</li>
</ul></li>
<li><p>
Instructor office hours: Tuesday 1:30pm-3:00pm in Hill 411 and by appointment
</p></li>
<li>
Teaching assistant: TBA (TAB@cs.rutgers.edu)
</li>
<li>
Teaching assistant office hours: TBA in TBA and by appointment
</li>
</ul>

<h3>
Assignments and Grading
</h3>

<p>
Homework will be assigned roughly every 1.5 weeks. There will be one in-class midterm and one final exam. Final grades will be an average of homework grades (30%), midterm grades (30%), and final exam grades (40%).
</p>

<h2>
January 23rd, 2012 - Lecture: Introduction
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Review of sets and induction.
</li>
<li>
Samples spaces and events.
</li>
</ul>

<h3>
Introduction
</h3>

<ul>
<li>
This title of this class is funny because it tells you nothing about it.
</li>
<li><p>
What this class is really about is <strong>discrete probability</strong> and <strong>combinatorics</strong>.
</p>

<ul>
<li>
Discrete means &quot;not continuous.&quot;
</li>
<li>
Probability means about change and percentage.
</li>
<li>
Combinatorics is the study of combining things.
</li>
<li>
More specifically, it is the study of counting the number of discrete objects fitting some description.
</li>
</ul></li>
<li><p>
Probability in computer science
</p>

<ul>
<li>
Random number generator (RNG)
</li>
<li>
Average case algorithms
</li>
<li>
Information security and cryptography
</li>
<li>
Networking, TCP/IP
</li>
</ul></li>
</ul>

<h3>
Formalizaing Probability
</h3>

<ul>
<li><p>
A set is a collection of objects. Ex:
</p>

<ul>
<li>
<span class="math">\(S = {a, b, c} \)</span>
</li>
<li>
$  = 1, 2, 3, … $ (natural numbers)
</li>
<li>
$  = {…, -2, -1, 0, 1, 2, …} $ (integers)
</li>
</ul></li>
<li><p>
Notation:
</p>

<ul>
<li>
<span class="math">\(\emptyset = {} \)</span>
</li>
<li>
Write <code>{x |</code> &quot;statement about <code>x}</code>&quot; to mean all <code>x</code> satisfying statement.
</li>
</ul></li>
<li><p>
Relations between sets
</p>

<ul>
<li>
<code>A</code> and <code>B</code> are sets.
</li>
<li>
<span class="math">\(A \subseteq B \)</span> means &quot;<code>A</code> is a subset of <code>B</code>&quot;
</li>
<li>
<span class="math">\(B \subseteq A \)</span> means &quot;<code>A</code> is a superset of <code>B</code>&quot;
</li>
</ul></li>
<li><p>
Operations on sets
</p>

<ul>
<li>
$ A B $ means &quot;union of <code>A</code> and <code>B</code>&quot;
</li>
<li>
$ A B $ means &quot;intersection of <code>A</code> and <code>B</code>&quot;
</li>
<li>
$ A B $ means &quot;set difference of <code>A</code> and <code>B</code>&quot; which means everything in <code>A</code> that is not in <code>B</code>
</li>
<li>
When a &quot;universe&quot; is understood, we can define <span class="math">\(A^{c}\)</span> as the &quot;complement&quot; of <code>A</code>
</li>
</ul></li>
<li><p>
Set identities <span class="math">\[ A \cup (B \cap C) = \]</span> <span class="math">\[(A \cup B) \cap (A \cup C)\]</span> <span class="math">\[A \cap (B\cup C) = (A\cap B) \cup (A \cap C)\]</span>
</p>

<ul>
<li><p>
Prove: <span class="math">\((A \cup B)^{c} = A^c \cap B^c \)</span>
</p>

<ol>
<li>
Show: <span class="math">\((A \cup B)^{c} \subseteq A^c \cap B^c \)</span>
</li>
<li>
Show: <span class="math">\(A^c \cap B^c \subseteq (A \cup B)^{c}\)</span>
</li>
</ol></li>
<li><p>
A good way to remember these is to use Venn diagrams.
</p></li>
</ul></li>
<li><p>
DeMorgan's laws
</p></li>
<li><p>
Proofs by Induction
</p>

<ul>
<li>
Let's say you want to prove a statement for all natural numbers <code>n</code>.
</li>
<li><p>
For example, for all <code>n &gt; 0</code>, <span class="math">\(\sum_{i=0}^{n}2^i = 2^{n + 1}\)</span>
</p>

<ul>
<li>
Call the statements <span class="math">\(s&amp;#95;0, s&amp;#95;1, s&amp;#95;2, … \)</span>
</li>
<li>
Suppose it's not true for all of them.
</li>
<li>
Key obeservation: If they're not all true, then there is a smallest <code>i</code> such that <span class="math">\(S_i\)</span> is false.
</li>
</ul></li>
<li><p>
A proof by induction works in 2 steps:
</p>

<ol>
<li>
Prove statement for first <code>n</code>
</li>
<li>
Give a conditional proof, prove that for all <code>n</code>, if <span class="math">\(S_{n-1}\)</span> is true, then <span class="math">\(S_{n}\)</span>
</li>
</ol></li>
</ul></li>
<li><p>
Countability
</p>

<ul>
<li>
A set <code>S</code> is countable if it is finite, or if it can be pun in one-to-one correspondence with the natural numbers.
</li>
</ul></li>
<li><p>
Sample space
</p>

<ul>
<li>
Fix a &quot;sample space&quot; that represents everything that could happen in our random experiment
</li>
<li>
A sample space is always a set
</li>
<li>
<p>Examples of samples spaces</p>
<ol>
<li>
Tossing a coin: $ S = (H, T) = ({0, 1})<span class="math">\(&lt;/li&gt; &lt;li&gt;Tossing two coins: \)</span>S = ({HH, HT, TH, TT}) <span class="math">\(&lt;/li&gt; &lt;li&gt;Rolling a die: \)</span> S = ({1, 2, 3, 4, 5, 6})<span class="math">\(&lt;/li&gt; &lt;li&gt;Rolling 2 dice: \)</span> S = ((i, j) | $ i, j in$ (1,2,3,4,5,6))$
</li>
</ol></li>
</ul></li>
</ul>

<h2>
January 23rd, 2013 - Reading
</h2>

<h3>
2.1 Introduction
</h3>

<blockquote>
  <p>
In this chapter, we introduce the concept of the probability of an event and then show how probabilities can be computed in certain situations. As a preliminary, however, we need the concept of the sample space and the events of an experiment.
</p>
</blockquote>

<h3>
2.2 Sample Space and Events
</h3>

<ul>
<li><p>
The set of all possible outcomes of an experiment is known as the <strong>sample space</strong> of the experiment and is denoted by S. Examples:
</p>

<ol>
<li>
The sex of a newborn child: S = {g, b}
</li>
<li>
Horse race with seven horses: S = {all 7! permutations of (1, 2, 3, 4, 5, 6, 7)}
</li>
<li>
Flipping two coins: S = {(H, H), (H, T), (T, H), (T, T)}
</li>
<li>
Tossing two dice: S = {(i, j): i, j = 1, 2, 3, 4, 5, 6}
</li>
</ol></li>
<li><p>
Any subset E of the sample space is known as an <strong>event</strong>. For the previous examples,
</p>

<ol>
<li>
E = {g} would be having a girl.
</li>
<li>
E = {all outcomes in S beginning with 3} would be 3 winning the race.
</li>
<li>
E = {(H, H), (H, T)} would be an event where you get heads first.
</li>
<li>
E = {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)} would be an event where the sum of the two rolls is seven.
</li>
</ol></li>
<li><p>
A <strong>union</strong> of any two events E and F results in an event with all outcomes in both E and F.
</p></li>
<li>
An <strong>intersection</strong> of any two events E and F results in a event with only those outcomes in both E and F.
</li>
<li>
If E and F share no outcomes, making their intersection the empty set (denoted by <span class="math">\(\emptyset\)</span>, they are said to be <strong>mutually exclusive</strong>.
</li>
<li>
The complement of E is defined as all events not in E that are in the sample space, denoted by $ E^c $.
</li>
</ul>

<h2>
January 28th, 2013 - Lecture: Probability Measures
</h2>

<h3>
Announcements
</h3>

<ul>
<li>
Recitation starts this week.
</li>
<li>
Homework 1 will be out tonight and due in a week.
</li>
</ul>

<h3>
Topics
</h3>

<ul>
<li>
Operations on events.
</li>
<li>
Abstract definition of probability measure.
</li>
<li>
Equivalent definition for countable sample spaces.
</li>
<li>
Basic consequences of the definition.
</li>
</ul>

<h3>
Probability
</h3>

<ul>
<li>
Sample space = any set
</li>
<li>
&quot;Event&quot; = an subset of the sample space
</li>
<li>
Fix a sample space <code>S</code>
</li>
<li>
Take any events $ E S, E_2 S <span class="math">\(&lt;/li&gt; &lt;li&gt;Consider the set \)</span> E_1 E_2 S $
</li>
<li><p>
Example: Flip two coins
</p>

<ul>
<li>
$ S = HH, HT, TH, TT <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> E_1 = HT, HH <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> E_2 = TH, HH <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> E_1 E_2 = HT, HH, TH $ = &quot;heads came up&quot;, &quot;either heads came up on the 1st or 2nd toss&quot;
</li>
<li>
$ E_1 E_2 = HH $
</li>
</ul></li>
<li><p>
Example: Rolling a die
</p>

<ul>
<li>
$ E_1 = $ &quot;die was rolled&quot; = {1, 3, 5}
</li>
<li>
$ E_2 = $ &quot;die roll was divisible by 3&quot; = {3, 6}
</li>
<li>
$ E_1 E_2 = 3<span class="math">\(&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Note: If \)</span>E_1 = $ and $ E_2$ is any event, then $ E_1, E_2$ are mutually exclusive.
</p></li>
<li><p>
Complement: If E is an event, then so is $ E^c s $ means &quot;not E&quot; or &quot;E didn't happen.&quot;
</p>

<ul>
<li>
<p>For the example of the two coins,</p>
<ul>
<li>
E = &quot;1st coin was heads&quot; means that $ E^c = TH, TT <span class="math">\(&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If \)</span> E_1 E_2 $, then what can we say?
</p>

<ul>
<li>
Proposal: If E1 happened then E2 also happened.
</li>
<li>
<p>Example: two dice:</p>
<ul>
<li>
First event: &quot;a one was rolled&quot; = {1}
</li>
<li>
Second event: &quot;the dice summed to 3&quot; = {(1,2),(2,1)}
</li>
<li>
The second event is a subset of the first event.
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Probability measures
</h3>

<ul>
<li>
We've been talking about whether or not something happened, but we'll now move to how <em>likely</em> it is for something to happen.
</li>
<li><p>
We want to assign to even event a number between zero and one represent how &quot;likely&quot; that event is.
</p>

<ul>
<li>
We are going to assume things about randomness and predictibility, we quickly get into philosophy otherwise.
</li>
</ul></li>
<li><p>
We want a function P that givens the probabilty P(E) for every E $ $ S
</p></li>
<li><p>
Example: A coin flip
</p>

<ul>
<li>
S = {H, T}
</li>
<li>
P({H}) = 1/2
</li>
<li>
P({T}) = 1/2
</li>
<li>
P({H, T}) = 1
</li>
<li>
P( $ $ ) = 0
</li>
</ul></li>
<li><p>
Example: Tossing two coins
</p>

<ul>
<li>
S + {HH, HT, TH, TT}
</li>
<li>
P({HH}) = 1/4 (and for HT, TH, ... )
</li>
<li>
If E $ $ S, P(E) = <span class="math">\(\frac{|E|}{|S|} \)</span>
</li>
</ul></li>
<li><p>
Definition: A function P is a probability measure if it maps every event E $ $ S to a number and satisfies the following three rules:
</p>

<ol>
<li>
For every E, O &lt;= P(E) &lt;= 1
</li>
<li>
P(S) = 1
</li>
<li>
<p>Countable additivity</p>
<ul>
<li>
For every sequence $ E_1, E_2, ... $ of event, if the $ E_i$ are all mutually exlcusive, so <span class="math">\(E&amp;#95;i \cap E&amp;#95;j = \emptyset \)</span> <span class="math">\[ P\left(\bigcup_{i=1}^\infty E_i\right)=\sum_{i=1}^\infty P(E_i) \]</span>
</li>
</ul></li>
</ol></li>
<li><p>
Claim: If P is a probabilty measure, then P($ $ ) = 0
</p>

<ul>
<li>
Proof: Take $ E_1, E_2, ... $ where $ E_i = $ for every i. The sequence is mutually exclusive.
</li>
</ul></li>
<li><p>
When S is finite, we can replace (3) with the &quot;finite additivity&quot;: &gt; $ P (<em>{i = 1}^n E_1 ) = </em>{i = 1}^n P(E_i) $
</p></li>
<li><p>
Definition: If S is countable then we can say that P is discrete.
</p>

<ul>
<li>
Every sample set in this class will be countable.
</li>
</ul></li>
<li><p>
Claim: If P is discrete, then it is completely determined by its value on one-element sets.
</p></li>
<li>
If you know that P({x}) for every $ x S <span class="math">\(, then you know that P(E) for every E \)</span> $ S
</li>
<li>
<p>Write E = {$ x_1, x_2, ... <span class="math">\(} Let \)</span> E_i = x_i $</p>
<ul>
<li>
The <span class="math">\(E&amp;#95;i \)</span> are mutually exclusive.
</li>
<li>
$ _{i = 1}^E_1 = E $
</li>
<li>
So by rule 3,
</li>
</ul></li>
</ul>

<h2>
January 28th, 2013 - Reading
</h2>

<h3>
2.3 Axioms of Probability
</h3>

<ul>
<li>
One way of defining probability is in relative frequency.
</li>
<li>
For some event E in sample space S, we define n(E) to be the number of times in the first n repetitions of the experiment that the event E occurs. Therefore, <span class="math">\(P(E) = \lim&amp;#95;{x \to \infty}\frac{n{E}}{n} \)</span>
</li>
<li><p>
While intuitive, how do we know that n(E)/n will converge to some constant limiting value that will be the same for each possible sequence of repetitions of the experiment?
</p>

<ul>
<li>
Proponents of this definitions say that this is an &quot;axiom&quot; of the system, an assumption.
</li>
<li>
Critics say that this is too complicated an assumption.
</li>
</ul></li>
<li><p>
Would it not be more reasonable to assume a set of simpler and more self-evident axioms about probability and then attempt to prove that such a constant limiting frequency does in some sense exist?
</p></li>
<li><p>
Axioms:
</p>

<ol>
<li><p>
$ 0 P(E) 1 $
</p>

<ul>
<li>
The probability that the outcome of the experiment is an outcome in E is some number between 0 and 1
</li>
</ul></li>
<li><p>
$ P(S) = 1 $
</p>

<ul>
<li>
The outcome will be a point in the sample space S
</li>
</ul></li>
<li><p>
$ P(_{i=1}<sup>E_i)=_{i=1}</sup>P(E_i) $
</p>

<ul>
<li>
For any sequence of mutually exclusive events, the probability of at least one of these events occurring is just the sum of their respective probabilities.
</li>
</ul></li>
</ol></li>
</ul>

<h3>
2.4 Some simple propositions (through proposition 4.3)
</h3>

<ul>
<li>
Proposition 4.1: <span class="math">\[ P(E^c) = 1 − P(E) \]</span>
</li>
<li>
Proposition 4.2: <span class="math">\[ If E \subset F, then P(E) \le P(F). \]</span>
</li>
<li>
Proposition 4.3: <span class="math">\[ P(E \cup F) = P(E) + P(F) − P(EF) \]</span>
</li>
</ul>

<h2>
January 28th, 2013 - Homework 1
</h2>

<ol>
<li><p>
Let <span class="math">\(n\geq 2\)</span> and <span class="math">\(A_1,\ldots,A_n\)</span> be sets in some universe S. In this problem we will give a proof by induction of the identity <span class="math">\[ \left(\bigcap_{i=1}^n A_i\right)^c = \bigcup_{i=1}^n A_i^c. \]</span>
</p>

<ol>
<li><p>
(5 points) State and prove the base case for an inductive proof, meaning that the identity is true when <span class="math">\(n=2\)</span>.
</p>

<ol>
<li>
<span class="math">\(\left(\bigcap_{i=1}^2 A_i\right)^c = \bigcup_{i=1}^2 A_i^c\)</span>
</li>
<li>
$ (A_1 A_2 )^c = A_1<sup>c A_2</sup>c$ is true by DeMorgan’s
</li>
</ol></li>
<li><p>
(5 points) State and prove the inductive step, where one shows that the identity is true for general <span class="math">\(n&gt;2\)</span>, assuming it is true for <span class="math">\(n-1\)</span>.
</p>

<ol>
<li>
$ (<em>{i=1}^n A_i)^c = </em>{i=1}<sup>n A_i</sup>c $ assume true for n
</li>
<li>
$ (<em>{i=1}^{n - 1} A_i)^c = </em>{i=1}^{n - 1} A_i^c $ induction for n - 1
</li>
<li>
<span class="math">\(\bigcup&lt;em&gt;{i = 1}^n A&amp;#95;i^c \cup A&lt;/em&gt;{n + 1}^c = \)</span> <span class="math">\[ \left( \bigcap_{i = 1}^n A_i \right) \cup A_{n + 1}^c\]</span>
</li>
<li>
<span class="math">\(\left(\bigcap&lt;em&gt;{i=1}^n A&amp;#95;i\right)^c = \)</span> <span class="math">\($ \left( \bigcap&lt;/em&gt;{i = 1}^n A&amp;#95;i \right) \cup A&amp;#95;{n + 1}^c\)</span>
</li>
</ol></li>
</ol></li>
<li><p>
Give sample spaces that model the outcomes for the following experiments. You may use a regular expression or other formalisms that you find convenient. (2 points each)
</p>

<ol>
<li>
Rolling 3 dice. <span class="math">\[S = \lbrace (i,j,k) &amp;#58; | &amp;#58; i,j,k \in \]</span> <span class="math">\[\lbrace 1, 2, 3, 4, 5, 6 \rbrace \rbrace  &amp;#33;\,\]</span>
</li>
<li>
Rolling a die until an even result comes up, or the die is rolled three times. <span class="math">\[S = \lbrace  ( i ) , ( j , i ), (  j , j , \lbrace i,j\rbrace  ) &amp;#58; |\]</span> <span class="math">\[ &amp;#58; i \in \lbrace 1,2,4\rbrace , j \in \lbrace 1,3,5\rbrace  \rbrace  \]</span>
</li>
<li>
Tossing a pair of coins until they both come up tails. <span class="math">\[S = \lbrace  (e_1), (e_2,e_1), (e_2,e_2,e_1), \]</span> <span class="math">\[(e_2,e_2,e_2, ..., e_1) &amp;#58; | \]</span> <span class="math">\[e_1 = TT, e_2 \in \lbrace HT, TH, HH\rbrace \rbrace \]</span>
</li>
<li>
Draw 2 balls from an urn which contains 6 balls, each with a distinct label from <span class="math">\(\lbrace 1,2,3,4,5,6\rbrace \)</span>. <span class="math">\[S = \lbrace i,j &amp;#58; | &amp;#58; i,j \in \lbrace 1,2,3,4,5,6\rbrace \]</span> <span class="math">\[  \land i \neq j \rbrace \]</span>
</li>
<li>
Draw 1 ball from the same urn, then replace it and draw a ball again. <span class="math">\[S = \lbrace (i,j) &amp;#58; | \]</span> <span class="math">\[&amp;#58; i,j \in \lbrace 1,2,3,4,5,6\rbrace \rbrace \]</span>
</li>
</ol></li>
<li><p>
For each of the sample space, describe the events (as sets) <span class="math">\(A\cup B\)</span> and <span class="math">\(A \cap B\)</span>, when A and B are as follows. (2 points each)
</p>

<ol>
<li>
A = &quot;5 is rolled exactly twice&quot; and B = &quot;dice values add to an odd number&quot;. <span class="math">\[A \cup B = \lbrace  \lbrace i, i, i\rbrace , \lbrace j, j, i\rbrace  &amp;#58; | \]</span> <span class="math">\[ &amp;#58; i \in \lbrace 1, 3, 5 \rbrace , j \in \lbrace 2, 4, 6\rbrace \rbrace  \]</span> <span class="math">\[A \cap B = \lbrace \lbrace 5, 5, i\rbrace  &amp;#58; \mid \]</span> <span class="math">\[ &amp;#58; i \in \lbrace 1,3\rbrace \rbrace  \]</span>
</li>
<li>
A = &quot;1 comes up exactly twice&quot; and B = &quot;3 comes up exactly twice&quot;. <span class="math">\[A \cup B = \lbrace (1,1,i), (j, 1, 1), (1, j, 1) &amp;#58; | \]</span> <span class="math">\[ &amp;#58; i \in \lbrace 2,3,4,5,6\rbrace  \]</span> <span class="math">\[\land j \in \lbrace 3,5\rbrace  \rbrace  \]</span> <span class="math">\[\cup \lbrace (3,3,i), (j, 3, 3), (3, j, 3) &amp;#58; | \]</span> <span class="math">\[ &amp;#58; i \in \lbrace 1,2,4,5,6\rbrace \]</span> <span class="math">\[\land j \in \lbrace 1,5\rbrace  \rbrace  \]</span> <span class="math">\[A \cap B = \emptyset \]</span>
</li>
<li>
A = &quot;both coins come up heads at the same time at some point&quot; and B = &quot;both coins come up tails at the same time at some point,&quot; <span class="math">\[A \cup B = S = \]</span> <span class="math">\[\lbrace  (e_1), (e_2,e_1), (e_2,e_2,e_1), (e_2,e_2,e_2, ..., e_1) &amp;#58; \]</span> <span class="math">\[| &amp;#58;e_1 = TT, e_2 \in \lbrace HT, TH, HH\rbrace \rbrace \]</span> <span class="math">\[A \cap B = A = \lbrace  (e_1, e_2, e_3, ... , e_n) &amp;#58; \]</span> <span class="math">\[\mid &amp;#58; e_n = TT \land e_i \in \lbrace HT, TH, HH\rbrace \]</span> <span class="math">\[ \land 1 \le i \le n - 1 \land \]</span> <span class="math">\[ n \in N \land \exists j e_j = HH \]</span> <span class="math">\[\land 1 \le j \le n - 1 \rbrace \]</span>
</li>
<li>
A = &quot;1 is drawn at least once&quot; and B = &quot;1 is drawn twice&quot;. <span class="math">\[A\cup B = A = \lbrace \lbrace i,j\rbrace  &amp;#58; | \]</span> <span class="math">\[&amp;#58; i,j \in \lbrace 1,2,3,4,5,6\rbrace  \land \]</span> <span class="math">\[(i \neq j) \land (i = 1 \oplus j = 1) \rbrace  \]</span> <span class="math">\[A \cap B = \emptyset \]</span>
</li>
<li>
A = &quot;1 is drawn at least once&quot; and B = &quot;1 is drawn twice&quot; <span class="math">\[A \cup B = \lbrace \lbrace 1,1\rbrace ,\lbrace 1,2\rbrace ,\lbrace 1,3\rbrace\]</span> <span class="math">\[ ,\lbrace 1,4\rbrace ,\lbrace 1,5\rbrace ,\lbrace 1,6\rbrace \rbrace \]</span> <span class="math">\[A \cap B = \lbrace \lbrace 1,1\rbrace \rbrace  \]</span>
</li>
</ol></li>
</ol>

<h2>
January 30th, 2013 - Lecture: Begin counting and its applications
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Finish proof about probability of a union.
</li>
<li>
Counting: The multiplication rule, permutations of n objects and k-out-of-n objects.
</li>
<li>
The birthday problem.
</li>
<li>
Permutations when some objects are indistinguishable.
</li>
<li>
Counting walks on grids.
</li>
</ul>

<h3>
Introduction
</h3>

<ul>
<li><p>
Claim: If P is a probability measure on S and $ E S <span class="math">\(, \)</span>F S $ are events, then $ P(E F) = P(E) + P(F) - P(E F) $. Proof:
</p>

<ul>
<li><p>
<span class="math">\(P( E \cup F)\)</span>
</p>

<ul>
<li>
The first step in a lot of these proofs is to &quot;disjointify&quot; <span class="math">\(E \cup F \)</span>
</li>
<li>
The handy way to do this is to write that as $ E (E^c F) = P(E) + P(E^c F)<span class="math">\(&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Now we need \)</span> P(E^c F) = P(F) - P(E F) $
</p>

<ul>
<li>
<p>Equivilently, <span class="math">\(P(F) = P(E \cap F) + P(E^c \cap F) \)</span>. This is true because:</p>
<ul>
<li>
$ F = (E F) (E^c F) <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> E F $ and $ E^c F $ are mutually exclusive.
</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<h3>
Counting
</h3>

<ul>
<li><p>
Multiplication rule: If I have <span class="math">\(n&amp;#95;1 \)</span> choices in an experiment, and if I have for each of my first choices another <span class="math">\(n_2\)</span> choices, and the same <span class="math">\(n_3\)</span> choices, up to <span class="math">\(n_r\)</span>. In that setting, then I have $ n_1 n_2 n_3 ... n_r $ total choices.
</p>

<ul>
<li><p>
Example: How many license plate numbers are there if they consist of 6 characters, the first six being letters and the last three being numbers? (ABX 161)
</p>

<ul>
<li>
26 choices for first letter
</li>
<li>
26 choices for the second letter
</li>
<li>
26 choices for the third letter
</li>
<li>
10 choices for the first number
</li>
<li>
10 choices for the second number
</li>
<li>
10 choices for the third number
</li>
<li>
Therefore, 26 x 26 x 26 x 10 x 10 x 10 =
</li>
</ul></li>
<li><p>
Example: What if no repetitions are allowed?
</p>

<ul>
<li>
26 possibilities
</li>
<li>
25 possibilities
</li>
<li>
24 possibilities
</li>
<li>
10 possibilities
</li>
<li>
9 possibilities
</li>
<li>
8 possibilities
</li>
<li>
26 x 25 x 24 x 10 x 9 x 8 =
</li>
</ul></li>
</ul></li>
<li><p>
Permutations: &quot;How many ways can I arrage n distinct objects in a line? (i.e. in order)
</p>

<ul>
<li>
Answer: Build an arrangement by picking 1st element, then 2nd element, etc, euntil nth.
</li>
<li><p>
Definition: n! = n(n + 1) ... x 2 x 1
</p>

<ul>
<li>
0! = 1 (There is only one way to arrange zero items)
</li>
</ul></li>
<li><p>
What is we only arrange k &lt; n of the objects?
</p>

<ul>
<li>
<span class="math">\(n - k + 1 = \frac{n!}{(n - k)!} \)</span>
</li>
</ul></li>
<li><p>
Example: A class of 6 men and 4 women
</p>

<ol>
<li><p>
How many ways can they be ranked?
</p>

<ul>
<li>
10!
</li>
</ul></li>
<li><p>
What if you rank men and women seperately?
</p>

<ul>
<li>
6! + 4!
</li>
</ul></li>
</ol></li>
<li><p>
Birthday &quot;Paradox&quot; (Surprise)
</p>

<ul>
<li>
Assume birthdays are random, you can be born on any day, and ignore February 29th.
</li>
<li>
What is the size of the group required to give us a 50% chance of getting a match?
</li>
<li>
Use your intuition: There are 365 days, so probably around 1/365?
</li>
<li>
You'd expect 100 people, 200 people, once you approach half the number of days.
</li>
<li>
Let's study this using our probability.
</li>
<li>
Take n people, n random birthdays.
</li>
<li>
Sample space will be S = {1, ..., 365}^n
</li>
<li>
$ E S $ represents &quot;there is at least one match&quot;
</li>
<li>
$ P(E) = 1 - P(E^c) <span class="math">\(&lt;/li&gt; &lt;li&gt;It&#39;s kind of hard to account for E, with multiple matches, all matches, etc.&lt;/li&gt; &lt;li&gt;The complement of E is &quot;there are no matches.&quot;&lt;/li&gt; &lt;li&gt;How big is E complement?&lt;/li&gt; &lt;li&gt;\)</span> E^c S $ is a lists of n numbers without match.
</li>
<li>
$ |E^c| = 365 364 363 (365 - n + 1) <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> P(E^c) =  =  <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> .507 | n = 23 <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> .97 | n = 50 <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> &gt; .999999 | n = 100 <span class="math">\(&lt;/li&gt; &lt;li&gt;Intuition: It&#39;s not the number of people that matters, it&#39;s the number of pairs, \)</span>O(n^2)$ type of growth.
</li>
</ul></li>
</ul></li>
<li><p>
How many distinct strings can be formed from the letters in &quot;remember&quot;? Answer:
</p>

<ul>
<li>
There are 8! ways to arrange the letters in total.
</li>
<li>
There are 3! ways to arrange the Es without changing the word.
</li>
<li>
There are 2! ways to arrange the Rs without changing the word.
</li>
<li>
There are 2! ways to arrange the Ms without changing the word.
</li>
<li>
There is 1 way to rearrange the B without changing the word.
</li>
<li>
<span class="math">\(\mathrm{R_1 E_1 M_1 E_2 M_2 B_1 E_3 R_2} \)</span>
</li>
<li>
$  <span class="math">\(&lt;/li&gt; &lt;li&gt;How many times does &quot;remember&quot; show up in the list?&lt;/li&gt; &lt;li&gt;Permutations when some elements are interchangable: When putting n objects in order if n&amp;#95;1 are interchangabl and N&amp;#95;2 are interchangable, and ..., n&amp;#95;r are interchangable, the number of permutations \)</span>  $
</li>
</ul></li>
</ul>

<h2>
January 30th, 2013 - Recitation
</h2>

<ol>
<li><p>
Give an inductive proof of the identity: $ (<em>{i=1}<sup>{n})</sup>c = </em>{i=1}<sup>n(A_i</sup>c)<span class="math">\(. You may use the fact that for any two sets A and B, \)</span>(A B)^c = A^c B^c $.
</p></li>
<li><p>
What are the sample spaces for the following experiments?
</p>

<ul>
<li>
Throwing three dice
</li>
<li>
Flipping a coin until either (a) &quot;heads&quot; occurs, or (b) the coin is flipped 8 times.
</li>
<li>
Throwing a die until &quot;6&quot; occurs.
</li>
<li>
Drawing 2 balls from an urn, where the urn contains 5 balls, each with a distinct label from 1 to 5.
</li>
<li>
Picking one ball from the same urn as above, and then putting it back into the urn and again picking a ball.
</li>
</ul></li>
<li><p>
Which of the sample spaces from Exercise 2 are finite? Which are countable? (Justify your answers.)
</p></li>
<li>
<p>For the sample spaces you presented in Exercise 2, describe the events <span class="math">\(A \cup B \)</span> and <span class="math">\(A \cap B\)</span> when A and B are:</p>
<ul>
<li>
A = &quot;at least one 6&quot; and B = &quot;an odd total&quot;.
</li>
<li>
A = &quot;heads occurs twice&quot; and B = &quot;tails occurs twice&quot;
</li>
<li>
A = &quot;5 occurs an infinite number of times&quot; and B = &quot;6 occurs&quot;
</li>
<li>
A = &quot;5 is drawn at least once&quot; and B = &quot;5 is drawn twice&quot;
</li>
<li>
A = &quot;5 is drawn at least once&quot; and B = &quot;5 is drawn twice&quot;
</li>
</ul></li>
</ol>

<h2>
January 30th, 2013 - Reading
</h2>

<h3>
1.2 The Basic Principle of Counting
</h3>

<blockquote>
  <p>
The basic principle of counting will be fundamental to all our work. Loosely put, it states that if one experiment can result in any of m possible outcomes and if another experiment can result in any of n possible outcomes, then there are mn possible out- comes of the two experiments.
</p>
</blockquote>

<ul>
<li>
<strong>The basic principle of counting</strong>: Suppose that two experiments are to be performed. Then if experiment 1 can result in any one of m possible outcomes and if, for each outcome of experiment 1, there are n possible outcomes of experiment 2, then together there are mn possible out- comes of the two experiments.
</li>
<li>
<strong>The generalized basic principle of counting</strong>: If r experiments that are to be performed are such that the first one may result in any of n1 possible outcomes; and if, for each of these n1 possible outcomes, there are <span class="math">\(n_2\)</span> possible outcomes of the second experiment; and if, for each of the possible outcomes of the first two experiments, there are <span class="math">\(n_3\)</span> possible outcomes of the third experiment; and if ..., then there is a total of <span class="math">\(n&amp;#95;1 \cdot n&amp;#95;2 … n&amp;#95;r \)</span> possible outcomes of the r experiments.
</li>
</ul>

<h4>
Example 2a
</h4>

<ul>
<li>
A small community consists of 10 women, each of whom has 3 children. If one woman and one of her children are to be chosen as mother and child of the year, how many different choices are possible?
</li>
<li>
Solution. By regarding the choice of the woman as the outcome of the first experiment and the subsequent choice of one of her children as the outcome of the second experiment, we see from the basic principle that there are <span class="math">\(10 \times 3 = 30\)</span> possible choices.
</li>
</ul>

<h4>
Example 2b
</h4>

<ul>
<li>
A college planning committee consists of 3 freshmen, 4 sophomores, 5 juniors, and 2 seniors. A subcommittee of 4, consisting of 1 person from each class, is to be chosen. How many different subcommittees are possible?
</li>
<li>
Solution. We may regard the choice of a subcommittee as the combined outcome of the four separate experiments of choosing a single representative from each of the classes. It then follows from the generalized version of the basic principle that there are <span class="math">\(3 \times 4 \times 5 \times 2 = 120 \)</span> possible subcommittees.
</li>
</ul>

<h4>
Example 2c
</h4>

<ul>
<li>
How many different 7-place license plates are possible if the first 3 places are to be occupied by letters and the final 4 by numbers?
</li>
<li>
Solution. By the generalized version of the basic principle, the answer is <span class="math">\(26 \times 26 \times 26 \times 10 \times 10 \times 10 \times 10=175,760,000\)</span>.
</li>
</ul>

<h4>
Example 2d
</h4>

<ul>
<li>
How many functions defined on n points are possible if each functional value is either 0 or 1?
</li>
<li>
Solution. Let the points be 1,2,...,n. Since <span class="math">\(f(i)\)</span> must be either 0 or 1 for each i = 1, 2, . . . , n, it follows that there are <span class="math">\(2_n\)</span> possible functions.
</li>
</ul>

<h4>
Example 2e
</h4>

<ul>
<li>
In Example 2c, how many license plates would be possible if repetition among letters or numbers were prohibited?
</li>
<li>
Solution. In this case, there would be <span class="math">\(26 \times 25 \times 24 \times 10 \times 9 \times 8 \times 7 = 78,624,000\)</span> possible license plates.
</li>
</ul>

<h3>
1.3 Permutations
</h3>

<ul>
<li>
How many different ordered arrangements of the letters a, b, and c are possible?
</li>
<li>
By direct enumeration we see that there are 6, namely, abc, acb, bac, bca, cab, and cba.
</li>
<li>
Each arrangement is known as a <strong>permutation</strong>. Thus, there are 6 possible permutations of a set of 3 objects.
</li>
</ul>

<p>
<span class="math">\(n(n − 1)(n − 2) … 3 \times 2 \times 1 = n!\)</span>
</p>

<h4>
Example 3a
</h4>

<ul>
<li>
How many different batting orders are possible for a baseball team consisting of 9 players?
</li>
<li>
Solution. There are 9! = 362,880 possible batting orders.
</li>
</ul>

<h4>
Example 3b
</h4>

<ul>
<li><p>
A class in probability theory consists of 6 men and 4 women. An examination is given, and the students are ranked according to their performance. Assume that no two students obtain the same score.
</p>

<ol>
<li>
How many different rankings are possible?
</li>
<li>
If the men are ranked just among themselves and the women just among themselves, how many different rankings are possible?
</li>
</ol></li>
<li><p>
Solution.
</p>

<ol>
<li>
Because each ranking corresponds to a particular ordered arrangement of the 10 people, the answer to this part is 10! = 3,628,800.
</li>
<li>
Since there are 6! possible rankings of the men among themselves and 4! possi- ble rankings of the women among themselves, it follows from the basic principle that there are (6!)(4!) = (720)(24) = 17,280 possible rankings in this case.
</li>
</ol></li>
</ul>

<h4>
Example 3c
</h4>

<ul>
<li>
Ms. Jones has 10 books that she is going to put on her bookshelf. Of these, 4 are mathematics books, 3 are chemistry books, 2 are history books, and 1 is a language book. Ms. Jones wants to arrange her books so that all the books dealing with the same subject are together on the shelf. How many different arrangements are possible?
</li>
<li>
Solution. There are 4! 3! 2! 1! arrangements such that the mathematics books are first in line, then the chemistry books, then the history books, and then the language book. Similarly, for each possible ordering of the subjects, there are 4! 3! 2! 1! possible arrangements. Hence, as there are 4! possible orderings of the subjects, the desired answer is 4! 4! 3! 2! 1! = 6912.
</li>
</ul>

<h4>
Example 3d
</h4>

<ul>
<li>
How many different letter arrangements can be formed from the letters PEPPER?
</li>
<li>
Solution. We first note that there are 6! permutations of the letters <span class="math">\(P_1E_1P_2P_3E_2R\)</span> when the 3P’s and the 2E’s are distinguished from each other. However, consider any one of these permutations—for instance, P1P2E1P3E2R. If we now permute the P’s among themselves and the E’s among themselves, then the resultant arrangement would still be of the form PPEPER.
</li>
</ul>

<h4>
Example 3e
</h4>

<ul>
<li>
A chess tournament has 10 competitors, of which 4 are Russian, 3 are from the United States, 2 are from Great Britain, and 1 is from Brazil. If the tournament result lists just the nationalities of the players in the order in which they placed, how many outcomes are possible?
</li>
<li>
Solution: There are $  = 12600 $ possible outcomes
</li>
</ul>

<h4>
Example 3f
</h4>

<ul>
<li>
How many different signals, each consisting of 9 flags hung in a line, can be made from a set of 4 white flags, 3 red flags, and 2 blue flags if all flags of the same color are identical?
</li>
<li>
Solution: There are $  = 1260 $ possible outcomes
</li>
</ul>

<h3>
2.4 Some Simple Propositions (after proposition 4.3)
</h3>

<h2>
February 4th, 2013 - Lecture: Combinations
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Deriving the formula for combinations.
</li>
<li>
Basic identities and examples.
</li>
<li>
The antenna problem (Ex. 4c of Ross).
</li>
<li>
Counting divisions where all sets non-empty and where sets are allowed to be empty.
</li>
</ul>

<h3>
Combinations
</h3>

<ul>
<li>
How many ways can we choose k out of n objects when order does not matter?
</li>
<li>
Take S = {<span class="math">\(x_1, ... , x_n\)</span>}. How many k-element subsets does S have?
</li>
<li>
Call the number of k element subsets C(n,k)
</li>
<li><p>
In general,
</p>

<ul>
<li>
C(n,1) = n
</li>
<li>
C(n,n) = 1
</li>
<li>
C(n,2) = # of 2-element subsets
</li>
</ul></li>
<li><p>
S = {<span class="math">\(x_1, ... , x_n\)</span>}
</p>

<ul>
<li>
From every 2 element list -&gt; n(n - 1)
</li>
</ul></li>
<li><p>
Number of subsets will equal <span class="math">\[ \frac{\mathrm{number &amp;#58; of &amp;#58; 2 &amp;#58; element &amp;#58; sets}}{2} = \frac{n(n - 1)}{2} \]</span>
</p></li>
<li><p>
C(n,k) for general k (S = {<span class="math">\(x_1, ... , x_n\)</span>})
</p>

<ul>
<li>
<span class="math">\(\frac{n!}{(n - k)!} \)</span> ways to list k-out-of-n elements
</li>
<li>
Every k-element subset is represented k! times in big list
</li>
<li>
Big list has <span class="math">\(\frac{n!}{(n-k)!} \)</span> entries
</li>
<li>
$ C(n,k) k! =  <span class="math">\(&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Notation: \)</span> {{n} } = $ <span class="math">\[ {n \choose 1} = \frac{n!}{(n-1)!1!} = n\]</span>
</p></li>
<li>
Claim: $ {n k} = {n n-k} $ <span class="math">\[ {n \choose n - k} = \frac{n!}{(n - (n - k))!(n - k)!} = \]</span> <span class="math">\[\frac{n!}{k!(n - k)!} = {n \choose k} \]</span>
</li>
<li>
The number of possible 5-card poker hands is: $ {52 5} = 2578960 $
</li>
</ul>

<h2>
February 4th, 2013 - Reading
</h2>

<h3>
1.4 Combinations (through example 4c)
</h3>

<ul>
<li><p>
How many groups of <em>r</em> objects can be formed from a total of <em>n</em> objects?
</p>

<ul>
<li>
How many groups of 3 could be formed from the 5 items <em>A, B, C, D,</em> and <em>E</em>.
</li>
<li>
Since there are 5 ways to select the initial item, 4 ways to select the next item, and 3 ways to select the final item, there are thus <span class="math">\(5 \times 4 \times 3 \)</span> ways of selecting the group of 3 (order relevant).
</li>
<li>
In this formulation, each group as a set will be counted 6 times.
</li>
<li>
The total number of groups that can be formed is: <span class="math">\[ \frac{5 \times 4 \times 3}{3 \times 2 \times 1} = 10\]</span>
</li>
<li>
In general, <em>n(n - 1) … (n - r + 1)</em> represents the number of different ways that a group of <em>r</em> items could be selected from <em>n</em> items when order is relevant.
</li>
<li>
It follows that the number of different groups of <em>r</em> items that could be formed from a set of <em>n</em> items is <span class="math">\[ \frac{n(n-1) … (n - r + 1)}{r!} = \frac{n!}{(n - r)!r!}\]</span>
</li>
</ul></li>
<li><p>
<strong>Notation and terminology</strong>
</p>

<ul>
<li>
We define $ n r <span class="math">\(, for \)</span> r n $ by <span class="math">\[ {n \choose r} = \frac{n!}{(n - r)!r!}\]</span> and say that $ n r $ represents the number of possible combinations of <em>n</em> objects taken <em>r</em> at a time.
</li>
</ul></li>
</ul>

<h4>
Example 4a
</h4>

<ul>
<li>
A committee of 3 is to be formed from a group of 20 people. How many different committees are possible?
</li>
<li>
Solution: There are <span class="math">\({20 \choose 3} = \frac{20 \cdot 19 \cdot 18}{3 \cdot 2 \cdot 1} = 1140 \)</span> possible combinations.
</li>
</ul>

<h4>
Example 4b
</h4>

<ul>
<li>
<p>From a group of 5 women and 7 men, (1) how many different committees consisting of 2 women and 3 men can be formed? (2) What if 2 of the men are feuding and refuse to serve on the committee together?</p>
<ol>
<li>
Solution: $ {5 2}{7 3} = 350 $ possible combinations of 2 women and 3 men.
</li>
<li>
I'll ask about this problem in office hours.
</li>
</ol></li>
</ul>

<h4>
Example 4c
</h4>

<ul>
<li>
Consider a set of <em>n</em> antennas of which m are defective and <em>n − m</em> are functional and assume that all of the defectives and all of the functionals are considered indistinguishable. How many linear orderings are there in which no two defectives are consecutive?
</li>
<li>
Solution: <span class="math">\[ n - m + 1 \choose m \]</span> possible orders in which there is at least once functional antenna between any two defective ones.
</li>
</ul>

<h3>
1.6 The Number of Integer Solutions of Equations
</h3>

<h2>
February 6th, 2013 - Lecture: Binomial and Multinomial Coefficients
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Pascal's relationship and Pascal's triangle.
</li>
<li>
The binomial theorem and proofs by combinatorial reasoning and induction.
</li>
<li>
Some consequences and examples.
</li>
<li>
Multinomial coefficients and their applications to dividing groups and counting strings.
</li>
</ul>

<h3>
Introduction
</h3>

<ul>
<li><p>
For all <em>n, k &gt;= 0</em>. <span class="math">\[
{n \choose k} = {n - 1 \choose k - 1} + {n - 1 \choose k}
\]</span>
</p></li>
<li><p>
Proof:
</p>

<ul>
<li>
<span class="math">\({n \choose k}\)</span> = the number of k-elements of subsets of <span class="math">\(x&amp;#95;1 ... x&amp;#95;n \)</span>.
</li>
<li>
Observation: Take any of the <span class="math">\(x&amp;#95;1 .. x&amp;#95;n \)</span>, say <span class="math">\(x_n\)</span>. Then every k-element subset either cointains <span class="math">\(x_n\)</span> or it doesn't.
</li>
<li>
<span class="math">\(n - 1 \choose k - 1\)</span> is the number that do contain <span class="math">\(x_n\)</span> and <span class="math">\(n - 1 \choose k \)</span> is the number that don't.
</li>
<li>
The number that do contain <span class="math">\(x_n\)</span> is <span class="math">\(n - 1 \choose k - 1 \)</span>.
</li>
</ul></li>
<li><p>
Pascal's Triangle
</p>

<ul>
<li>
Put $ 0 0 $ at the top.
</li>
<li>
The next level is two instances of <span class="math">\(1 \choose 0 \)</span>
</li>
<li><p>
...
</p>

<pre><code>            1
        1   2   1
    1     3   3    1
1     4     6   4    1
</code></pre></li>
</ul></li>
</ul>

<h3>
Binomial Theorem
</h3>

<ul>
<li>
For all <em>x, y</em>, all <em>n &gt;= 1</em>, <span class="math">\[
(x + y)^n = \sum_{k = 0}^n {n \choose k}x^ky^{n - k}
\]</span>
</li>
<li><p>
Intuitive Combinatorial Proof (Simpler)
</p>

<ul>
<li>
When you expand <span class="math">\((x + y)^n\)</span>, it equals <span class="math">\((x + y)(x+y) ... (x + y) \)</span> &quot;<em>n</em>-times&quot;
</li>
<li>
One way to compute all &quot;monomials&quot; in product is to start with <em>x</em>, and then pick all the way done, one per time, and repeat the exercise until all combinations are exhausted, and you have every monomial.
</li>
<li>
Which monomials show up in sum?
</li>
<li>
<span class="math">\(x^a y^b\)</span> shows up <span class="math">\(n \choose a\)</span>
</li>
</ul></li>
<li><p>
Proof by induction
</p>

<ul>
<li>
We've already checked <em>n = 1</em>.
</li>
<li>
Now assume true for <em>n - 1</em> and prove for <em>n &gt; 1</em> <span class="math">\[ (x+y)^n = (x+y)(x+y)^{n-1} \]</span> <span class="math">\[= (x+y)\cdot\sum_{k=0}^{n-1}{n - 1 \choose k}x^k y^{n - 1 -k}\]</span> <span class="math">\[= x\sum_{k = 0}^{n - 1}{n - 1 \choose k}x^k y^{n - 1 - k} + \]</span> <span class="math">\[= y\sum_{k = 0}^{n - 1}{n - 1 \choose k}x^k y^{n - 1 - k} \]</span>
</li>
<li>
Write <em>i</em> for <em>k + 1</em>: <span class="math">\[\sum_{i = 1}^n {n - 1 \choose i - 1}x^i y^{n - 1} + \]</span> <span class="math">\[\sum_{i = 1}^n {i \choose n - 1}x^i y^{n - 1}\]</span> <span class="math">\[ = x^n + \sum_{i = 1}^{n - 1} \left[ {n - 1 \choose i - 1} + {n - 1 \choose i} \right] \times\]</span> <span class="math">\[x^i y^{n - i} + y^n \]</span>
</li>
</ul></li>
<li><p>
Corollary: For any <em>n &gt;= 1</em>, <span class="math">\(\sum&amp;#95;{k = 0}^{n}{n \choose k} = 2^n \)</span>
</p>

<ul>
<li>
Proof: <span class="math">\[2^n = (1 + 1)^n = \sum_{k = 0}^n{n \choose k}\]</span>
</li>
<li>
<p>Intuition: This sum should be the number of wqys to pick a subset of any size from a set of size <em>n</em>.</p>
<ul>
<li>
To pick a subset, we have <em>n</em> decisions where we decide if each <span class="math">\(x_i\)</span> is in the subset or not.
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Multinomial Coefficients
</h3>

<ul>
<li><p>
Arise from the following type of problem:
</p>

<ul>
<li>
Given a set of <em>n</em> distinct items, you need to divide them up into <em>r</em> groups of given sites <span class="math">\(n&amp;#95;1, n&amp;#95;2, ..., n&amp;#95;r \)</span> where <span class="math">\(n&amp;#95;1 + n&amp;#95;2 + ... + n&amp;#95;r = n \)</span>.
</li>
<li>
How many ways can this be done, for a given <em>n</em>, <em>r</em>, and <span class="math">\(n_1\)</span> up to <span class="math">\(n_r\)</span> where there all add up to <em>n</em>.
</li>
</ul></li>
<li><p>
This is very close to something we've already seen, and there are two ways of thinking about it.
</p>

<ol>
<li>
How many choices for 1st group? Then into the second, then into the third ... <span class="math">\[ n \choose n_1 \]</span> ways to choose the first group <span class="math">\[ n - n_1 \choose n_2 \]</span> ways to choose the second group, etc, and for the <span class="math">\(r^{th}\)</span> group: <span class="math">\[n - n_1 - n_2 - ... - n_{r-1} \choose n_r = 1 \]</span> so the multiplication principle tells us that if we multiply these all together, we get our answer. It cancels nicely: <span class="math">\[=\frac{n!}{(n - n_1)!n!}\cdot\frac{n-n_1}{(n - n_1 - n_2)!n!} ...  \]</span> and this cancels to <span class="math">\[\frac{n!}{n_1!n_2!...n_r!}\]</span>
</li>
</ol></li>
<li><p>
Example: There are 10 TAs in the department, and we need 3 TAs for OSs, 2 for algorithms, and 5 for databases. There there are: <span class="math">\[ \frac{10!}{3!2!5!} \]</span> ways to assign them, which is <span class="math">\(2520\)</span>.
</p></li>
</ul>

<h2>
February 6th, 2013 - Recitation
</h2>

<ol>
<li><p>
Determine the number of vectors <span class="math">\(x&amp;#95;1, x&amp;#95;2, x&amp;#95;3, ..., x&amp;#95;n \)</span> such that each <span class="math">\(x_i\)</span> is either 0 or 1. <span class="math">\[ \sum_{i = 1}^{n} x_i \le k \]</span> (constraint)
</p>

<ul>
<li>
<p>Solution <span class="math">\[x_i \in \lbrace 0, 1 \rbrace  \]</span></p>
<ul>
<li>
The set should be less than <span class="math">\(2^n\)</span>
</li>
<li>
<p>Three cases:</p>
<ol>
<li>
<em>n = k</em>: everything should be 1
</li>
<li>
<em>n &lt; k</em>: everything something 0
</li>
<li>
<em>n &gt; k</em>: the only interesting <em>k</em>, $ n k<span class="math">\(&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prove that \)</span><span class="math">\({n + m \choose r} = {n \choose 0}{m \choose r} + \)</span>$ <span class="math">\[{n \choose 1 }{m \choose r - 1} ... + {n\choose r} {m \choose 0} \]</span>
</p></li>
</ol>

<h2>
February 6th, 2013 - Reading
</h2>

<h3>
1.5 Multinomial Coefficients
</h3>

<ul>
<li>
<p><strong>Notation</strong></p>
<ul>
<li>
<p>If $ n_1 + n_2 + … + n_r = n <span class="math">\(, we define \)</span>n n_1</p>
<ul>
<li>
n_2 + … + n_r $ by
</li>
</ul></li>
</ul></li>
</ul>

<p>
<span class="math">\[ {n \choose n_1 + n_2 + … + n_r} = \frac{n!}{n_1! + n_2! + … + n_r!} \]</span> + Thus, <span class="math">\(n \choose n&amp;#95;1 + n&amp;#95;2 + … + n&amp;#95;r \)</span> represents the number of possible divisions of <em>n</em> distinct objects into r distinct groups of respective sizes <span class="math">\(n&amp;#95;1, n&amp;#95;2, … , n&amp;#95;r \)</span>.
</p>

<ul>
<li>
<strong>The multinomial theorem</strong> <span class="math">\[ (x_1 + x_2 + … + x_r)^n = \]</span> <span class="math">\[ \sum_{(n_1, …, n_r) : } \]</span>
</li>
</ul>

<h2>
February 7th, 2013 - Homework 2
</h2>

<ol>
<li><p>
(8 points) Prove that <span class="math">\(P(A \cup B) \le P(A) + P(B)\)</span> for any events A and B. Prove the general version by induction, which says that if <span class="math">\(A_1, ..., A_n\)</span> are events then <span class="math">\(P(\bigcup_{i = 1}^n A_i) \le P(\sum_{i = 1}^n A_i)\)</span>. When does this inequality become an equality?
</p>

<ul>
<li><p>
Base case
</p>

<p>
<span class="math">\[P(A_1 \cup A_2) \le P(A_1) + P(A_2)\]</span>
</p></li>
<li><p>
Assume it holds for a general case <span class="math">\((n=k)\)</span>
</p>

<p>
<span class="math">\[P\left(\bigcup_{i = 1}^k A_i\right) \le \sum_{i = 1}^k P(A_i)\]</span>
</p>

<p>
<span class="math">\[P(A_1 \cup A_2 \cup ... \cup A_k) \le P(A_1) + P(A_2) + ... + P(A_k)\]</span>
</p></li>
<li><p>
Prove that it hold for next case using your assumption.
</p>

<p>
<span class="math">\[P\left(\bigcup_{i = 1}^{k+1} A_i\right) \le \sum_{i = 1}^{k+1} P(A_i)\]</span>
</p>

<p>
<span class="math">\[P(A_1 \cup A_2 \cup ... \cup A_k \cup A_{k+1}) \le P(A_1) + P(A_2) + ... + P(A_k) + P(A_{k+1})\]</span>
</p>

<p>
<span class="math">\[(A_1 \cup A_2 \cup ... \cup A_k) = C\]</span>
</p>

<p>
<span class="math">\[P(A_1) + P(A_2) + ... + P(A_k) = D\]</span>
</p>

<p>
<span class="math">\[P(C \cup A_{k+1}) \le P(D) + P(A_{k+1})\]</span>
</p>

<p>
By the base case, this is true, and expression is proven for all events.
</p></li>
</ul></li>
<li><p>
(4 points) If <span class="math">\(P(A) = \frac{1}{2}\)</span>, <span class="math">\(P(B) = \frac{1}{5}\)</span>, and <span class="math">\(P(A\cup B) = 3/5\)</span>, what are <span class="math">\(P(A\cap B)\)</span>, <span class="math">\(P(A^c \cup B)\)</span>, and <span class="math">\(P(A^c \cap B)\)</span>?
</p>

<ul>
<li><p>
<span class="math">\(P(A\cap B) = \frac{1}{10}\)</span>
</p></li>
<li><p>
<span class="math">\(P(A^c \cup B) = 3/5\)</span> because the probability of <span class="math">\(A\)</span> is the same as <span class="math">\(A^c\)</span>
</p></li>
<li><p>
<span class="math">\(P(A^c \cap B) = \frac{1}{10}\)</span>
</p></li>
</ul></li>
<li><p>
(3 points) How many elements are there in the set
</p>

<p>
{<span class="math">\(x : 10^7 \le x \le 10^8\)</span>, and the base 10 representation of x has no digit used twice}?
</p>

<ul>
<li><p>
<span class="math">\(10^7 = 10000000\)</span>, and <span class="math">\(10^8 = 100000000\)</span>
</p></li>
<li><p>
The smallest number possible is <span class="math">\(12345678\)</span>, the greatest number possible is <span class="math">\(98765432\)</span>
</p></li>
<li><p>
For the first number in any element, the first option is {1, 2, 3, 4, 5, 6, 7, 8, 9}.
</p></li>
<li><p>
Then for the every second number, it’s every number besides the first choice plus 0.
</p></li>
<li><p>
The “tree” looks as follows, where <span class="math">\(i\neq j\neq k\neq l\neq m\neq n\neq o\neq p\)</span>:
</p>

<ol>
<li><p>
<span class="math">\[&amp;#123;1 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,2,3,4,5,6,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;2 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,3,4,5,6,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;3 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,4,5,6,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;4 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,5,6,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;5 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,4,6,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;6 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,4,5,7,8,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;7 i j k l m n o &amp;#125;\]</span> where <span class="math">\(&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,4,5,6,8,9&amp;#125;\)</span><span class="math">\(&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;\)</span><span class="math">\(&amp;#123;8 i j k l m n o &amp;#125;\)</span>$ where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,4,5,6,7,9&amp;#125;\]</span>
</p></li>
<li><p>
<span class="math">\[&amp;#123;9 i j k l m n o &amp;#125;\]</span> where <span class="math">\[&amp;#123;i,j,k,l,m,n,o&amp;#125; \in &amp;#123;0,1,2,3,4,5,6,7,8&amp;#125;\]</span>
</p></li>
</ol></li>
<li><p>
Every “node” (a) through (i) is <span class="math">\(9 \choose 1\)</span>.
</p></li>
<li><p>
When you pick the first number, you still have 9 choices because zero is added. When you choose the second number, you have 8 choices because a number is taken out and non are put back into the set of choices. When you pick your third number, you have 7 choices because a number is taken out and none put back in. ...
</p></li>
<li><p>
So for integer strings <span class="math">\(i\)</span> through <span class="math">\(o\)</span>:
</p>

<p>
<span class="math">\[[i]\quad\quad[j]\quad\quad[k]\quad\quad[l]\quad\quad[m]\quad\quad[n]\quad\quad[o]\quad\quad[p]\]</span>
</p></li>
<li><p>
The number of elements is equal to
</p>

<p>
<span class="math">\[{9 \choose 1}\times{9 \choose 1}\times{8 \choose 1}\times{7 \choose 1}\times{6 \choose 1}\times{5 \choose 1}\times{4 \choose 1}\times{3 \choose 1}\]</span>
</p></li>
</ul></li>
<li><p>
(3 points) An army output has 19 posts to staff using 30 indistinguishable guards. How many ways are there to distribute the guards if no post is left empty?
</p>

<ul>
<li><p>
This is a “stars and bars” problem, where the number of “stars” <span class="math">\(k\)</span> is equal to 30, and the number of “bars,” “bins,” or “posts” <span class="math">\(n\)</span> is equal to 19.
</p>

<p>
<span class="math">\[{k - 1 \choose n - 1} = {30 - 1 \choose 19 - 1} = {29 \choose 18} = 34597290\]</span>
</p></li>
</ul></li>
<li><p>
(1 point) What is the coefficient of <span class="math">\(x^{10}y^{13}\)</span> when <span class="math">\((x + y)^{23}\)</span> is expanded?
</p>

<ul>
<li><p>
This problem requires the binomial theorem, where <span class="math">\(n = 23\)</span>:
</p>

<p>
<span class="math">\[(x + y)^{23} = \sum_{k = 0}^{23} {23 \choose k}x^ky^{23 - k}\]</span>
</p></li>
<li><p>
When <span class="math">\(k = 10\)</span>, <span class="math">\(23 - k\)</span> will equal 13.
</p>

<p>
<span class="math">\[{23 \choose 10}x^{10} y^{13}\]</span>
</p></li>
<li><p>
So the coefficient for <span class="math">\(x^{10} y^{13}\)</span> will be
</p>

<p>
<span class="math">\[{23 \choose 10} = 1144066\]</span>
</p></li>
</ul></li>
<li><p>
(4 points) What is the coefficient of <span class="math">\(w^{9}x^{31}y^{4}z^{19}\)</span> when <span class="math">\((w + x + y + z)^{63}\)</span> is expanded? How many monomials appear in the expansion?
</p>

<ul>
<li><p>
This is a multinomial coefficient problem where <span class="math">\(k = 4\)</span> and <span class="math">\(n = 63\)</span>:
</p>

<p>
<span class="math">\[(a_1+a_2+...+a_k)^n=\sum_{\substack{n_1,n_2,...,n_k\ge 0 &amp;#92; 
n_1+n_2+...+n_k=n}}\frac{n!}{n_1!n_2!...n_k!}a_1^{n_1}a_2^{n_2}...a_k^{n_k}\]</span>
</p></li>
<li><p>
Set <span class="math">\(n_1 = 9\)</span>, <span class="math">\(n_2 =31\)</span>, <span class="math">\(n_3 = 4\)</span>, and <span class="math">\(n_4 = 19\)</span>.
</p>

<p>
<span class="math">\[\left(\frac{63!}{9!31!4!19!}\right)\times w^{9}x^{31}y^{4}z^{19}\]</span>
</p></li>
<li><p>
Therefore, the coefficient is
</p>

<p>
<span class="math">\[\frac{63!}{9!31!4!19!}\]</span>
</p></li>
<li><p>
The number of monomials in a multinomial coefficient can be expressed as a “stars and bars” problem.
</p>

<p>
<span class="math">\[66 \choose 3\]</span>
</p></li>
</ul></li>
<li><p>
(6 points) Let <span class="math">\(p\)</span> be a prime number and <span class="math">\(1 \le k \le p - 1\)</span>. Prove that <span class="math">\(p \choose k\)</span> is a multiple of <span class="math">\(p\)</span>. Show that this is not true if <span class="math">\(p\)</span> is not prime.
</p>

<p>
<span class="math">\[{p\choose k} = \frac{p(p-1) \times ... \times (p - k + 1)}{1 \times 2 \times ... \times (k - 1) \times k} = p{p - 1 \choose k}\]</span>
</p>

<p>
<span class="math">\[{4\choose 2} = 6\]</span>
</p>

<p>
6 is not a multiple of 4 and is not prime.
</p></li>
<li><p>
Verify that for any <span class="math">\(n \ge k \ge 1\)</span>
</p>

<p>
<span class="math">\[{n \choose 2} = {k \choose 2} + k(n - k) + {n - k \choose 2}\]</span>
</p>

<p>
Then give a combinatorial argument for why this is true.
</p>

<ul>
<li><p>
Observe that the binomial coefficient with two for all reals:
</p>

<p>
<span class="math">\[{r\choose 2} = \frac{r (r - 1)}{ 2}\]</span>
</p></li>
<li><p>
Apply fact to both sides:
</p>

<p>
<span class="math">\[\frac{n (n - 1)}{ 2} = \frac{k (k - 1)}{ 2} + k(n - k) + \frac{(n - k) (n - k - 1)}{ 2}\]</span>
</p></li>
<li><p>
Multiply everything by two
</p>

<p>
<span class="math">\[n (n - 1) = k (k - 1) + 2k(n - k) + (n - k)(n - k - 1)\]</span>
</p></li>
<li><p>
Expand out
</p>

<p>
<span class="math">\[n (n - 1) = k^2 - k + 2kn - 2k^2 + k+k^2-n-2 k n+n^2\]</span>
</p></li>
<li><p>
Simplify
</p>

<p>
<span class="math">\[n (n - 1) = -n+n^2\]</span>
</p></li>
<li><p>
Change form
</p>

<p>
<span class="math">\[n (n - 1) = n (n - 1)\]</span>
</p></li>
<li><p>
Combinatorial argument:
</p>

<ul>
<li><p>
<span class="math">\(n \choose 2\)</span> is the number of ways we can arrange <span class="math">\(n\)</span> objects into groups of 2.
</p></li>
<li><p>
Splitting <span class="math">\(n\)</span> into 2, one of those groups will be of size <span class="math">\(k\)</span>, making the other set of size <span class="math">\(n - k\)</span>
</p></li>
<li><p>
Using the new partitions <span class="math">\(k\)</span> and <span class="math">\(n-k\)</span>, arrange <span class="math">\(n\)</span> “things” into groups of two.
</p></li>
<li><p>
The first partition is of length <span class="math">\(k\)</span>, resulting in <span class="math">\(k\choose 2\)</span> ways, and for the second part, we have <span class="math">\(n-k\)</span> choose 2 ways, explaining the first and last terms.
</p></li>
<li><p>
Now choose 2 “things”, one from different groups of 2, we can choose 1 of the k objects and all of <span class="math">\(n-k\)</span> “things”, which is equal to <span class="math">\(k(n-k)\)</span>.
</p></li>
</ul></li>
</ul></li>
</ol>

<h2>
February 11th, 2013 - Lecture: Multinomial Theorem, Counting Problems, Inclusion-Exclusion
</h2>

<h3>
Topics
</h3>

<ul>
<li>
The multinomial theorem and applications
</li>
<li>
The &quot;tree method&quot; for various counting problems involving card hands and committees
</li>
<li>
Inclusion-exclusion
</li>
</ul>

<h3>
Lecture
</h3>

<ul>
<li><p>
<strong>Multinomial coefficients</strong>: see reading 1.5 in February 7th reading
</p>

<ul>
<li><p>
<strong>Multinomial theorem</strong>: see reading 1.5 in February 7th reading
</p>

<ul>
<li>
Sum runs over all vectors of non-negative integers that sum to <em>n</em>.
</li>
<li><p>
Example: $ (x_1 + x_2 + x_3)^2 $ = <span class="math">\[\sum_{(n_1, n_2, n_3) : n_1, n_2, n_3 = 2} {2 \choose n_1, n_2, n_3} x_{1}^{n_1} x_{2}^{n_2} x_{3}^{n_3} \]</span> <span class="math">\[ = {2 \choose 2, 0, 0} x_{1}^{2} x_{2}^{0} x_{3}^{0} +  \]</span> <span class="math">\[ {2 \choose 0, 2, 0} x_{1}^{0} x_{2}^{2} x_{3}^{0} + \]</span> <span class="math">\[ {2 \choose 0, 0, 2} x_{1}^{0} x_{2}^{0} x_{3}^{2} + \]</span> <span class="math">\[ {2 \choose 1, 1, 0} x_{1}^{1} x_{2}^{1} x_{3}^{0} + \]</span> <span class="math">\[ {2 \choose 1, 0, 1} x_{1}^{1} x_{2}^{0} x_{3}^{1} + \]</span> <span class="math">\[ {2 \choose 0, 1, 1} x_{1}^{0} x_{2}^{1} x_{3}^{1} \]</span>
</p></li>
<li><p>
Example: What is the coefficient of <span class="math">\(x^3 y^4 z\)</span> when <span class="math">\((x + y + z)^13 \)</span> is expanded? It corresponds to: <span class="math">\[ {13 \choose 3, 9, 1} = 2860 \]</span>
</p></li>
<li>
Question: How many terms are in the sum? Equivilently, how many monomials are in the expansion? (If <em>n = 2</em>, <em>r = 3</em>, then 6)
</li>
</ul></li>
</ul></li>
<li><p>
<strong>More on counting</strong>
</p>

<ul>
<li>
Deck of cards: 52 cards, 4 suits, 13 ranks, one of each suit/rank combination
</li>
<li>
Number of 5 card hands: <span class="math">\[ 53 \choose 5 \]</span>
</li>
<li><p>
Number of flushes:
</p>

<ul>
<li>
4 different suits for flushes
</li>
<li>
For each suit, the number of flushes is $ 13 5 <span class="math">\(&lt;/li&gt; &lt;li&gt;Therefore, \)</span><span class="math">\(4 \times {13 \choose 5}\)</span>$
</li>
</ul></li>
<li><p>
Example: Suppose a committe of <em>k</em> people from a group of 7 women and 4 men.
</p>

<ul>
<li><p>
How many ways can form the committee if it has:
</p>

<ol>
<li>
3 women and 3 men exactly <span class="math">\[{7 \choose 3} \times {4 \choose 3} \]</span>
</li>
<li><p>
The committee is any size, but it has equal number of men and women. Choices:
</p>

<ol>
<li>
1 each <span class="math">\[= {7 \choose 1} \times {4 \choose 1} + \]</span>
</li>
<li>
2 each <span class="math">\[{7 \choose 2} \times {4 \choose 2} + \]</span>
</li>
<li>
3 each <span class="math">\[{7 \choose 3} \times {4 \choose 3} + \]</span>
</li>
<li>
4 each <span class="math">\[{7 \choose 4} \times {4 \choose 4} \]</span>
</li>
</ol></li>
<li><p>
If the commitee has 4 people, at least 2 of which are women?
</p>

<ul>
<li><p>
Steps:
</p>

<ol>
<li>
Pick the number of women
</li>
<li>
Pick the women
</li>
<li>
Pick men for the remaining spots.
</li>
</ol></li>
<li><p>
Possibilities:
</p>

<ol>
<li>
2 women <span class="math">\[= {7 \choose 2} \times {4 \choose 2} + \]</span>
</li>
<li>
3 women <span class="math">\[{7 \choose 3} \times {4 \choose 1} + \]</span>
</li>
<li>
4 women <span class="math">\[{7 \choose 4} \times {4 \choose 0} \]</span>
</li>
</ol></li>
</ul></li>
</ol></li>
</ul></li>
<li><p>
Example: How many hands have exactly 3 aces? &quot;Pick 3 aces, then pick 2 non-aces&quot; <span class="math">\[{4 \choose 3}\times{48 \choose 2}\]</span>
</p></li>
<li>
<p>Example: How many full houses?</p>
<ul>
<li>
Pick the rank of 3-of-a-kind (AAA, 222, ...)
</li>
<li>
Pick the rank of 2-of-a-kind
</li>
<li>
Pick the B-of-a-kind
</li>
<li>
Pick the 2-of-a-kind <span class="math">\[13 \times 12 \times {4 \choose 3}\times {4 \choose 2} \]</span>
</li>
</ul></li>
</ul></li>
<li><p>
Probability theory <span class="math">\[ P(A \cup B \cup C) = \]</span> <span class="math">\[ P(A) + P(B) + P(C) - \]</span> <span class="math">\[ P(A \cap B) - P(A \cap C) -  \]</span> <span class="math">\[ P(A \cap C) + P(A \cap B \cap C) \]</span>
</p>

<ul>
<li><p>
If P is a probability measure an $ A_1, ... A_n $ are events: <span class="math">\[P(A_1 \cup A_2 \cup ... \cup A_n) = \]</span> <span class="math">\[\sum_{i = 1}^n P(A) - \sum_{i \lt j} P(A_i \cap A_j) + \]</span> <span class="math">\[ \sum_{i \lt j \lt k}P(A_i \cap A_j \cap A_k) - ... \]</span> <span class="math">\[+ (-1)^{n + 1} P(A_1 \cap ... A_n)\]</span>
</p></li>
<li><p>
Include/Exclude Set Version
</p>

<ul>
<li>
Some formula, but with set-size intead of P(1) <span class="math">\[|A_1 \cup ... \cup A_n| = \]</span> <span class="math">\[\sum|A_1| - \sum|A_i \cap A_j \]</span> <span class="math">\[\sum A_i \cap A_j \cap A_k \]</span>
</li>
</ul></li>
<li><p>
Symmetry: For any <em>i</em>, |A_i| is the same. <span class="math">\[A_i = {39 \choose 5} \]</span>
</p>

<ul>
<li>
For any <span class="math">\(i \le j \)</span>, <span class="math">\(|A_i \cap A_j|\)</span> is the same <span class="math">\[26 \choose 5 \]</span>
</li>
</ul></li>
</ul></li>
</ul>

<h2>
February 11th, 2013 - Reading
</h2>

<h3>
2.4 Some Simple Propositions (from page 31)
</h3>

<h2>
February 13th, 2013 - Lecture
</h2>

<h3>
Include/Exclude Formula
</h3>

<ul>
<li>
If <span class="math">\(E_1, ..., E_n\)</span> be events, then
</li>
</ul>

<p>
<span class="math">\[P(E_1 \cup E_2 \cup ... \cup E_n) = \]</span>
</p>

<p>
<span class="math">\[\sum_{i = 1}^n P(E_1) - \sum_i \le j P(E_i \land E_j) + \]</span>
</p>

<p>
<span class="math">\[\sum_{i \lt j \lt k}P(E_i \cap E_j \cap E_K - ... \]</span>
</p>

<p>
<span class="math">\[(-1)^{n+1} P(E_i \cap ... E_n) \]</span>
</p>

<h3>
de MontMort's Problem (1713)
</h3>

<ul>
<li>
Have n students seated in class, no empty seats.
</li>
<li>
Students 1 through n.
</li>
<li>
Everyone stands up, all are assigned, random seats.
</li>
<li>
What's the probability that somebody gets own seat?
</li>
</ul>

<h4>
Dearrangements
</h4>

<ul>
<li>
$ E = $ &quot;someone gets own seat&quot;
</li>
<li>
<span class="math">\(E&amp;#95;i = \)</span> &quot;student i gets own seat&quot;
</li>
</ul>

<p>
<span class="math">\[ E = E_1 \cup E_2 \cup ... \cup E_n \]</span>
</p>

<h4>
Exploit &quot;symmetry&quot;
</h4>

<p>
<span class="math">\[P(E) = P(E_1 \cup E_2 \cup ... \cup E_n) = \]</span>
</p>

<ul>
<li>
Observe what is <span class="math">\(P(E&amp;#95;1) = \frac{1}{n} = \frac{n-1}{n!} \)</span>
</li>
<li><p>
Moreover, <span class="math">\(P(E&amp;#95;1) = \frac{1}{n} \)</span> for any i.
</p></li>
<li><p>
The number of arrangements that put 1 and 2 in seats 1 and 2 over the number of all possible arrangements is
</p></li>
</ul>

<p>
<span class="math">\[\frac{(n-2)!}{n!} = \frac{1}{n(n-1)} \]</span>
</p>

<ul>
<li>
Works for any i and j:
</li>
</ul>

<p>
<span class="math">\[P(E_1 \cap E_j \cap E_k) = \frac{1}{n(n-1)(n-2)} \]</span>
</p>

<h3>
Conditional probability
</h3>

<ul>
<li>
How we update &quot;beliefs&quot; as new information comes to light.
</li>
<li>
Determine for events A, B, P(A/B) =
</li>
</ul>

<p>
<span class="math">\[\frac{P(A \cap B)}{P(B)} \]</span>
</p>

<pre><code>+ When B does not equal zero.
</code></pre>

<h4>
Explanation #1
</h4>

<p>
<span class="math">\[P(\lbrace 1 \rbrace) | B) \]</span>
</p>

<h4>
Explanation #2 - &quot;Frequentist&quot;
</h4>

<ul>
<li><p>
We have some experiment we are going to repeat, generating data.
</p>

<pre><code>01011010 A
11101011 B
00010110 
11010100
...
</code></pre></li>
<li><p>
Count number of times B happens
</p></li>
<li>
Count number of times that A happens.
</li>
</ul>

<h4>
Example - Flipping two coinis
</h4>

<ul>
<li>
S = {HH, TH, HT, TT}, all 1/4
</li>
<li><p>
What's the probabilty of getting 2 heads, given that first toss was heads? It's 1/2
</p>

<ul>
<li>
A = {HH}
</li>
<li>
B = {HT, HH}
</li>
</ul>

<p>
<span class="math">\[P(A|B) = \frac{P(\lbrace HH \rbrace)}{\lbrace HT, HH \rbrace}\]</span>
</p></li>
<li><p>
What's the probability of HH, given that at least one heads came up?
</p>

<ul>
<li>
A = {HH}
</li>
<li>
B = {HT, TH, HH}
</li>
</ul>

<p>
<span class="math">\[P(A|B) = \frac{P(HH)}{P(HT, TH, HH)} = \]</span> <span class="math">\[ \frac{\frac{1}{4}}{\frac{1}{4} + \frac{1}{4} + \frac{1}{4}} = \frac{1}{3} \]</span>
</p></li>
</ul>

<h3>
Example 2 - Fuses
</h3>

<ul>
<li>
Suppose we have 7 fuses, and 5 are working and 2 are broken.
</li>
<li>
<p>We want to find the broken ones, we're going to pick them one at a time, and we test.</p>
<ul>
<li>
<p>What's the probability that we only need 2 tests?</p>
<ul>
<li>
Define <span class="math">\(E_1\)</span> as &quot;first test shows up broken.
</li>
<li>
<span class="math">\(E_2\)</span> as &quot;second ...&quot; ...
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Gigerenzer's Experiment
</h3>

<ul>
<li><p>
He did the following experiment:
</p>

<ul>
<li>
He got the doctors to agree on some numbers.
</li>
<li>
They agreed a given patient (say 40-50 year old woman), you know nothing about her except she's in good health.
</li>
<li>
She has cancer with probability 0.008.
</li>
<li>
They had a cancer test that, if patient has cancer, returns positive .9 rightly.
</li>
<li>
If they don't, it will return positive .07 of they time.
</li>
</ul></li>
<li><p>
He asked 24 doctors:
</p>

<ul>
<li><p>
Suppose a woman's test is positive, what is the probability that she has cancer?
</p>

<ul>
<li>
First once said he didn't konw.
</li>
<li><p>
The other 24 returned this:
</p>

<pre><code>8 said &lt;= 10%
8 said    50% - 80%
8 said    90%
</code></pre></li>
</ul></li>
</ul></li>
<li><p>
Lets do some math
</p>

<ul>
<li>
C = &quot;patient has cancer&quot;
</li>
<li>
T = &quot;test is positive&quot; <span class="math">\[P(C) = 0.008 \]</span> <span class="math">\[P(T|C) = 0.9 \]</span> <span class="math">\[P(T|C^c) = 0.07 \]</span>
</li>
<li>
What's the probability of illness given that the test came up positive? <span class="math">\(P(C|T) = ? \)</span> <span class="math">\[P(C|T) = \frac{P(C\cap T)}{P(T)} \]</span> <span class="math">\[P(T|C) = \frac{P(C \cap T)}{P(C)} \]</span>
</li>
<li>
Want P(T)
</li>
</ul></li>
<li><p>
<strong>Lemma</strong>: For any event T, C <span class="math">\[P(T) = P(T \cap C) + P(T \cap C^c) \]</span>
</p></li>
<li><p>
Answer: 9.4%
</p></li>
</ul>

<h4>
Intuition, not a proof at all
</h4>

<ul>
<li>
<p>Say you take 1000 women,</p>
<ul>
<li>
Should have 8 with cancer ill.
</li>
<li>
How may should test positive? ~7 will be positive
</li>
</ul></li>
</ul>

<h2>
February 13th, 2013 - Recitation
</h2>

<ol>
<li><p>
Prove the following is true: <span class="math">\[ {n \choose 2} = {k \choose 2} + K(n - k) + {{n-k} \choose 2} \]</span>
</p>

<ul>
<li>
What is <span class="math">\(n \choose 2\)</span>?
</li>
<li>
We're going to have to put a <em>k</em> into the left hand side.
</li>
<li>
So you have n object, I can select 2 object as n choose 2
</li>
<li>
I could do the same selection of 2 objects by partitioning them into n and n-k objects.
</li>
<li>
Or I could select one from one side, etc.
</li>
<li>
Since I am selecting two objects, what are the possibilities.
</li>
<li>
Both of them could be from the left or right, or one could be on both sides.
</li>
</ul></li>
<li><p>
Prove: <span class="math">\[\sum_{k=1}^n k {n \choose k} = n2^{n-1} \]</span>
</p>

<ul>
<li>
If you expand out the left hand side: <span class="math">\[1 \times {n \choose 1} + 2 \times {n \choose 2} + ... + n {n \choose n} = n2^{n-1} \]</span>
</li>
</ul></li>
<li><p>
Let n be the number of people and we have to form a commitee of arbitrary size and select a chair from that committee.
</p>

<ul>
<li><p>
Method 1 <span class="math">\[k {n \choose k} \]</span> is the number of ways to select a commite of size k and a chair
</p>

<ul>
<li>
Hence, the total number of ways to form a commite of arbitrary size is <span class="math">\[\sum_{k=1}^{n} k {n \choose k} \]</span>
</li>
</ul></li>
<li><p>
Method 2
</p>

<ul>
<li>
We can first select the chair in n-ways. From the remaining n - 1 people we can select a subset in <span class="math">\(2^{n-1} ways.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prove: \)</span><span class="math">\(\sum_{i=0}^n (-1)^i {n \choose 1} = 0 \)</span>$
</p></li>
</ol>

<h2>
February 18th, 2013 - Lecture: Bayes's Theorem
</h2>

<h3>
Topics:
</h3>

<ul>
<li>
Bayes's theorem: Statement, examples, applications to spam filtering and other fields.
</li>
<li>
General version of Bayes's theorem and the red/black hat problem.
</li>
</ul>

<h3>
Boxes and balls
</h3>

<ul>
<li><p>
<strong>Example</strong>: There two boxes, the first contains 2 green balls and 7 res and the second contains 4 green and 3 red. I pick a random box, then a ball from that box. If I pick a red ball, what is the probability that I picked the first box?
</p>

<ul>
<li>
E = &quot;red ball was picked&quot;
</li>
<li>
F = &quot;1st box was picked <span class="math">\[P(F | E) = \frac{P(E \cap F)}{P(E)} = \]</span> <span class="math">\[\frac{\frac{7}{18}}{\frac{38}{63}} \]</span>
</li>
</ul>

<p>
<ol style="list-style-type: upper-roman">
<li><span class="math">\[P(E \cap F) = P(E | F) \times P(F) = \]</span> <span class="math">\[\frac{7}{9} \times \frac{1}{2} = \frac{7}{18} \]</span></li>
<li><span class="math">\[ P(E) = P(E \cap F) + P(E \cap F^c) = \]</span> <span class="math">\[\frac{7}{18} + P(E | F^c) \times P(F^c) = \]</span> <span class="math">\[\frac{7}{18} + \frac{3}{7} \times \frac{1}{2}\]</span>
</p></li>
</ul>
</li>
</ol>
<h3>
&quot;Bayesian Reasoning&quot;
</h3>

<ul>
<li>
A method for updating &quot;belief&quot;/&quot;uncertainty&quot; based on evidence.
</li>
<li>
<p>This is useful when you want to know the probability of some outcome given some evidence when you know the probability of the outcome and you also konw the quality of your evidence, and the probability of the evidence given the outcome, and finally, the probability of the evidence showing up if the outcome didn't happen.</p>
<ol>
<li>
P(outcome|evidence)
</li>
<li>
P(outcome)
</li>
<li>
P(evidence|outcome)
</li>
<li>
P(evidence|outcome^c)
</li>
</ol></li>
</ul>

<h3>
Bayesian Spam Filtering
</h3>

<ul>
<li>
Outdated but interesting anyway
</li>
<li><p>
The idea is that you estimate P(Email contains some word w | it's spam)
</p>

<ul>
<li>
Rolex
</li>
<li>
Viagra
</li>
<li>
Rutgers
</li>
<li>
Cash
</li>
<li>
LaTeX
</li>
</ul></li>
<li><p>
Then the probability of the P(Emails contains the same word w | it's not spam)
</p></li>
<li>
P(Email is spam)
</li>
<li>
Using these, there is Bayes theorem, which computes P(outcome|evidence) = P(spam | contains w)
</li>
<li>
<p>The attack is called Bayesian poisoning</p>
<ul>
<li>
You retrain this when you email it everyday.
</li>
<li>
Spammers pass it the words you are interested in, which you mark as spam, and then the ones you aren't interested in eventually pass.
</li>
</ul></li>
</ul>

<h3>
Bayes Theorem
</h3>

<ul>
<li>
<p>If E and F are events with <span class="math">\(P(E) \neq 0 \)</span> and <span class="math">\(P(E) \neq 0 \)</span>, then <span class="math">\[ P(F | E) = \frac{P(E | F) \times P(F)}{P(E | F)\times P(F) + P(E | F^c) \times P(F^c)} \]</span></p>
<ol>
<li>
<span class="math">\(P(E \cap F) = P(E | F) \times P(F) \)</span>
</li>
<li>
<span class="math">\(P(E) = P(E | F) \times P(F) + P(E | F^c) \times P(F^c) \)</span>
</li>
</ol></li>
</ul>

<h3>
General version of Bayes theorem
</h3>

<ul>
<li>
For multiple outcomes
</li>
<li><p>
Let <span class="math">\(F_1 ... F_n\)</span> be events that
</p>

<ol>
<li>
Are mutually exclusive
</li>
<li>
<span class="math">\(F&amp;#95;1 \cup ... \cup F&amp;#95;n = S \)</span>
</li>
<li>
<span class="math">\(\forall i P(F&amp;#95;i \neq 0) \)</span>
</li>
</ol></li>
<li><p>
Then for any j <span class="math">\[ P(F_j | E) = \]</span> <span class="math">\[\frac{P(E|F_j) \times P(F_j)}{\sum_{i=1}^n P(E|F_i) \times P(F_i)} \]</span>
</p></li>
</ul>

<h4>
Example with Lizards
</h4>

<ul>
<li>
<span class="math">\(F_1 ... F_n\)</span> are lizards in the family i
</li>
<li>
E = &quot;something genome of new lizard&quot;
</li>
<li>
Need to estimate the priors
</li>
</ul>

<h2>
February 18th, 2013 - Reading
</h2>

<h3>
Ross 3.3
</h3>

<h3>
Rosen 7.3
</h3>

<h2>
February 19th, 2013 - Homework 3
</h2>

<ol>
<li><p>
(6 points) What the probability that a 5 card hand contains exactly 3 spades? What if we condition on the hand containing at least 1 spade?
</p>

<ul>
<li><p>
A deck of cards has 52 cards, 4 suits, 13 ranks, one of each suit/rank combination.
</p></li>
<li><p>
There are <span class="math">\(52 \choose 5\)</span> possible hands.
</p></li>
<li><p>
There are <span class="math">\(13\)</span> cards which are spades.
</p></li>
<li><p>
Let <span class="math">\(E&amp;#95;i = \)</span> “a spade was drawn” and <span class="math">\(F&amp;#95;i = \)</span> “anything except a spade was drawn.”
</p></li>
<li><p>
We want <span class="math">\(P(E_1 \cap E_2 \cap E_3 \cap F_1 \cap F_2)\)</span>.
</p></li>
<li><p>
The probability of <span class="math">\(E_1\)</span> is <span class="math">\(\frac{13}{52}\)</span>, because thirteen of the cards are spades.
</p></li>
<li><p>
Assuming <span class="math">\(E_1\)</span>, the probability of <span class="math">\(E_2\)</span> is <span class="math">\(\frac{12}{51}\)</span>, because there is one less spade and one less card in general.
</p></li>
<li><p>
Assuming <span class="math">\(E_2\)</span>, the probability of <span class="math">\(E_3\)</span> is <span class="math">\(\frac{11}{50}\)</span>, because there are now two less spades and two less cards in general.
</p></li>
<li><p>
There are now 49 cards in all, 10 of which <em>are</em> spades.
</p></li>
<li><p>
Assuming <span class="math">\(E_1\)</span> through <span class="math">\(E_3\)</span>, the probability of <span class="math">\(F_1 = \frac{39}{49}\)</span>.
</p></li>
<li><p>
Assuming <span class="math">\(F_1\)</span>, the probability of <span class="math">\(F_2 = \frac{38}{48}\)</span>.
</p>

<p>
<span class="math">\[\frac{13}{52}\times\frac{12}{51}\times\frac{11}{50}\times\frac{39}{49}\times\frac{38}{48} = 0.008154261704\]</span>
</p></li>
<li><p>
Alternatively, there are 13 choose 3 ways of picking a spade, 39 choose 2 way of picking a “not spade,” and there are 52 choose 5 possible options:
</p>

<p>
<span class="math">\[\frac{{13 \choose 3} {39 \choose 2}}{{52 \choose 5}} = \frac{{286} \times {741}}{{2598960}} = 0.008154261704\]</span>
</p></li>
<li><p>
For part two, you only have to pick four cards out of 51, because one is a space already.
</p>

<p>
<span class="math">\[\frac{{13 \choose 2} {39 \choose 2}}{{51 \choose 4}}\]</span>
</p></li>
</ul></li>
<li><p>
(7 points) Suppose <span class="math">\(n\)</span> people each throw a six-sided die. Let <span class="math">\(A_n\)</span> be the event that at least two distinct people roll the same number. Calculate <span class="math">\(P(A_n)\)</span> for <span class="math">\(n=1,2,3,4,5,6,7\)</span>.
</p>

<ul>
<li><p>
For <span class="math">\(A_1\)</span>, It is impossible for two distinct people to roll the same number if only person rolls. <span class="math">\[P(A_1) = 0\]</span>
</p></li>
<li><p>
For <span class="math">\(A_2\)</span>, there are 36 possible throws, but only 6 of could contain two distinct people rolling the same number. <span class="math">\[P(A_2) = \frac{6}{6^2} = \frac{1}{6} = 0.1666666667\]</span>
</p></li>
<li><p>
For <span class="math">\(A_3\)</span> consider the complement, which can be described as “No two distinct people roll the same number.” The probability that all three people roll unique numbers is <span class="math">\(1 \times \frac{5}{6} \times \frac{4}{6} \times\)</span>, <span class="math">\[P(A_3) = 1 - P(A_3^c) = 1 - \frac{5}{6} \times \frac{4}{6} = 0.4444444444\]</span>
</p></li>
<li><p>
For <span class="math">\(A_4\)</span>, consider the complement again, and apply the same reasoning. <span class="math">\[P(A_4) = 1 - 1 \times \frac{5}{6} \times \frac{4}{6} \times \frac{3}{6} = \frac{13}{18} =0.7222222222\]</span>
</p></li>
<li><p>
For <span class="math">\(A_5\)</span>, consider the complement again, and apply the same reasoning. <span class="math">\[P(A_5) = 1 - \frac{5}{6} \times \frac{4}{6} \times \frac{3}{6} \times \frac{2}{6} = 0.9074074074\]</span>
</p></li>
<li><p>
For <span class="math">\(A_6\)</span>, consider the complement again. There are <span class="math">\(6!\)</span> ways of arranging the integer elements in the string “123456.” This is out of a <span class="math">\(6^6\)</span> ways of writing a string with integers from one to six. <span class="math">\[P(A_6) = 1 - \frac{6!}{6^6} = 0.98456790123\]</span>
</p></li>
<li><p>
If 7 people roll a dice with 6 sides, it is inevitable that at least two distinct people roll the same number. <span class="math">\[P(A_7) = 1\]</span>
</p></li>
</ul></li>
<li><p>
(3 points) Suppose we draw 2 balls at random from an urn that contains 5 distinct balls, each with a different number from <span class="math">\(&amp;#123;1,2,3,4,5&amp;#125;\)</span>, and define the events <span class="math">\(A\)</span> and <span class="math">\(B\)</span> as <span class="math">\[A = \text{&lt;code&gt;5 is drawn at least once&quot;} \quad \text{and} \quad
    B = \text{&lt;/code&gt;5 is drawn twice&quot;}\]</span> Compute <span class="math">\(P(A)\)</span> and <span class="math">\(P(B)\)</span>.
</p>

<ul>
<li><p>
<span class="math">\(P(B) = 0\)</span>
</p></li>
<li><p>
<span class="math">\(P(A) = \frac{4}{{5\choose 2}} = 0.4\)</span>
</p></li>
</ul></li>
<li><p>
(4 points) In the previous problem, suppose we place the first ball back in the urn before drawing the second. Compute <span class="math">\(P(A),P(B),P(A|B),P(B|A)\)</span> in this version of the experiment.
</p>

<ul>
<li><p>
<span class="math">\(P(A) = 1 - \left(\frac{4}{5}\right)^2 = .36 \)</span>
</p></li>
<li><p>
<span class="math">\(P(B) = \left(\frac{1}{5}\right)^2 = .04\)</span>
</p></li>
<li><p>
<span class="math">\(P(A|B) = 1\)</span>
</p></li>
<li><p>
<span class="math">\(P(B|A) = \frac{1}{5}\)</span>
</p></li>
</ul></li>
<li><p>
(4 points) Suppose <span class="math">\(5\)</span> percent of cyclists cheat by using illegal doping. The blood test for doping returns positive <span class="math">\(98\)</span> percent of the people doping and <span class="math">\(12\)</span> percent who do not. If Lance’s test comes back positive, what the probability that he is doping? (Ignoring all other evidence, of course...)
</p>

<ul>
<li><p>
<span class="math">\(P(C) = \)</span> “the probability a cyclist cheated by doping” <span class="math">\(= .05\)</span>
</p></li>
<li><p>
<span class="math">\(P(T|C) = \)</span> “the probability a cyclist giving a positive test if they doped” <span class="math">\(= .98\)</span>
</p></li>
<li><p>
<span class="math">\(P(T|C^c) = \)</span> “the probability a cyclist giving a positive test if they <em>did not</em> dope” $ = .12<span class="math">\(&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;We want the probability of a cyclist doping given a positive test, which is \)</span>P(C|T)$.
</p>

<p>
<span class="math">\[P(T|C) = .98 = \frac{P(T \cap C)}{P(C)} = \frac{P(T \cap C)}{.05}\]</span>
</p>

<p>
<span class="math">\[P(T \cap C) = .049\]</span>
</p>

<p>
<span class="math">\[P(T \cap C^c) = .12 = \frac{P(T \cap C^c)}{P(C^c)} = \frac{P(T \cap C^c)}{.95}\]</span>
</p>

<p>
<span class="math">\[P(T \cap C^c) = .114\]</span>
</p></li>
<li><p>
For any event <span class="math">\(T\)</span> and <span class="math">\(C\)</span>,
</p>

<p>
<span class="math">\[P(T) = P(T \cap C) + P(T \cap C^c)\]</span>
</p>

<p>
<span class="math">\[P(T) = .049 + .114 = .163\]</span>
</p>

<p>
<span class="math">\[P(C|T) = \frac{P(C \cap T)}{P(T)} = \frac{.049}{.163} = 0.3006134969 \approx .30\]</span>
</p></li>
<li><p>
Intuition:
</p>

<ul>
<li><p>
Take 100 cyclists, 5 of them actually doped according to these numbers.
</p></li>
<li><p>
If all 100 cyclists are tested, it’s very, very likely the 5 will return positive.
</p></li>
<li><p>
Of the remaining 95 cyclists, 12 percent of them will also return positive, which makes a little less than 12 cyclists.
</p></li>
<li><p>
So for each of the 17 who tested positive, there are five who actually doped, which means each has a <span class="math">\(\frac{5}{17} = 0.2941176471 \approx .30\)</span> probability of doping.
</p></li>
</ul></li>
</ul></li>
<li><p>
(6 points) If <span class="math">\(A \subseteq B\)</span>, can <span class="math">\(A\)</span> and <span class="math">\(B\)</span> be independent? What we if require that <span class="math">\(P(A)\)</span> and <span class="math">\(P(B)\)</span> both not equal <span class="math">\(0\)</span> or <span class="math">\(1\)</span>?
</p>

<ul>
<li><p>
<span class="math">\(A\)</span> and <span class="math">\(B\)</span> are independent if they satisfy the condition <span class="math">\(P(A \cap B) = P(A)P(B)\)</span>
</p></li>
<li><p>
If <span class="math">\(A \subseteq B\)</span>, it is true that <span class="math">\(A \cap B = B\)</span>.
</p></li>
<li><p>
This means that <span class="math">\(P(A \cap B)=P(B)\)</span>.
</p></li>
<li><p>
In order to be independent when <span class="math">\(A \subseteq B\)</span> is true, <span class="math">\(P(B)\)</span> must equal <span class="math">\(P(A)\cdot P(B)\)</span>.
</p></li>
<li><p>
The only way anything multiplied by something can equal itself is if that something is one.
</p></li>
<li><p>
Therefore, when <span class="math">\(A \subseteq B\)</span>, <span class="math">\(A\)</span> and <span class="math">\(B\)</span> can be independent when <span class="math">\(P(A) = 1\)</span>.
</p></li>
<li><p>
Furthermore, being as anything multiplied by zero yields zero, when <span class="math">\(A \subset B\)</span>, <span class="math">\(A\)</span> and <span class="math">\(B\)</span> can be independent if <span class="math">\(P(B) = 0\)</span>.
</p></li>
<li><p>
So no.
</p></li>
</ul></li>
<li><p>
<strong>Extra credit (5 points)</strong> Consider the experiment where two dice are thrown. Let <span class="math">\(A\)</span> be the event that the sum of the two dice is 7. For each <span class="math">\(i \in &amp;#123;1,2,3,4,5,6&amp;#125;\)</span> let <span class="math">\(B_i\)</span> be the event that at least one <span class="math">\(i\)</span> is thrown.
</p>

<ol>
<li><p>
Compute <span class="math">\(P(A)\)</span> and <span class="math">\(P(A|B_1)\)</span>.
</p>

<ul>
<li><p>
<span class="math">\(P(A) = \frac{6}{6^2} = \frac{1}{6}\)</span>
</p></li>
<li><p>
<span class="math">\(P(A|B_1) = \frac{P(A \cap B_1)}{P(B_1)}= \frac{\frac{3}{{6 \choose 2}}}{\frac{1}{6} + \frac{1}{6}} = 0.6\)</span>
</p></li>
</ul></li>
<li><p>
Prove that <span class="math">\(P(A|B_i) = P(A|B_j)\)</span> for all <span class="math">\(i\)</span> and <span class="math">\(j\)</span>.
</p>

<ul>
<li><p>
Being as the sum has to be seven, there is one and only way to sum to seven for the integers 1 through 6.
</p></li>
<li><p>
So it doesn’t matter what is rolled on the first roll, the probability “rides on” the second roll being the number the first roll needs to sum to seven.
</p></li>
<li><p>
The probability of rolling any given number on a fair die is always <span class="math">\(\frac{1}{6}\)</span>.
</p></li>
<li><p>
Therefore, for all <span class="math">\(i\)</span> and <span class="math">\(j\)</span>, the probability cannot be anything but <span class="math">\(\frac{1}{6}\)</span>.
</p></li>
</ul></li>
<li><p>
Since you know that some <span class="math">\(B_i\)</span> always occurs, does it make sense that <span class="math">\(P(A) \neq P(A | B_i)\)</span>? (After all, if <span class="math">\(E\)</span> is an event with <span class="math">\(P(E) = 1\)</span>, then for any event <span class="math">\(F\)</span>, <span class="math">\(P(F|E) = P(F)\)</span>. What is going on? Does this seem paradoxical?)
</p></li>
</ol></li>
</ol>

<h2>
February 20th, 2013 - Lecture: Independence
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Multiplication rule for conditional probability.
</li>
<li>
Independent events: two equivalent definitions and several examples with cards and dice.
</li>
<li>
People v. Collins and the prosecutor's fallacy.
</li>
<li>
Mutual independence.
</li>
</ul>

<h3>
Multiplication Rule for Conditional Probability
</h3>

<ul>
<li><p>
<strong>Example</strong>: Hat contains 3 cards, RR, RB, BB. Pick a card, put it on table, see a R side, what's the probability the other side is B?
</p>

<ul>
<li>
Wrong: 1/2
</li>
<li>
Right: 1/3
</li>
</ul></li>
<li><p>
If E_1 through E_2 are events, all P(E_i) do not equal zero, <span class="math">\[P(E_1 \cap E_2 \cap ... \cap E_n) = \]</span> <span class="math">\[P(E_1) \times P(E_2 | E_1) \times ... \times P(E_n | E_1 \cap ... \cap E_{n-1}) \]</span>
</p>

<ul>
<li>
This might be easier to express as the intersection of smaller events.
</li>
<li>
This is really easy thing to prove.
</li>
<li>
&quot;Proves itself.&quot;
</li>
<li><p>
<strong>Proof</strong>:
</p>

<ol>
<li>
<span class="math">\(P(E_1) \times \frac{P(E_2 \cap E_1)}{P(E_1)} \times ... \times \frac{P(E_1 \cap ... \cap E_n)}{P(E_1 \cap ... \cap E_{n-1}})\)</span>
</li>
<li>
Cancel. Beautiful cancellation.
</li>
</ol></li>
<li><p>
This allows you to say, what happens when the first once happens? And now, what about the second one? So on so forth.
</p></li>
<li><p>
<strong>Example</strong>: What is the probability of a five card hand not containing a pair?
</p>

<ul>
<li>
E_i = &quot;first i cards in hard do not contain a pair&quot;
</li>
<li>
<em>Claim</em>: What we want is <span class="math">\(E&amp;#95;1 \cap E&amp;#95;2 \cap ... \cap E&amp;#95;5 \)</span>
</li>
<li>
What's funny about this is that we don't want E_1 through E_4, we are actually only interested in E_5.
</li>
<li>
The reason we do this, however, is because it makes this calculation easier.
</li>
<li><p>
Possibilities:
</p>

<ol>
<li><p>
<span class="math">\(P(E&amp;#95;1) = 1 \)</span>
</p>

<ul>
<li>
The probaility of the first hand not being a pair is 1, because it's one card.
</li>
</ul></li>
<li><p>
<span class="math">\(P(E&amp;#95;2 | E&amp;#95;1) = \frac{48}{51} \)</span>
</p></li>
<li>
<span class="math">\(P(E&amp;#95;3 | E&amp;#95;1 \cap E&amp;#95;2) = \frac{44}{50} \)</span>
</li>
<li>
<span class="math">\(P(E&amp;#95;4 | E&amp;#95;1 \cap ... \cap E&amp;#95;3) = \frac{40}{44} \)</span>
</li>
<li>
<span class="math">\(P(E&amp;#95;5 | E&amp;#95;1 \cap ... \cap E&amp;#95;4) = \frac{36}{48} \)</span>
</li>
</ol></li>
<li><p>
Multiply these values together to get 50.7%.
</p></li>
</ul></li>
</ul></li>
</ul>

<h3>
Independence
</h3>

<ul>
<li><p>
<strong>Definition</strong>: Say events E and F are independant if $P(E F)
</p>

<ul>
<li>
P(E) P(F) <span class="math">\(. + &lt;strong&gt;Equivalent definition&lt;/strong&gt;: E and F are independent i(P(F) = 0 or P(E|F) = P(E)\)</span>.
</li>
<li>
<p><strong>Proof</strong>:</p>
<ol>
<li>
Take E and F set <span class="math">\(P(E \cap F) = P(E) \times P(F) \)</span>.
</li>
<li>
If <span class="math">\(P(F) = 0 \)</span>, then done.
</li>
<li>
If not, then <span class="math">\(P(E|F) = \frac{P(E \cap F)}{P(F)}\)</span>
</li>
</ol></li>
</ul></li>
<li><p>
<strong>Exercise</strong>: Toss two coins
</p>

<ul>
<li>
E = &quot;1st coin was H&quot;
</li>
<li>
F = &quot;2nd coin was H&quot;
</li>
<li>
P(E) = P(F) = 1/2
</li>
</ul></li>
<li><p>
<strong>Exercise</strong>: Tossing two dice
</p>

<ul>
<li>
E = &quot;sum of dice is 6&quot;
</li>
<li>
F = &quot;first die was 4&quot;
</li>
<li>
<span class="math">\(P(E) = P(\lbrace(1,5),(2,4),(3,3),(4,2),(5,1)\rbrace) = \frac{5}{36}\)</span>
</li>
<li>
P(F) = 1/6
</li>
<li>
<span class="math">\(P(E \cap F) = P(\lbrace(4,2)\rbrace) = \frac{1}{36} \)</span>
</li>
<li>
<span class="math">\(P(E) \times P(F) = \frac{5}{36} \times \frac{1}{6} \)</span>
</li>
</ul></li>
<li><p>
<strong>Exercise</strong>: Suppose a family has 3 kids.
</p>

<ul>
<li>
E = &quot;family has at least 1 boy and 1 girl&quot;
</li>
<li>
F = &quot;family has at most 1 boy&quot;
</li>
<li>
<p>Are these indepedant?</p>
<ul>
<li>
<span class="math">\(S = \lbrace BBB, BBG, BGB, GBB, BGG, GBG, GGB, GGG\rbrace\)</span>
</li>
<li>
<span class="math">\(E = S - \lbrace BBB, GGG \rbrace \)</span>
</li>
<li>
<span class="math">\(F = \lbrace BGG, GBG, GGB, GGG \rbrace \)</span>
</li>
<li>
<span class="math">\(E \cap F = \lbrace BGG, GBG, GGB \rbrace \)</span>
</li>
<li>
$ P(E F) =  <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span> P(E) P(F) =   =  $
</li>
<li>
Therefore, independent.
</li>
</ul></li>
</ul></li>
</ul>

<h4>
&quot;Independent&quot; and &quot;Mutually Exclusive&quot; are not the same
</h4>

<ul>
<li>
<p><strong>Example</strong>: 2 dice</p>
<ul>
<li>
E = &quot;sum was 6&quot;
</li>
<li>
F = &quot;first die was 6&quot;
</li>
<li>
E and F are not mutually exclusive, but not independent.
</li>
</ul></li>
</ul>

<h4>
Prosecutor's Fallacy: People v. Collins (1968)
</h4>

<ul>
<li>
<p>The prosectution came up with these numbers:</p>
<ul>
<li>
The probability of a black man with a beard is 1 in 10
</li>
<li>
The probability of a man with a mustache is 1 in 4
</li>
<li>
White woman with ponytail is 1 in 10
</li>
<li>
So on so forth, 1 in 3, 1 in 10, 1 in 1000
</li>
<li>
So he made this calculation: <span class="math">\[\frac{1}{10} \times \frac{1}{4} \times \frac{1}{10} \times \frac{1}{10} \times \frac{1}{3} \times \frac{1}{1000} = \frac{1}{12 \times 10^6} \]</span>
</li>
<li>
<em>Say</em> that the population of LA was <span class="math">\(24 \times 10^6\)</span>
</li>
<li>
Then, you'd &quot;expect&quot; to have 2 couples fitting this evidence.
</li>
<li>
P(evidence | innocent) <span class="math">\(\neq\)</span> P(innocent | evidence)
</li>
</ul></li>
</ul>

<h4>
With 3 events
</h4>

<ul>
<li>
E is indepedant of F and G
</li>
<li>
F is indepedant of G
</li>
<li>
Then is E indepedant of F intersect G?
</li>
<li>
<p><strong>Example</strong>: rolling two die</p>
<ul>
<li>
E = &quot;sum to 7&quot;
</li>
<li>
F = &quot;first die was 4&quot;
</li>
<li>
G = &quot;second die was 3&quot; <span class="math">\[P(E | F \cap G) = 1\]</span> <span class="math">\[P(E) = \frac{1}{6} \]</span>
</li>
<li>
<strong>Definition</strong>: E, F, and G are independant (alternatively &quot;mutually independant&quot;) if: <span class="math">\[P(E \cap F) = P(E) \times P(F) \]</span> <span class="math">\[P(E \cap G) = P(E) \times P(G) \]</span> <span class="math">\[P(F \cap G) = P(F) \times P(G) \]</span> <span class="math">\[P(E \cap F \cap G) = P(E) \times P(F) \times P(G) \]</span>
</li>
</ul></li>
</ul>

<h4>
With n events
</h4>

<ul>
<li>
<strong>Definition</strong>: <span class="math">\(E&amp;#95;1, ..., E&amp;#95;n \)</span> are indepedant if for every subset <span class="math">\(I \subseteq \lbrace 1, 2, ..., n \rbrace\)</span> of at least size 2
</li>
<li>
<strong>Example</strong>: Flip n coins
</li>
</ul>

<h2>
February 20th, 2013 - Recitation
</h2>

<ul>
<li>
Proove: <span class="math">\[{n \choose n_1 \cdot n_2 \cdot ... \cdot n_r} = \]</span> [[[{n-1 n_1 - 1 ... n_r} + ... + {n-1 n_1 ... n<em>{r-1} n</em>{r-1} $$
</li>
</ul>

<h2>
February 20th, 2013 - Reading
</h2>

<h3>
3.3 Bayes' Formula
</h3>

<h3>
3.4 Independent Events
</h3>

<ul>
<li>
<strong>Equation 4.1</strong>: <span class="math">\(P(EF) = P(E)P(F)\)</span>
</li>
<li>
<strong>Independent events</strong>: Two events E and F are said to be independent if Equation (4.1) holds.
</li>
<li>
<strong>Dependent events</strong>: Two events E and F that are not independent are said to be dependent.
</li>
</ul>

<h3>
Optional: People v. Collions court opinion.
</h3>

<h3>
British case involving Sally Clark.
</h3>

<h2>
February 25th, 2013 - Lecture
</h2>

<h3>
Announcements
</h3>

<ol>
<li>
HW 3 due Wednesday
</li>
<li>
Review on Wednesday
</li>
<li>
Midterm on Monday
</li>
</ol>

<h3>
Independence of Several Events
</h3>

<ul>
<li>
Event are indepedant if <span class="math">\(P(EF) = P(E)P(F)\)</span>
</li>
<li><p>
<strong>Example</strong>: Three events with dice:
</p>

<ul>
<li>
E = &quot;sum is 7&quot;
</li>
<li>
F = &quot;1st die is four&quot;
</li>
<li>
G = &quot;2nd die is three&quot;
</li>
<li>
E, F, G are <em>pairwise independant</em>
</li>
<li>
But the <span class="math">\(P(E | F \cap G) = 1\)</span>, not <span class="math">\(P(E) = \frac{1}{u}\)</span>
</li>
</ul></li>
<li><p>
<strong>Definition</strong>: Events <span class="math">\(E&amp;#95;1, E&amp;#95;2, ... E&amp;#95;n \)</span> are <strong>indepedent</strong> (or <strong>mutually independent</strong>) if for every subset of numbers from 1 through n of size greater than or equal to 2: <span class="math">\[P\left(\bigcap_{i \in I} E_i\right) = \prod_{i \in I} P(E_i)\]</span>
</p>

<ul>
<li>
Equivilently, <span class="math">\[ P(E_{i_1} \cap E_{i_2} \cap ... \cap E_{i_n}) = P(E_{i_1}) \cdot P(E_{i_2}) \cdot ... \cdot P(E_{i_k}) \]</span>
</li>
</ul></li>
<li><p>
<strong>Claim</strong>: For any <span class="math">\(i\)</span> and <span class="math">\(j_i, ..., j_k\)</span> (not including i) <span class="math">\[P(E_i | E_{j_1} \cap ... \cap E_{j_k}) = P(E_i)\]</span>
</p>

<ul>
<li>
<strong>Proof</strong>: This equation equals <span class="math">\[ \frac{P(E_1 \cap E_{j_1} \cap ... \cap E_{j_k})}{P(E_{j_1} \cap ... \cap E_{j_k})}\]</span>
</li>
</ul></li>
<li><p>
<strong>Example</strong>: Suppose we flip a &quot;biased&quot; coin <span class="math">\(n\)</span> times <span class="math">\[ P(H) = p, &amp;#58; P(T) = 1 - p, &amp;#58; (O \lt p \lt 1)\]</span>
</p>

<ol>
<li><p>
What is the probability we getting heads all n times?
</p>

<ul>
<li>
Define <span class="math">\(E&amp;#95;i = \)</span> &quot;ith flip was heads.&quot; <span class="math">\[E = E_1 \cap ... \cap E_n, &amp;#58; P(E) = P(E_1) \times ... \times P(E_n)\]</span>
</li>
</ul></li>
<li><p>
What is the probability we get at least one H?
</p>

<ul>
<li>
<span class="math">\(F = \)</span> &quot;at least on H&quot;
</li>
<li>
<span class="math">\(C(F) = \)</span>&quot;all T on n flips&quot;
</li>
<li>
<span class="math">\(P(F) = 1 - C(F) = 1 - (1 - p)^n\)</span>
</li>
<li>
<span class="math">\(P(F^c) = (1 - p)^n\)</span>
</li>
</ul></li>
<li><p>
What is the probability of getting exactly k heads?
</p>

<ul>
<li>
First find the probability of HHHH...H (k heads) TTTTT...T (n-k tails) (h times)
</li>
<li>
<span class="math">\(p \times p \times p \times p ... \)</span> (k times) <span class="math">\(\times (1-p) \times (1 -p) \times ... \times (1 - p) = p^k (1-p)^{n - k} \)</span>
</li>
<li>
How many H/T strings, length n, with k Hs? <span class="math">\(n \choose k\)</span>
</li>
<li>
P(exactly k Hs) = <span class="math">\({n \choose k} \cdot p^k \cdot (1 - p)^{n-k}\)</span>
</li>
</ul></li>
</ol></li>
</ul>

<h3>
Primality Testing
</h3>

<ul>
<li>
One way to check if a number is prime is to divide by the square root of number (or something)
</li>
<li><p>
So people found what's called the <a href="http://en.wikipedia.org/wiki/Miller–Rabin_primality_test">Miller-Rabin</a> test
</p>

<pre><code>when running MR on x
    if x is not prime
        then MR outputs "not prime" with probability 1/4
        else says "don't know" the other 3/4 times
    if x is prime
        then MR always says "don't know"
</code></pre></li>
</ul>

<h4>
Algorithmic amplification
</h4>

<ul>
<li><p>
The idea is to run MR n times on x
</p>

<ul>
<li>
If it ever says &quot;not prime&quot;, output &quot;not prime&quot;
</li>
<li>
If not, output &quot;probabily prime&quot;
</li>
</ul></li>
<li><p>
What is the probability of outputting &quot;probability prime&quot; if X is not prime
</p>

<ul>
<li>
$ ()^n $
</li>
</ul></li>
<li><p>
<a href="http://en.wikipedia.org/wiki/AKS_primality_test">AKS test</a>
</p></li>
</ul>

<h3>
Ramsey Numbers
</h3>

<ul>
<li><p>
<strong>Example</strong>: &quot;The probabilistic method&quot;
</p>

<ul>
<li>
Take a complete graph <span class="math">\(n\)</span> vertices, color every edge R or B, and you &quot;lose&quot; if you color in a triangle.
</li>
<li><p>
Avoid making an all red or all blue <span class="math">\(K_k\)</span>
</p>

<pre><code>o----R----o
|\        |
|  \      |
B    R    B
|      \  |
|        \|
o----B----o (or something)
</code></pre></li>
</ul></li>
<li><p>
<strong>Theorem</strong> If n is large enough relative to k, then you can't avoid making all R or B <span class="math">\(K_k\)</span> <span class="math">\[n \gt R(k)\]</span>
</p></li>
<li>
<span class="math">\(R(3) = 6\)</span>, <span class="math">\(R(4) = 18\)</span> (1979), <span class="math">\(R(5)\)</span> is unknown
</li>
<li><p>
<strong>Theorem</strong>: <span class="math">\(R(k) \ge n^{\frac{k}{2}} \)</span> when <span class="math">\(k \gt 4\)</span>, if $ n = 2^{}<span class="math">\(, then you can color \)</span>K_n$ and avoid a all R or B <span class="math">\(K_K\)</span>
</p>

<ul>
<li><p>
<strong>Outline</strong>: (Erdos)
</p>

<ol>
<li>
Calculate the probability that a random coloring &quot;works&quot;, not that is greater than 0
</li>
<li>
Therefore one exists, and done.
</li>
</ol></li>
<li><p>
Color each edge independently Red or Blue with probability of one half
</p>

<ul>
<li>
E = &quot;some K_k gets all Red or Blue edges&quot;
</li>
<li>
How many <span class="math">\(K_K\)</span> are these in <span class="math">\(K&amp;#95;K \)</span>? <span class="math">\(n \choose k\)</span>
</li>
<li>
Want: <span class="math">\[ P(E) = P\left( \bigcup_{i =1}^{n \choose k} E_i \right) \lt 1 \]</span> $$]
</li>
</ul></li>
</ul></li>
</ul>

<h2>
February 25th, 2013 - Lecture: Mutual Independence and Applications
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Definition of mutual independence
</li>
<li>
Examples
</li>
<li>
<p>Applications to biased coin flips</p>
<ul>
<li>
primality testing
</li>
<li>
the bound on Ramsey numbers
</li>
</ul></li>
</ul>

<h2>
February 25th, 2013 - Reading
</h2>

<h3>
Ross 3.4.
</h3>

<h3>
Rosen 7.2.
</h3>

<h2>
Midterm 1 Review
</h2>

<h3>
Review Sheet 1
</h3>

<h4>
Set Theory Review
</h4>

<ul>
<li>
<span class="math">\(x \in A\)</span> means &quot;x is an element of A&quot;; <span class="math">\(x \notin A\)</span> means it's not.
</li>
<li><p>
<strong>relations between sets</strong>
</p>

<ul>
<li>
<span class="math">\(A \subseteq B\)</span> means that if x is in A, then x is in B
</li>
<li>
<span class="math">\(A \supseteq B\)</span> means <span class="math">\(B \subseteq A\)</span>
</li>
<li>
<span class="math">\(A = B\)</span> means <span class="math">\(A \subseteq B\)</span> and <span class="math">\(B \subseteq A\)</span>
</li>
</ul></li>
<li><p>
<strong>operations on sets</strong>
</p>

<ul>
<li>
<span class="math">\(A^c = &amp;#123;x \in S : x \notin A&amp;#125;\)</span> (complement)
</li>
<li>
<span class="math">\(\emptyset = S^c\)</span> (the empty set)
</li>
<li>
<span class="math">\(A \cap B = &amp;#123;x \in S : x \in A \land x \in B&amp;#125;\)</span> (intersection)
</li>
<li>
<span class="math">\(A \cup B = &amp;#123;x \in S : x \in A \lor x \in B &amp;#125;\)</span> (union)
</li>
<li>
<span class="math">\(A \setminus B =&amp;#123;x\in S :x\in A \land x \notin B &amp;#125; = A \cap B^c\)</span>
</li>
</ul></li>
<li><p>
<strong>set identities</strong>
</p>

<ul>
<li>
<span class="math">\(A \cup (B \cap C) = (A \cup B) \cap (A \cup C)\)</span>
</li>
<li>
<span class="math">\(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\)</span>
</li>
<li>
<span class="math">\((A \cap B)^c = (A^c) \cup (B^c)\)</span> (de Morgan's law)
</li>
<li>
<span class="math">\((A \cup B)^c = (A^c) \cap (B^c)\)</span> (de Morgan's law)
</li>
</ul></li>
</ul>

<h4>
Probability Theory
</h4>

<ul>
<li><p>
<strong>Random experiment</strong>
</p>

<ul>
<li>
Idealized or conceptual experiment.
</li>
<li>
It can be useful to imagine that the experiment can be repeated infinitely often under identical conditions, but with different outcomes.
</li>
</ul></li>
<li><p>
<strong>Sample Space</strong>
</p>

<ul>
<li>
The set of outcomes (elementary events) of a random experiment.
</li>
</ul></li>
<li><p>
<strong>An event</strong>
</p>

<ul>
<li>
A subset of the sample space of an experiment.
</li>
<li>
If the experiment is performed and the out of <span class="math">\(x \in S\)</span> we say that &quot;x occurs.&quot;
</li>
<li>
If not, &quot;x does not occur&quot;
</li>
</ul></li>
<li><p>
<strong>Probability measure</strong>
</p>

<ul>
<li>
<p>A real-valued non-negative function on events in S which satisfy these axioms:</p>
<ul>
<li>
<span class="math">\(P(S) = 1\)</span>
</li>
<li>
<span class="math">\(P(A \cup B) = P(A) + P(B)\)</span> whenever <span class="math">\(A \cap B = \emptyset\)</span>
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Conditional Probability
</h4>

<ul>
<li>
<strong>Conditional probability formula</strong>
</li>
</ul>

<p>
<span class="math">\[ P(B|A) = \frac{P(A \cap B)}{P(A)} \]</span>
</p>

<ul>
<li>
<strong>Hypotheses</strong>
</li>
</ul>

<p>
<span class="math">\[ P(A) = \sum_{i = 1}^{n} P(H_1)P(A|H_i) \]</span>
</p>

<ul>
<li>
<strong>Bayes' rule</strong>
</li>
</ul>

<p>
<span class="math">\[P(H_i|A) = \frac{P(A|H_1)P(H_1)}{\sum_{j = 1}^n P(H_j)P(A|H_j)}\]</span>
</p>

<h4>
Independence
</h4>

<ul>
<li>
Events are <strong>independent</strong> if and only if
</li>
</ul>

<p>
<span class="math">\[P(B|A) = P(B) \]</span>
</p>

<ul>
<li>
This means that the probability of B given the information that A has occurred is the original probability of B, so A gives no new information about B's probability. Using the conditional probability formula, for the left-hand side we see that
</li>
</ul>

<p>
<span class="math">\[P(A \cap B) \setminus P(A) = P(B) \]</span>
</p>

<ul>
<li>
Multiplying both sides of this equation by <span class="math">\(P(A)\)</span> we get the <strong>product law</strong> for independent events
</li>
</ul>

<p>
<span class="math">\[P(A \cap B) = P(A)P(B)\]</span>
</p>

<h3>
Axioms of Probability - Self Test Problems and Exercises
</h3>

<ol>
<li><p>
A cafeteria offers a three-course meal consisting of an entree, a starch, and a dessert. The possible choices are given in the following table:
</p>

<pre><code>Course          Choices
Entree          Chicken or roast beef
Starch          Pasta or rice or potatoes
Dessert         Ice cream or Jello or apple pie or a peach
</code></pre>

<p>
A person is to choose one course from each category.
</p>

<ol>
<li><p>
How many outcomes are in the sample space?
</p>

<ul>
<li>
There are there points a person gets to choose something.
</li>
<li>
At each of those points, there are 2, 3, and 4 choices.
</li>
<li>
So I think it's <span class="math">\(2! + 3! + 4!\)</span>, but I'm pretty bad at math.
</li>
<li>
In hindsight, that's about putting things in order, and has nothing to do with this problems.
</li>
<li>
It's simply: <span class="math">\(2 \cdot 3 \cdot 4\)</span>
</li>
</ul></li>
<li><p>
Let A be the event that ice cream is chosen. How many outcomes are in A?
</p>

<ul>
<li>
This only says something about the final decision.
</li>
<li>
Therefore, the previous number still holds, with a restriction on the final choice.
</li>
<li>
Therefore, <span class="math">\(2 \cdot 3\)</span>
</li>
</ul></li>
<li><p>
Let B be the event that chicken is chosen. How many outcomes are in B?
</p>

<ul>
<li>
Similar formula.
</li>
<li>
<span class="math">\(3 \cdot 4\)</span>
</li>
</ul></li>
<li><p>
List all the outcomes in the event AB.
</p>

<ol>
<li>
Chicken
</li>
<li>
(Pasta|Rice|Potatoes)
</li>
<li>
<p>Ice cream</p>
<ul>
<li>
More formally, {(chicken, x, ice cream) | x <span class="math">\(\in\)</span> {pasta, rich, potatoes}
</li>
</ul></li>
</ol></li>
<li><p>
Let C be the event that rice is chosen. How many outcomes are in C?
</p>

<ul>
<li>
<span class="math">\(2 \cdot 3\)</span>
</li>
</ul></li>
<li><p>
List all the outcomes in the event ABC.
</p>

<ul>
<li>
{(Chicken, rice, ice cream)}
</li>
</ul></li>
</ol></li>
<li><p>
A customer visiting the suit department of a certain store will:
</p>

<ul>
<li>
purchase a suit, <span class="math">\(.22\)</span>
</li>
<li>
purchase a shirt, $ .30<span class="math">\(&lt;/li&gt; &lt;li&gt;purchase a tie, \)</span>.28<span class="math">\(&lt;/li&gt; &lt;li&gt;both a suit and a shirt, \)</span> .11<span class="math">\(&lt;/li&gt; &lt;li&gt;both a suit and a tie, \)</span>.14<span class="math">\(&lt;/li&gt; &lt;li&gt;both a shirt and a tie, \)</span>.10<span class="math">\(&lt;/li&gt; &lt;li&gt;all 3 items with probability \)</span> .06$
</li>
</ul>

<p>
What is the probability that a customer purchases
</p>

<ul>
<li><p>
none of these items?
</p>

<ul>
<li>
I think it's too easy for them to just give us the .06 number and then have us calculate the complement. But that's my first guess, .94. (I know this is wrong.)
</li>
<li>
We're interested in the union of purchasing a suit, purchasing a shirt, and purchasing a tie.
</li>
<li>
The probability of buying all of these items is: <span class="math">\[P(A \cap B \cap C) = .22 + .30 + .28 + .11 + .14 + .10 + .06\]</span>
</li>
<li>
Subtract one from this to get the probability of buying none.
</li>
<li>
I worry that I would not see this on a test.
</li>
</ul></li>
<li><p>
exactly 1 of these items?
</p>

<ul>
<li>
Add the probabilities that correspond <em>to</em> the sample space.
</li>
<li>
Subtract the probabilities that <em>do not</em> correspond to this event. <span class="math">\[.22 + .30 + .28 - .11 - .14 - .10 - .06\]</span>
</li>
<li>
Of course, this is totally wrong.
</li>
<li>
Right, so formally, where A is &quot;purchase suit&quot;, B is &quot;purchase shirt&quot;, and C is &quot;purchase tie.&quot; We want: <span class="math">\[P(AB \cup AC \cup BC)\]</span>
</li>
</ul></li>
</ul></li>
<li><p>
A deck of cards is dealt out. What is the probability that the 14th card dealt is an ace? What is the probability that the first ace occurs on the 14th card?
</p>

<ul>
<li>
So there are four aces in a deck, meaning there are 48 that aren't.
</li>
<li>
But every card is equally likely to be pulled at any time.
</li>
<li>
This is called <strong>symmetry</strong>.
</li>
</ul></li>
<li><p>
Let A denote the event that the midtown temperature in Los Angeles is 70◦F, and let B denote the event that the midtown temperature in New York is 70◦F. Also, let C denote the event that the maximum of the midtown temperatures in New York and in Los Angeles is 70◦F. If P(A) = .3, P(B) = .4, and P(C) = .2, find the probability that the minimum of the two midtown temperatures is 70◦F.
</p>

<ul>
<li>
A is &quot;the temp of LA is 70F&quot;, .3
</li>
<li>
B is &quot;the temp of NY is 70F&quot;, .4
</li>
<li>
<span class="math">\(P(A \cap B) = .3 + .4 - P(AB)\)</span>
</li>
<li>
Call D &quot;the minimum of the two is 70F&quot;
</li>
<li>
Observe that <span class="math">\(A \cap B = D \cap C\)</span>, and therefore <span class="math">\(AB = DC\)</span>.
</li>
<li>
<span class="math">\(P(D \cap C) = P(D) + .2 - P(DC)\)</span>
</li>
<li>
<span class="math">\(P(D) + .2 - P(DC) = .3 + .4 - P(AB)\)</span>
</li>
<li>
<span class="math">\(P(D) + .2 = .3 + .4\)</span>
</li>
<li>
<span class="math">\(P(D) = .3 + .4 - .2 = .5\)</span>
</li>
</ul></li>
<li><p>
An ordinary deck of 52 cards is shuffled. What is the probability that the top four cards have
</p>

<ol>
<li><p>
different denominations?
</p>

<ul>
<li>
There are <span class="math">\(52!\)</span> total possibilities.
</li>
<li>
I think that each of the four top cards can be counted as <span class="math">\(13 \choose 1\)</span>.
</li>
<li>
Wait no.
</li>
<li>
You're picking four cards, ignore the rest of the deck.
</li>
<li>
There are <span class="math">\(52 \cdot 51 \cdot 50 \cdot 49\)</span> total ways.
</li>
<li>
Denomiation means 1 or 2 or 3 or ... or King or Ace.
</li>
<li>
After each is picked, there are four less cards that are &quot;available.&quot; <span class="math">\[\frac{52 \cdot 48 \cdot 44 \cdot 40}{52 \cdot 51 \cdot 50 \cdot 49}\]</span>
</li>
</ul></li>
<li><p>
different suits?
</p>

<ul>
<li>
Like the last problem, there are <span class="math">\(52 \cdot 51 \cdot 50 \cdot 49\)</span> total ways.
</li>
<li>
But this time, each time a card is picked, there are 13 less choices. <span class="math">\[\frac{52 \cdot 39 \cdot 26 \cdot 13}{52 \cdot 51 \cdot 50 \cdot 49}\]</span>
</li>
</ul></li>
</ol></li>
<li><p>
Urn A contains 3 red and 3 black balls, whereas urn B contains 4 red and 6 black balls. If a ball is randomly selected from each urn, what is the probability that the balls will be the same color?
</p>

<ul>
<li>
R = &quot;both balls were red.&quot;
</li>
<li>
B = &quot;both balls were black.&quot;
</li>
<li>
There is a probability of zero that both these events happen.
</li>
<li>
<span class="math">\(P(R \cup B) = P(R) + P(B) - 0\)</span>
</li>
<li>
<span class="math">\(P(R) = \frac{3 \cdot 4}{6 \cdot 10}\)</span>
</li>
<li>
<span class="math">\(P(R) = \frac{3 \cdot 6}{6 \cdot 10}\)</span>
</li>
</ul></li>
<li><p>
In a state lottery, a player must choose 8 of the numbers from 1 to
</p>

<ol>
<li>
The lottery commission then performs an experiment that selects 8 of these 40 numbers. Assuming that the choice of the lottery commission is equally likely to be any of the <span class="math">\(40 \choose 8\)</span> combinations, what is the probability that a player has
</li>
</ol>

<ul>
<li><p>
all 8 of the numbers selected by the lottery commission?
</p>

<ul>
<li>
They can only have one of the combinations. <span class="math">\[\frac{1}{40 \choose 8}\]</span>
</li>
</ul></li>
<li><p>
7 of the numbers selected by the lottery commission?
</p>

<ul>
<li>
I think that there are <span class="math">\(40 \cdot 39 ... \cdot 32\)</span> total combinations.
</li>
<li>
Having one of them would mean, I think, <span class="math">\[\frac{40 \cdot 39 ... \cdot 33}{40 \cdot 39 ... \cdot 32}\]</span>
</li>
<li>
Unfortunately, this is very wrong. The better way to think of this is there are 8 choose 7 ways of picking 7 numbers.
</li>
<li>
I can't work out on my own what you need to multiply this by, I think it has something to do with the number of 7 combination choices in a 40 choice pool.
</li>
</ul></li>
<li><p>
at least 6 of the numbers selected by the lottery commission?
</p></li>
</ul></li>
<li><p>
From a group of 3 freshmen, 4 sophomores, 4 juniors, and 3 seniors a committee of size 4 is randomly selected. Find the probability that the committee will consist of
</p>

<ol>
<li><p>
1 from each class <span class="math">\[\frac{3 \times 4 \times 4 \times 3}{14 \choose 4}\]</span>
</p>

<ul>
<li>
Every number in the numerator is like &quot;number in class&quot; choose 1, which is just &quot;number in class.&quot;
</li>
</ul></li>
<li><p>
2 sophomores and 2 juniors <span class="math">\[\frac{{4 \choose 2} \times {4 \choose 2}}{14 \choose 4}\]</span>
</p>

<ul>
<li>
You have to pick two out of each 4, and then divide it by the entire sample space.
</li>
</ul></li>
<li><p>
only sophomores or juniors $$
</p>

<ul>
<li>
It could be all sophomores or all juniors, or a combination thereof, so just count them as one group with 8 choose 4.
</li>
</ul></li>
</ol></li>
<li><p>
For a finite set A, let N(A) denote the number of elements in A. Show that: <span class="math">\[ N(A \cup B) = N(A) + N(B) − N(AB) \]</span>
</p>

<pre><code>     +--------------+    +-------------+
    /                \  /               \
   /                  \/                 \
  /                   /\                  \
 /                   /  \                  \
|                   |    |                 | 
|          A        | AB |       B         |
|                   |    |                 |
 \                   \  /                  /
  \                   \/                  /
   \                  /\                 /
    \                /  \               /
     +--------------+    +-------------+
</code></pre>

<ul>
<li>
<span class="math">\(A \cup B\)</span> is equal to those elements in A plus all the elements in B.
</li>
<li>
But that's double counting! Region AB is both in A and B, so you need to substract one instance of AB from the outcome.
</li>
<li>
Therefore, <span class="math">\(N(A \cup B) = N(A) + N(B) - N(AB)\)</span>
</li>
</ul></li>
<li><p>
Consider an experiment that consists of six horses, numbered 1 through 6, running a race, and suppose that the sample space consists of the 6! possible orders in which the horses finish. Let A be the event that the number-1 horse is among the top three finishers, and let B be the event that the number-2 horse comes in second. How many outcomes are in the event <span class="math">\(A \cup B\)</span>?
</p>

<ul>
<li>
<span class="math">\(A = \)</span> &quot;the number-1 horse is among the top three finishers&quot;
</li>
<li>
<span class="math">\(B = \)</span> &quot;B be the event that the number-2 horse comes in second&quot;
</li>
<li>
<span class="math">\(A \cup B = \)</span> &quot;the number-1 horse is among the top three finishers and the number-2 horse came second.&quot;
</li>
<li>
For <span class="math">\(B\)</span>, only one outcome is necessary, that the number-2 horse is second, the rest are totally variable.
</li>
<li>
I think this means that the possible outcomes are therefore: <span class="math">\[\frac{5!}{6!}\]</span> because you're only taking one of the choices &quot;off the table.&quot;
</li>
<li>
Similarly, there are <span class="math">\(5!\)</span> ways of specifying the position of horse one.
</li>
<li>
<p>So what is <span class="math">\(N(AB)\)</span>?</p>
<ul>
<li>
There is only one way for the number two horse to be in the second position.
</li>
<li>
There are two options for the first horse to be in the top three, then, in position one and position three.
</li>
<li>
For each of these options, there are 4 choices for the unclaimed spot. <span class="math">\(2 \times 4!\)</span> <span class="math">\[N(A \cup B) = \frac{5!}{6!} + \frac{5!}{6!} - (2 \times 4!)\]</span>
</li>
</ul></li>
</ul></li>
<li><p>
A 5-card hand is dealt from a well-shuffled deck of 52 playing cards. What is the probability that the hand contains at least one card from each of the four suits?
</p>

<ul>
<li>
This is event is really asking what is the probability that the first four cards are of different suits. (The fifth isn't relevant because it <em>has</em> to be a repreated suit.) <span class="math">\[\frac{{52 \choose 1}}{52} \frac{{39 \choose 1}}{51} \frac{{26 \choose 1}}{50} \frac{{13 \choose 1}}{49}\]</span>
</li>
</ul></li>
<li><p>
A basketball team consists of 6 frontcourt and 4 backcourt players. If players are divided into roommates at random, what is the probability that there will be exactly two roommate pairs made up of a backcourt and a frontcourt player?
</p></li>
</ol>

<h3>
Midterm Review Powerpoint
</h3>

<h4>
Counting
</h4>

<ul>
<li><p>
<strong>Multiplication principle</strong>
</p>

<ul>
<li>
If we can classify a set of objects by a sequence of decisions, then the (# objects) = (# choices on first decisions) x ... x (# of choices in last decision)
</li>
<li>
Mathematically, <span class="math">\[|A_1 \times A_2 \times ... \times A_n| = |A_1| \cdot |A_2| \cdot ... \cdot |A_n|\]</span>
</li>
</ul></li>
<li><p>
<strong>Permutations</strong>
</p>

<ul>
<li>
Number of ways to put <span class="math">\(n\)</span> things in order: <span class="math">\[n(n - 1)\times(n - 2)\times...\times 2 \times 1 = n!\]</span>
</li>
<li>
Number of ways to put <span class="math">\(k\)</span>-out-of-<span class="math">\(n\)</span> things in order: <span class="math">\[n(n - 1)\times(n - 2)\times ... \times (n - k + 1) = \frac{n!}{(n -k)!}\]</span>
</li>
</ul></li>
<li><p>
<strong>Permutations with repeats</strong>
</p>

<ul>
<li>
Ways to order <span class="math">\(n\)</span> objects, with <span class="math">\(n_1\)</span> alike, ..., <span class="math">\(n_r\)</span> alike. <span class="math">\[\frac{n!}{n_1! ... n_r!} = {n \choose n_1, ..., n_r}\]</span>
</li>
</ul></li>
<li><p>
<strong>Combinations</strong>
</p>

<ul>
<li>
Ways to choose <span class="math">\(k\)</span>-out-of-<span class="math">\(n\)</span> things where order doesn't matter: <span class="math">\[\frac{n!}{k!(n - k)!} = {n \choose k}\]</span>
</li>
</ul></li>
<li><p>
<strong>Partitions</strong>
</p>

<ul>
<li>
Number of ways to divide <span class="math">\(n\)</span> indistinguishable obects in <span class="math">\(r\)</span> non-empty piles: <span class="math">\[{n - 1 \choose r - 1}\]</span>
</li>
<li>
Number of ways to divide <span class="math">\(n\)</span> indistinguishable objects in <span class="math">\(r\)</span> piles, empty allowed: <span class="math">\[n + r - 1 \choose r - 1 \]</span>
</li>
</ul></li>
<li><p>
<strong>Binomial theorem</strong> <span class="math">\[(x + y)^n = \sum_{k = 0}^n {n \choose k}x^k y^{n-k}\]</span>
</p></li>
<li>
<strong>Combinatorial identities</strong> <span class="math">\[\sum_{k = 0}^n {n \choose k} = 2^n\]</span> <span class="math">\[{n \choose k} = {n - 1 \choose k} + {n - 1 \choose k - 1}\]</span>
</li>
<li><p>
<strong>Multinomial theorem</strong> <span class="math">\[(x_1 + ... + x_r)^n =\]</span> <span class="math">\[\sum_{0 \lt n_1 \lt ... \lt n_r \lt n} {n \choose n_1, ..., n_r} x_1^{n_1}...x_r^{n_r}\]</span>
</p>

<ul>
<li>
Sum over all ways to express <span class="math">\(n\)</span> as sum of <span class="math">\(r\)</span> non-negative integers: <span class="math">\[0 \le n_1 \le ... \le n_r \le n : n_1 + ... + n_r = n\]</span>
</li>
</ul></li>
</ul>

<h4>
Probability
</h4>

<ul>
<li><p>
<strong>Sample spaces and events</strong>
</p>

<ul>
<li>
Sample spaces is a set <span class="math">\(S\)</span>
</li>
<li>
Events are subsets of <span class="math">\(S\)</span>
</li>
<li>
Intersection of <span class="math">\(E\)</span> and <span class="math">\(F\)</span>: “Both <span class="math">\(E\)</span> and <span class="math">\(F\)</span> happen”
</li>
<li>
Union of <span class="math">\(E\)</span> and <span class="math">\(F\)</span>: “Either <span class="math">\(E\)</span> or <span class="math">\(F\)</span> or both happen”
</li>
<li>
Complement of <span class="math">\(E\)</span>: “<span class="math">\(E\)</span> does not happen”
</li>
<li>
<span class="math">\(E\)</span> and <span class="math">\(F\)</span> are mutually exclusive if intersection of <span class="math">\(E\)</span> and <span class="math">\(F\)</span> is empty
</li>
</ul></li>
<li><p>
<strong>Basic identities</strong>
</p>

<ul>
<li>
<span class="math">\(P(\emptyset) = 0\)</span>
</li>
<li>
<span class="math">\(P(E^c) = 1 - P(E)\)</span>
</li>
<li>
<span class="math">\(P(E \cup F) = P(E) + P(F) - P(E \cap F)\)</span>
</li>
<li>
<span class="math">\(P(E) = P(E \cap F) + P(E \cap F^c)\)</span>
</li>
<li>
If <span class="math">\(P\)</span> and <span class="math">\(F\)</span> are mutually exclusive, then <span class="math">\[P(E \cup F) = P(E) + P(F)\]</span>
</li>
</ul></li>
<li><p>
<strong>Uniform probability measure</strong>
</p>

<ul>
<li>
If all outcomes are equally likely (fair dice, fair coin, random card, poker hand, etc) then, <span class="math">\[P(E) = \frac{|E|}{|S|}\]</span>
</li>
</ul></li>
<li><p>
<strong>Conditional probability</strong>
</p>

<ul>
<li>
Probability of <span class="math">\(E\)</span>, given that <span class="math">\(F\)</span> happened <span class="math">\[P(E|F) = \frac{P(E \cap F)}{P(F)}\]</span>
</li>
</ul></li>
<li><p>
<strong>Bayes's theorem</strong>
</p>

<ul>
<li>
Think <span class="math">\(E\)</span> is &quot;evidence&quot; and <span class="math">\(F\)</span> is &quot;outcome&quot; <span class="math">\[P(F|E) = \frac{P(E|F)P(F)}{P(E|F)P(F) + P(E|F^c)P(F^c)}\]</span>
</li>
</ul></li>
<li><p>
<strong>Multiplication rule for conditional probability</strong> <span class="math">\[P(E_1 \cap ... \cap E_n) = \]</span> <span class="math">\[P(E_1)P(E_2|E_1)P(E_3|E_1\cap E_2) ... \]</span> <span class="math">\[P(E_n | E_1 \cap E_2 \cap ... \cap E_{n-1})\]</span>
</p></li>
<li><p>
<strong>Indepedant events</strong>
</p>

<ul>
<li>
<span class="math">\(E\)</span> and <span class="math">\(F\)</span> are <em>independant</em> if <span class="math">\[P(E \cap F) = P(E) \times P(F) \]</span>
</li>
<li>
Equivilently, <span class="math">\[P(E|F) = P(E) \lor P(F) = 0\]</span>
</li>
</ul></li>
<li><p>
<strong>Prosectutor's fallacy</strong> <span class="math">\[P(E|F) \neq P(F|E)\]</span>
</p></li>
</ul>

<h4>
Review Problems
</h4>

<ol>
<li><p>
How many ways can we walk up/right from point A to point B?
</p>

<pre><code>  +----+----+----+ B
  |    |    |    |
  +----+----+----+
  |    |    |    |
  +----+----+----+
  |    |    |    |
  +----+----+----+
  |    |    |    |
A +----+----+----+
</code></pre>

<ul>
<li>
A clever way to solve this problem is to represent &quot;ups&quot; and &quot;right&quot; as characters in a string.
</li>
<li>
You have to traverse &quot;on the lines&quot; - the strings will have to be seven characters long.
</li>
<li>
You can only go &quot;up&quot; four times in any given run.
</li>
<li>
You can only go &quot;right&quot; 3 times. <span class="math">\[7 \choose 3, 4\]</span>
</li>
</ul></li>
<li><p>
You have 18 non-identical children.
</p>

<ol>
<li><p>
Assign them to 4 possibily empty teams.
</p>

<ul>
<li>
Again, you can cleverly solve this problem with strings.
</li>
<li>
But I'll do it with sets just to be different.
</li>
<li>
You have four sets, <span class="math">\(A, B, C, D\)</span>, each of which can have can have an elements from the set <span class="math">\(\lbrace 1, ..., 18 \rbrace\)</span>
</li>
<li>
Because each of the four sets can possibly have 18 children in it, there are <span class="math">\(4^18\)</span> possibilities.
</li>
<li>
Gosh that didn't go as well as planned.
</li>
</ul></li>
<li><p>
How many ways can 18 identical balls be divided into 4 possibily empty groups.
</p>

<ul>
<li>
The formula for putting <span class="math">\(n\)</span> objects into <span class="math">\(r\)</span> possibly empty groups is <span class="math">\[n + r - 1 \choose r - 1 \]</span>
</li>
<li>
There are 18 identical balls (<span class="math">\(n\)</span>) and 4 possibly empty groups (<span class="math">\(R\)</span>) <span class="math">\[18 + 4 - 1 \choose 4 - 1 \]</span>
</li>
<li>
Refer to stars and bars, partitions.
</li>
</ul></li>
</ol></li>
<li><p>
How many ways can 18 children be divided into 4 groups so that: Group 1 has 4 children, Group 2 has 6 children, Group 3 has 5 children, Group 4 has 3 children?
</p></li>
<li><p>
Suppose we deal a random 5-card poker hand. What is the probability that we get exactly 1 Queen? What about exactly 4 Hearts? Are they independent?
</p>

<ul>
<li><p>
<span class="math">\(E = \)</span> &quot;get exactly one Queen&quot;
</p>

<ul>
<li>
There are 52 choose 5 possible hands.
</li>
<li>
There are 4 Queens in every deck.
</li>
<li>
Observe there is symettry. <span class="math">\[\frac{{4 \choose 1} \times {48 \choose 4}}{52 \choose 5}\]</span>
</li>
</ul></li>
<li><p>
<span class="math">\(F = \)</span> &quot;exactly four Hearts&quot;
</p>

<ul>
<li>
There are 52 choose 5 possible hands.
</li>
<li>
There are 13 hearts in every deck. <span class="math">\[\frac{{13 \choose 4}\times{39 \choose 1}}{52 \choose 5}\]</span>
</li>
</ul></li>
<li><p>
Events are independant if <span class="math">\(P(E \cap F) = P(E) \times P(F) \)</span>
</p>

<ul>
<li>
What is the probability of getting one Queen and exactly four Hearts?
</li>
<li>
Of course, the Queen <em>could be</em> the Heart.
</li>
<li>
So you have to count the instances that the Queen is the fifth card (not heart) as well as those where as the one where <em>it is</em> a Heart.
</li>
<li>
There are 12 choose 4 ways of picking an Heart which is <em>not</em> a queen, and then 3 choose 1 ways of picking a Queen which <em>is not</em> a Heart.
</li>
<li>
There is only one Queen of Hearts, and you need 3 more hearts out of th remaining 12 Hearts after that.
</li>
<li>
Then you need to pick 1 of the remaining 36 cards which are not Hearts <em>and</em> not Queens. <span class="math">\[\frac{{12 \choose 4}{3 \choose 1} + {12 \choose 3}{36 \choose 1}}{{52 \choose 5}}\]</span>
</li>
<li>
I can eyeball that I'd very much doubt this multiplication adds up, and if this were the test, I'd show it. But being as I know it actually doesn't multiply based on the answers, I'm just going to skip the step. &quot;Leave it as an exercise for the reader&quot;, as textbooks would say.
</li>
</ul></li>
</ul></li>
<li><p>
In a 5-card poker hand, how many ways can we get a 3-of-a-kind? Full house does not count.
</p>

<ul>
<li>
There are 13 different ranks, and this requires you choose one of them.
</li>
<li>
Of that rank, which has 4 members, you must choose 3 of them.
</li>
<li>
There are 12 ranks left, of which you must choose 2.
</li>
<li>
Because full houses don't count (which I assume is when the other two cards are of the same suit as well), you have to pick 1 from 4, and then 1 from 3 (they <em>can't</em> be the same). <span class="math">\[{13 \choose 1}\times{4 \choose 3}\times{12 \choose 2}\times{4 \choose 1}\times{3 \choose 1}\]</span>
</li>
</ul></li>
</ol>

<h3>
Conditional Probability and Independence - Self Test Problems and Exercises
</h3>

<ol>
<li><p>
In a game of bridge, West has no aces. What is the probability of his partner’s having (a) no aces? (b) 2 or more aces? (c) What would the probabilities be if West had exactly 1 ace?
</p>

<ul>
<li>
I would answer this, but like most people under the age of 64, I have no idea how bridge works.
</li>
</ul></li>
<li><p>
The probability that a new car battery functions for over 10,000 miles is .8, the probability that it functions for over 20,000 miles is .4, and the probability that it functions for over 30,000 miles is .1. If a new car battery is still working after 10,000 miles, what is the probability that
</p>

<ul>
<li><p>
its total life will exceed 20,000 miles?
</p>

<ul>
<li>
<span class="math">\(E = \)</span> &quot;battery functions for over 10,000 miles&quot;
</li>
<li>
<span class="math">\(F = \)</span> &quot;battery functions for over 20,000 miles&quot;
</li>
<li>
<span class="math">\(G = \)</span> &quot;battery functions for over 30,000 miles&quot;
</li>
<li>
I think what the question is asking is what is the probability that the the battery functions for 20,000 miles given that it went for 10,000 miles. I don't <em>quite</em> have the intuition behind that, but I just kind of &quot;smell&quot; that that is what this problem wants. <span class="math">\[P(F|E) = \frac{P(E \cap F)}{P(E)}\]</span>
</li>
<li>
Because <span class="math">\(E \in F\)</span>, <span class="math">\(E \cap F\)</span> is simply <span class="math">\(F\)</span>. <span class="math">\[P(F|E) = \frac{P(F)}{P(E)} = \frac{.4}{.8}\]</span>
</li>
</ul></li>
<li><p>
its additional life will exceed 20,000 miles?
</p>

<ul>
<li>
This is the probability that the car reaches 30,000 miles given that it's gone both 20,000 and 10,000. <span class="math">\[P(G|E) = \frac{P(E \cap G)}{P(E)} = \frac{.1}{.8}\]</span>
</li>
</ul></li>
</ul></li>
<li><p>
How can 20 balls, 10 white and 10 black, be put into two urns so as to maximize the probability of drawing a white ball if an urn is selected at random and a ball is drawn at random from it?
</p>

<ul>
<li>
This question is actually pretty cool.
</li>
<li>
I can't quite work out how you'd prove it rigoursly, but the basic idea is that the number will always float around 50% <strong>unless</strong> you do one clever trick.
</li>
<li>
Make it so that one of the urns has only one white ball in it, making it so 100% of the time you at least get one white.
</li>
<li>
Then place the rest in the other, waste none of the white on the &quot;hole in one&quot; so they jack up the probability on the second draw.
</li>
</ul></li>
<li><p>
Urn A contains 2 white balls and 1 black ball, whereas urn B contains 1 white ball and 5 black balls. A ball is drawn at random from urn A and placed in urn B. A ball is then drawn from urn B. It happens to be white. What is the probability that the ball transferred was white?
</p>

<ul>
<li>
<span class="math">\(P(E) = \)</span> &quot;the transferred ball was white&quot;
</li>
<li>
<span class="math">\(P(F) = \)</span> &quot;the ball drawn from B was white&quot; <span class="math">\[P(E|F) = \frac{P(E \cap F)}{P(F)}\]</span>
</li>
</ul></li>
</ol>

<h3>
Homework 1
</h3>

<ol>
<li><p>
Give sample spaces that model the outcomes for the following experiments. You may use a regular expression or other formalisms that you find convenient.
</p>

<ol>
<li><p>
Rolling 3 dice:
</p>

<p>
<span class="math">\[\lbrace (i, j, k) &amp;#58;|&amp;#58; i, j, k \in \lbrace 1, 2, 3, 4, 5, 6 \rbrace\rbrace\]</span>
</p></li>
<li><p>
Rolling a die until an even result comes up, or the die is rolled three times:
</p>

<p>
<span class="math">\[\lbrace (i), (ji), (jji), (jjj...ji) &amp;#58; | &amp;#58; i \in \lbrace 2, 4, 6\rbrace j \in \lbrace 1, 3, 5\rbrace\rbrace \]</span>
</p></li>
<li><p>
Tossing a pair of coins until they both come up tails.
</p>

<p>
<span class="math">\[\lbrace (i), (j, i), (j, j, i), (j, j, j, ..., i) &amp;#58; | &amp;#58; i = (TT), j = \lbrace HH, TH, HT \rbrace\rbrace\]</span>
</p></li>
<li><p>
Draw 2 balls from an urn which contains 6 balls, each with a distinct label from <span class="math">\(\lbrace 1,2,3,4,5,6\rbrace\)</span>.
</p>

<p>
<span class="math">\[\lbrace i,j &amp;#58; | &amp;#58; i, j \in \lbrace 1,2,3,4,5,6\rbrace \land i \neq j \rbrace \]</span>
</p></li>
<li><p>
Draw 1 ball from the same urn, then replace it and draw a ball again.
</p>

<p>
<span class="math">\[\lbrace i,j &amp;#58; | &amp;#58; i, j \in \lbrace 1,2,3,4,5,6\rbrace\]</span>
</p></li>
</ol></li>
</ol>

<h3>
Homework 2
</h3>

<ol>
<li><p>
If <span class="math">\(P(A) = \frac{1}{2}\)</span>, <span class="math">\(P(B) = \frac{1}{5}\)</span>, and <span class="math">\(P(A\cup B) = 3/5\)</span>, find:
</p>

<ul>
<li><p>
<span class="math">\(P(A\cap B)\)</span>
</p>

<ul>
<li>
The idea is that the probability of both <span class="math">\(A\)</span> and <span class="math">\(B\)</span> happening is equal to the probability of <span class="math">\(A\)</span> happening multiplied by the probability of <span class="math">\(B\)</span> happeneing. <span class="math">\[\frac{1}{2} \times \frac{1}{5} = \frac{1}{10}\]</span>
</li>
</ul></li>
<li><p>
<span class="math">\(P(A^c \cup B)\)</span> <span class="math">\[P(A^c \cup B) = P(A^c) + P(B) - P(A^c \cap B)\]</span> <span class="math">\[\frac{1}{2} + \frac{1}{5} - \frac{1}{10} = .6\]</span>
</p></li>
<li>
<p><span class="math">\(P(A^c \cap B)\)</span></p>
<ul>
<li>
The idea is that being as <span class="math">\(P(A) = \frac{1}{2}\)</span>, the complement is equally likely to happen, because <span class="math">\(P(S) = 1\)</span> and <span class="math">\(P (A) = 1- P(A^c)\)</span> <span class="math">\[\frac{1}{10}\]</span>
</li>
</ul></li>
</ul></li>
<li><p>
How many elements are there in the set
</p>

<p>
<span class="math">\(\lbrace x : 10^7 \le x \le 10^8\)</span>, and the base 10 representation of x has no digit used twice<span class="math">\(\rbrace\)</span>?
</p>

<ul>
<li>
<span class="math">\(10^7 = 10000000\)</span>, and <span class="math">\(10^8 = 100000000\)</span>
</li>
<li>
The smallest number possible is <span class="math">\(12345678\)</span>
</li>
<li>
The largest number possible is <span class="math">\(98765432\)</span>
</li>
<li>
For the first number, you have 9 total integers to choose from (it can't be zero). <span class="math">\[9 \choose 1\]</span>
</li>
<li>
For the second number, zero is now an option. So likewise, <span class="math">\[9 \choose 1\]</span>
</li>
<li>
Now, for every value after this one, you still have to choose one, you just have one less choice, all the way down to there being 2 remaining numbers that will be left out of your element. <span class="math">\[{9 \choose 1}{9 \choose 1}{8 \choose 1}{7 \choose 1}{6 \choose 1}{5 \choose 1}{4 \choose 1}{3 \choose 1}\]</span>
</li>
</ul></li>
<li><p>
An army output has 19 posts to staff using 30 indistinguishable guards. How many ways are there to distribute the guards if no post is left empty?
</p>

<ul>
<li>
Stars and bars. <span class="math">\[{{30 - 1} \choose {19 - 1}}\]</span>
</li>
</ul></li>
<li><p>
What is the coefficient of <span class="math">\(x^{10}y^{13}\)</span> when <span class="math">\((x + y)^{23}\)</span> is expanded?
</p>

<ul>
<li>
<p>Binomial theorem: <span class="math">\[(x + y)^n = \sum_{k = 0}^n {n \choose k}x^k y^{n-k}\]</span></p>
<ul>
<li>
<span class="math">\(n = 23\)</span>
</li>
<li>
<span class="math">\(k = 10\)</span> <span class="math">\[(x + y)^23 = {23 \choose 10}x^10 y^{23 - 10}\]</span>
</li>
</ul></li>
</ul></li>
<li><p>
What is the coefficient of <span class="math">\(w^{9}x^{31}y^{4}z^{19}\)</span> when <span class="math">\((w + x + y + z)^{63}\)</span> is expanded? How many monomials appear in the expansion?
</p>

<ul>
<li>
This is the multinomial theorem when <span class="math">\(r = 4\)</span> and <span class="math">\(n = 63\)</span>. <span class="math">\[{63 \choose 4, 9, 19, 32} x_1^{4}x_2^{9}x_3^{19}x_4^{31}\]</span>
</li>
</ul></li>
</ol>

<h3>
Homework 3
</h3>

<ol>
<li><p>
What is the probability that a 5 card hand has exactly 3 spades?
</p>

<ul>
<li>
This problem has symmetry, every card is equally likely at any given point.
</li>
<li>
Therefore the answer can be expressed by taking the primality of the relevant subset over the sample space.
</li>
<li>
How many 5 card hands contain exactly 3 spades? <span class="math">\[{13 \choose 3}{39 \choose 2}\]</span>
</li>
<li>
The answer is this value over <span class="math">\[{52 \choose 5}\]</span>
</li>
</ul></li>
<li><p>
What is the probability that a 5 card hand has exactly 3 spades, conditioned on having at least one spade?
</p>

<ul>
<li>
<span class="math">\(P(E|F)\)</span> where <span class="math">\(E = \)</span> &quot;having 3 spades,&quot; and <span class="math">\(F = \)</span> &quot;having a least one spade&quot; <span class="math">\[P(E|F) = \frac{P(E \cap F)}{P(F)}\]</span>
</li>
<li>
If you have 3 spades, you definitely have at least one spade.
</li>
<li>
Therefore, <span class="math">\(E \cap F = E\)</span> <span class="math">\[P(E|F) = \frac{P(E)}{P(F)}\]</span>
</li>
<li>
We know <span class="math">\(P(F)\)</span>, and will calculate <span class="math">\(P(E)\)</span> using the complement.
</li>
<li>
The complement is that none of the cards are spades.
</li>
<li>
There are 13 spades in a deck, so there are 39 choose 5 ways of getting a hand with no spades.
</li>
<li>
There are 52 choose 5 total hands, the probability of event <span class="math">\(E\)</span> is: <span class="math">\[ 1 - \frac{{39 \choose 5}}{{52 \choose 5}} \]</span>
</li>
</ul></li>
<li><p>
Suppose <span class="math">\(n\)</span> people each throw a six-sided die. Let <span class="math">\(A_n\)</span> be the event that at least two distinct people roll the same number. Calculate <span class="math">\(P(A_n)\)</span> for <span class="math">\(n=1,2,3,4,5,6,7\)</span>.
</p>

<ul>
<li>
For <span class="math">\(n_7\)</span>, the event that two distinct people roll the same number is inevitable.
</li>
<li>
For the remaining 6 rolls, the complement will be much easier to calculate.
</li>
<li>
What is the likelihood for events 1 through 6 that every roll is unique?
</li>
<li>
<span class="math">\(n_1\)</span> will <em>always</em> be unique.
</li>
<li>
<span class="math">\(n_2\)</span> will be unique 5 out of 6 times, as <span class="math">\(n_1\)</span> rolled some number.
</li>
<li>
<span class="math">\(n_3\)</span>, similarly, will be unique 4 out of 6 times.
</li>
<li>
<span class="math">\(n_4\)</span> will be unique 3 out of 6 times.
</li>
<li>
<span class="math">\(n_5\)</span> will be unique 2 out of 6 times.
</li>
<li>
<span class="math">\(n_6\)</span> will be unique 1 out of 6 times.
</li>
<li>
(Note that <span class="math">\(n_7\)</span> will never be unique.) <span class="math">\[P(A_i) = 1 - \prod_{n = 0}^i \frac{7 - i}{6}\]</span>
</li>
</ul></li>
<li><p>
Suppose we draw 2 balls at random from an urn that contains 5 distinct balls, each with a different number from <span class="math">\(&amp;#123;1,2,3,4,5&amp;#125;\)</span>, and define the events <span class="math">\(A\)</span> and <span class="math">\(B\)</span> as <span class="math">\[A = \text{&quot;5 is drawn at least once&quot;} \quad \text{and} \quad
    B = \text{&quot;5 is drawn twice&quot;}\]</span> Compute <span class="math">\(P(A)\)</span> and <span class="math">\(P(B)\)</span>.
</p>

<ul>
<li>
There are four ways to pick five along with another number. <span class="math">\[P(A) = \frac{4}{5 \choose 2} = 0.4 \]</span>
</li>
<li>
Observe that there is zero probability of getting <em>any</em> number twice. <span class="math">\[P(B) = 0\]</span>
</li>
</ul></li>
<li><p>
In the previous problem, suppose we place the first ball back in the urn before drawing the second. Compute <span class="math">\(P(A),P(B),P(A|B),P(B|A)\)</span> in this version of the experiment.
</p>

<ul>
<li>
I think it would be helpful, again, to consider the complement.
</li>
<li>
The complement of <span class="math">\(A\)</span>, is that 5 is never drawn.
</li>
<li>
Each time you draw, considering that you replace, you have a one in five chance of picking 5.
</li>
<li>
For the complement, then, you have a 4 in five chance of drawing <em>not</em> five. (Apparently not that helpful.) <span class="math">\[P(A) = 1 - \frac{4}{5}^2 = .36\]</span>
</li>
<li>
Okay I think I messed that one up.
</li>
<li><p>
Much simpler version:
</p>

<ul>
<li>
There are <span class="math">\(5^2\)</span> possible drawings.
</li>
<li>
There are 4 ways of drawing 5 first, 4 ways of drawing 5 second, and 1 way of drawing 2 fives which makes for nine ways. <span class="math">\[P(A) = \frac{9}{25} = .36\]</span>
</li>
</ul></li>
<li><p>
For <span class="math">\(P(B)\)</span>, there are still 25 possible ways of drawing.
</p></li>
<li>
What's changed is that there is now only one way to draw five twice - actually getting it twice.
</li>
<li>
Therefore, <span class="math">\[\frac{1}{25}\]</span>
</li>
<li>
For <span class="math">\(P(A|B)\)</span>, the translation is, &quot;what is the probability of A given than B happened.&quot;
</li>
<li>
Well if five was drawn twice, it was <em>definitely</em> drawn once. <span class="math">\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]</span>
</li>
<li>
<span class="math">\(A \cap B\)</span> is equal to <span class="math">\(B\)</span>. <span class="math">\[P(A|B) = \frac{P(B)}{P(B)} = 1\]</span>
</li>
<li>
For <span class="math">\(P(B|A)\)</span>, the &quot;translation&quot; is, what is the probability of rolling a five twice given that a five was rolled once? <span class="math">\[P(B|A) = \frac{P(B \cap A)}{P(A)}\]</span>
</li>
<li>
<span class="math">\(B \cap A\)</span> is still <span class="math">\(B\)</span> <span class="math">\[P(B|A) = \frac{\frac{1}{25}}{.36} = \frac{1}{9}\]</span>
</li>
</ul></li>
<li><p>
Suppose <span class="math">\(5\)</span> percent of cyclists cheat by using illegal doping. The blood test for doping returns positive <span class="math">\(98\)</span> percent of the people doping and <span class="math">\(12\)</span> percent who do not. If Lance’s test comes back positive, what the probability that he is doping? (Ignoring all other evidence, of course...)
</p>

<ul>
<li>
This is a Bayes's theorem type of question. <span class="math">\[P(F|E) = \frac{P(E|F)P(F)}{P(E|F)P(F) + P(E|F^c)P(F^c)}\]</span>
</li>
<li>
Think of <span class="math">\(E\)</span> as &quot;evidence&quot; and <span class="math">\(F\)</span> as &quot;outcome.&quot;
</li>
<li>
Call <span class="math">\(E\)</span> &quot;Lance's blood test came back positive.&quot;
</li>
<li>
Call <span class="math">\(F\)</span> &quot;Lance was doping.&quot;
</li>
<li>
Before we know anything else, Lance being a random cyclist, we know the probability of his doping, <span class="math">\(P(F) = .05\)</span>
</li>
<li>
We also know the probability of Lance's test coming back positive given that he doped, <span class="math">\(P(E|F) = .98\)</span>
</li>
<li>
We <em>also</em> know the probability of Lance's test coming back positive given that he <em>didn't</em> dope, <span class="math">\(P(E|F^c) = .12\)</span>.
</li>
<li>
Finally, we <em>also</em> know the probability of Lance <em>not</em> doping, <span class="math">\(P(F^c) = .95\)</span>.
</li>
<li>
We now have all the elements for Bayes's theorem. <span class="math">\[P(F|E) = \frac{(.98)(.05)}{(.98)(.05) + (.12)(.95)}\]</span>
</li>
</ul></li>
</ol>

<h2>
March 6th, 2013 - Lecture
</h2>

<h3>
Random variables
</h3>

<ul>
<li>
<strong>Random variables</strong>: <span class="math">\(A\)</span> random variable on a sample space, space <span class="math">\(S\)</span> is any function from <span class="math">\(S\)</span> to <span class="math">\(\mathbb{R}\)</span>. <span class="math">\[X \cdot S \to \mathbb{R} \]</span>
</li>
<li>
<p><strong>Example</strong>: Tossing 3 coins <span class="math">\[S = \lbrace HHH, ... TTT \rbrace\]</span></p>
<ul>
<li>
Define <span class="math">\(X = \)</span> &quot;number of heads that came up.&quot; <span class="math">\[X \cdot R \to R\]</span> <span class="math">\[X(HHH) = 3\]</span> <span class="math">\[X(TTT) = 0\]</span>
</li>
<li>
<span class="math">\(Y =\)</span> &quot;number of tails that come up.&quot;
</li>
<li>
Then, <span class="math">\(Y(w) = 3 - X(w)\)</span> for all <span class="math">\(w \in S\)</span>.
</li>
<li>
<span class="math">\(Z = \)</span> &quot;winnings in game where you win a dollar for each H, lose a dollar for each T.&quot; <span class="math">\[Z(HHH) = 3\]</span>
</li>
</ul></li>
<li>
<p>If <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are RVs (random variables), and <span class="math">\(a, b, c \in \mathbb{R}\)</span>, then <span class="math">\(Z = aX + bY + c\)</span> is a RV.</p>
<ul>
<li>
<strong>Indicator random variable</strong> for each event <span class="math">\(A \subseteq S\)</span>
</li>
</ul></li>
<li>
<strong>Example</strong>: Suppose we toss <span class="math">\(b\)</span> balls into <span class="math">\(N\)</span> bins. Define <span class="math">\(X_i\)</span> as the number of balls that land in the <span class="math">\(i\)</span>th bin (for <span class="math">\(i = 1, ... , n\)</span>). So <span class="math">\(X_1 + X_2 = \)</span> numbers of balls in the 1st or 2nd bincs, <span class="math">\[\forall w \left(\sum_{i = 1}^n X_i(w) = b\right)\]</span>
</li>
<li>
<strong>Example</strong>: When rolling two dice <span class="math">\[X((i, j)) = i + j\]</span> <span class="math">\[\to Y((i, j)) = |i - j|\]</span>
</li>
</ul>

<h3>
Range of a random variable
</h3>

<ul>
<li>
<strong>Range of a RV</strong>: The set of values that <span class="math">\(X\)</span> takes.
</li>
<li>
<strong>Example</strong>: For <span class="math">\(X = \)</span> &quot;number H's in 3 win tosses.&quot; <span class="math">\[Range(X) = \lbrace0,1,2,3\rbrace\]</span> <span class="math">\[Range(X) = \lbrace0,1,2,3\rbrace\]</span>
</li>
</ul>

<h3>
Partition of a random variable
</h3>

<pre><code>        S
    +-------+    x     +-------+ 
    |  A_1  | -------&gt; |  a_1  |
    |  A_2  | -------&gt; |  a_2  |
    |  A_3  | -------&gt; |  a_3  |
    |   .   |          |   .   |
    |   .   | -------&gt; |   .   |
    |   .   |          |   .   |
    |  A_k  | -------&gt; |  a_k  |
    +-------+          +-------+
</code></pre>

<p>
<span class="math">\[A_1 = \lbrace w \in S &amp;#58; : &amp;#58; X(w) = a_1 \rbrace \]</span> <span class="math">\[A_2 = \lbrace w \in S &amp;#58; : &amp;#58; X(w) = a_2 \rbrace \]</span>
</p>

<ul>
<li>
<p>Suppose <span class="math">\(Range(X) = \lbrace a_1, ... a_k \rbrace\)</span>,</p>
<ul>
<li>
For each <span class="math">\(i\)</span>, define <span class="math">\(A_i = \lbrace w \in S &amp;#58; : &amp;#58; X(w) = a_i \rbrace\)</span> for <span class="math">\(i = 1 ... k\)</span>. <span class="math">\[A_1 \subseteq S\]</span>
</li>
<li>
<strong>Claim</strong>: For all <span class="math">\(i \neq j\)</span>, <span class="math">\(A_i \cap A_j = \emptyset\)</span>.
</li>
<li>
<strong>Claim</strong>: Every <span class="math">\(w \in S\)</span> is some <span class="math">\(A_i\)</span>.
</li>
<li>
This means that <span class="math">\(\lbrace A_1, ..., A_k \rbrace\)</span> &quot;partition <span class="math">\(S\)</span>.&quot;
</li>
</ul></li>
<li>
<p><strong>Example</strong>: If <span class="math">\(x = \)</span> &quot;number of Hs in 3 tosses,&quot; what is <span class="math">\(A_x\)</span>? <span class="math">\[Range(X) = \lbrace 0, 1, 2, 3 \rbrace (k = 4)\]</span></p>
<ul>
<li>
Need <span class="math">\(\lbrace A_0, A_1, A_2, A_3 \rbrace\)</span> where <span class="math">\(A_i = \lbrace w \in S &amp;#58; : &amp;#58; X(w) = i \rbrace\)</span> <span class="math">\[A_0 = \lbrace TTT \rbrace\]</span>
</li>
</ul></li>
<li>
<p><strong>Example</strong>: Flip a coin until we get <span class="math">\(H\)</span>, on <span class="math">\(n\)</span> flips <span class="math">\[S = \lbrace H, TH, ..., T^{n - 1}H, T^n \rbrace\]</span></p>
<ul>
<li>
<span class="math">\(X = \)</span> &quot;number of flips&quot;
</li>
<li>
<p>What is <span class="math">\(A_x\)</span>?</p>
<ul>
<li>
<span class="math">\(Range(X) = \lbrace1, ... , n \rbrace\)</span>
</li>
<li>
Need <span class="math">\(A_1, ..., A_n\)</span> where <span class="math">\(A_i = X^{-1}(i)\)</span>
</li>
</ul></li>
</ul></li>
</ul>

<h3>
Frequency Function of a random variable
</h3>

<ul>
<li>
Formalize &quot;<span class="math">\(X\)</span> takes value <span class="math">\(I\)</span> with probability <span class="math">\(p\)</span>&quot;
</li>
<li><p>
If <span class="math">\(Range(X) = \lbrace a_1, ... , a_n \rbrace\)</span>, we write <span class="math">\[P(X = a_i) = P(A_i) = P({w : X(w) = a_i})\]</span>
</p>

<ul>
<li>
<strong>Frequency function of x</strong> (also probability mass function): <span class="math">\[f_x(a_i) = P(x = a_i)\]</span>
</li>
</ul></li>
<li><p>
<strong>Example</strong>: <span class="math">\(X = \)</span> &quot;number of Hs in 3 tosses&quot;
</p>

<ul>
<li>
<span class="math">\(f_x(0) = P(X = 0) = \frac{1}{8}\)</span>
</li>
<li>
<span class="math">\(f_x(1) = P(X = 1) = \frac{3}{8}\)</span>
</li>
<li>
<span class="math">\(f_x(2) = P(X = 2) = \frac{3}{8}\)</span>
</li>
<li>
<span class="math">\(f_x(3) = P(X = 3) = \frac{1}{8}\)</span>
</li>
</ul></li>
</ul>

<h3>
Independent random variables
</h3>

<ul>
<li>
<strong>Independent</strong>: If <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are RVs on <span class="math">\(S\)</span>, they are <strong>independant</strong> if for all <span class="math">\(i, j\)</span> <span class="math">\[P(X = i \land Y = j)\]</span>
</li>
</ul>

<h3>
Repeated Indepedent Trials
</h3>

<ul>
<li>
Start with a sample space <span class="math">\(T\)</span> with probability measure <span class="math">\(P\)</span>.
</li>
<li>
Formalize repeating <span class="math">\(n\)</span> times: <span class="math">\[S^{(n)} = \lbrace w = (w_1, ..., w_2) &amp;#58; : &amp;#58; w_i \in T \rbrace = T^n\]</span>
</li>
</ul>

<h3>
Bernoulli trials
</h3>

<ul>
<li>
<p>Sample space <span class="math">\(S = \lbrace s, f \rbrace\)</span>, &quot;success&quot; and &quot;failure.&quot;<br /></p>
<ul>
<li>
Define <span class="math">\(P(s) = p\)</span>, <span class="math">\(P(f) = 1 - p\)</span>
</li>
<li>
Consider repeat <span class="math">\(n\)</span> times <span class="math">\(S^{(n)} = \)</span> &quot;all string of length <span class="math">\(n\)</span> with <span class="math">\(s\)</span> and <span class="math">\(t\)</span>&quot; <span class="math">\[P^{(n)}(w) = P(w_1)P(w_2) ... P(w_n)\]</span>
</li>
</ul></li>
</ul>

<h2>
March 11th, 2013 - Reading
</h2>

<h3>
4.1 Random Variables
</h3>

<ul>
<li>
These quantities of interest, or, more formally, these real-valued functions defined on the sample space, are known as <strong>random variables</strong>.
</li>
<li>
The total number of events that occur without specifying the order in which they came up.
</li>
</ul>

<h4>
Example 1a
</h4>

<ul>
<li><p>
Suppose that our experiment consists of tossing 3 fair coins. If we let Y denote the number of heads that appear, then Y is a random variable taking on one of the values 0, 1, 2, and 3 with respective probabilities
</p>

<p>
<span class="math">\[P\lbrace Y = 0 \rbrace = P \lbrace (T, T, T) \rbrace = \frac{1}{8} \]</span> <span class="math">\[P\lbrace Y = 1 \rbrace = P \lbrace (H, T, T), (T, H, T), (T, T, H), \rbrace = \frac{3}{8} \]</span> <span class="math">\[P\lbrace Y = 2 \rbrace = P \lbrace (H, H, T), (T, H, H), (H, T, H), \rbrace = \frac{3}{8} \]</span> <span class="math">\[P\lbrace Y = 3 \rbrace = P \lbrace (H, H, H) \rbrace = \frac{1}{8} \]</span>
</p></li>
</ul>

<h2>
March 11th, 2013 - Lecture: More on Random Variables; Expectation
</h2>

<h3>
Introduction
</h3>

<ul>
<li>
Topics: More on binomial random variables.
</li>
<li>
Geometric and negative binomial random variables.
</li>
<li>
Definition of expectation and the alternative formulation.
</li>
<li>
Reading: See notes part 3 on Sakai.
</li>
</ul>

<h3>
Warmup for Homework
</h3>

<ul>
<li>
<strong>Example</strong>: Deal a 5 card hand, let <span class="math">\(X = \)</span> &quot;the number of aces&quot;, <span class="math">\(Y = \)</span> &quot;the number of spades&quot;, and <span class="math">\(Z = X(w) \cdot Y(w)\)</span> <span class="math">\[ Range(X) = \lbrace 0, 1, 2, 3, 4 \rbrace \]</span> <span class="math">\[ Range(Y) = \lbrace 0, 1, 2, 3, 4, 5 \rbrace \]</span> <span class="math">\[ Range(Z) = Range(XY) = \lbrace 0, ..., 9 \rbrace \]</span>
</li>
</ul>

<h4>
Are <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> Independent?
</h4>

<ul>
<li>
<p>If indepedent, then dor all <span class="math">\(i \in Range(X), j \in Range(Y)\)</span> <span class="math">\[ P(X = i \cap Y = j) = P(X = i)P(Y = j) \]</span> <span class="math">\[ P(X = 2 \cap Y = 5) = \emptyset \]</span></p>
<ul>
<li>
But <span class="math">\(P(X = 2) \gt 0, P(Y = 5)\)</span> so not independent.
</li>
</ul></li>
</ul>

<h3>
Binomial Frequency Function
</h3>

<ul>
<li>
<span class="math">\(S^{(n)} = \)</span> &quot;all strings of length <span class="math">\(N\)</span> formed with <span class="math">\(s\)</span> and <span class="math">\(t\)</span> characters&quot;
</li>
<li>
<span class="math">\(S^{(n)}(w) = P(w_1) ... P(w_n)\)</span> where <span class="math">\(w = w_1 ... w_n\)</span> <span class="math">\[ p^k (q - p)^{n - k} \]</span>
</li>
<li>
<span class="math">\(S_n = \)</span> &quot;the number of <span class="math">\(s\)</span>s in <span class="math">\(n\)</span> trials&quot;
</li>
<li>
<span class="math">\(Range(S_n) = \lbrace 0, 1, ... n \rbrace \)</span>
</li>
<li>
<span class="math">\(f_{s_n}(k) = {n \choose k}P^k (1 - p)^{n - k} = P(S_n = k)\)</span>
</li>
<li><p>
Need <span class="math">\[ \sum_{k = 0}^n f_{s_n}(k) 1\]</span> <span class="math">\[ = \sum_{k = 0}^n {n \choose k} P^k (1 - p)^{n - k} = \]</span> <span class="math">\[ (p + P(1 - p))^n = 1^n = 1 \]</span>
</p></li>
<li><p>
<strong>Example</strong>: An airplane has 200 seats, but we should 202 tickets. Assume passengers fail to show with probability <span class="math">\(0.03\)</span> <em>indepedantly</em>. What is the chance that flight is over full?
</p>

<ul>
<li>
This &quot;<span class="math">\(s\)</span>&quot; is &quot;passenger <span class="math">\(i\)</span> fails to show.&quot;
</li>
<li>
<span class="math">\(X = \)</span> &quot;the number of passengers who failr to show&quot;
</li>
<li>
Event we want is <span class="math">\(X = 1 \cup X = 1\)</span> <span class="math">\[ P(X = 0) = {202 \choose 0}p^0 (1 - p)^{202 - 0} = .03\]</span> <span class="math">\[ P(X = 1) = {202 \choose 2}p^1 (1 - p)^{202 - 1}\]</span>
</li>
</ul></li>
</ul>

<h3>
Repeat Bernoulli trial until success
</h3>

<ul>
<li>
<span class="math">\(S&#39;= \lbrace s, fs, ffs, ... \rbrace \)</span>
</li>
<li>
<span class="math">\(P^{(n)} = \)</span> as before
</li>
<li>
Define <span class="math">\(W_1 = \)</span> &quot;the number of trials until success.&quot;
</li>
<li>
<span class="math">\(Range(W_1) = \lbrace 1, 2, ... \rbrace\)</span>
</li>
<li>
<span class="math">\(A_{w_1} = \lbrace A_1, A_2, ... \rbrace\)</span>, where <span class="math">\(A_i = \lbrace W : W_1(w) = i \rbrace \subseteq S\)</span>.
</li>
<li>
<span class="math">\(P(i) = P(W_1 = 1) = P(A_i) = (1 - p)^{i - 1} p\)</span> <span class="math">\[\sum_{i  = 1}^\infty (1 - p)^{i - 1}\]</span> <span class="math">\[ = p \cdot \frac{1}{p} =  \]</span>
</li>
</ul>

<h3>
Negative Binomial Frequency Fun
</h3>

<ul>
<li>
<p>Repeat Bernoulli unti lwe get <span class="math">\(k\)</span> <span class="math">\(s\)</span>'s <span class="math">\[ S = f\ast s f\ast s ... f \ast s \]</span></p>
<ul>
<li>
<span class="math">\(W_k = \)</span> &quot;number of triials until <span class="math">\(k\)</span> <span class="math">\(s\)</span>'s&quot;
</li>
<li>
<span class="math">\(A_{w_k} = \lbrace k, k + 1, ... \rbrace&lt;/li&gt; &lt;li&gt;\)</span>A_k = ss ... s <span class="math">\(&lt;/li&gt; &lt;li&gt;\)</span>A_{k + 1} = fs ... s, sfs ... s, ..., ss...sfs $ <span class="math">\[|A_{k+1}| = k\]</span>
</li>
</ul></li>
</ul>

<h3>
Miracle of the linearity of expectation
</h3>

<ul>
<li>
For any random variables <span class="math">\(X\)</span> and <span class="math">\(Y\)</span>, the expectation splits and you sum it. <span class="math">\[E(X + Y) = E(X) + E(Y)\]</span>
</li>
</ul>

<h3>
St. Petersburg Paradox
</h3>

<ul>
<li>
You pay c dollars
</li>
<li>
I flip coin until H. Say it takes <span class="math">\(i\)</span> flips.
</li>
<li>
I give you <span class="math">\(2^{i - 1}\)</span> dollars
</li>
</ul>

<h2>
March 13th, 2013 - Homework 4
</h2>

<ol>
<li><p>
(3 points) In the card game bridge we deal 13-card hands to 4 players named North, South, East, and West (all 52 cards are dealt). What is the probability that East and West have no spades?
</p>

<ul>
<li><p>
The total number of possibilities in the sample space for this situation is the total number of possible card orders over the number of cards each player is dealt factorial raised to the fourth.
</p>

<p>
<span class="math">\[\frac{52!}{13!^4}\]</span>
</p></li>
<li><p>
There are 13 Spades in a deck of cards, and then 39 non-Spade cards.
</p></li>
<li><p>
For the hands of East and West, who collectively represent 26 cards, and who are vying for 39 non-Spade cards, this is the number of possibilities:
</p>

<p>
<span class="math">\[39 \choose 26\]</span>
</p></li>
<li><p>
North and South also account for 26 total cards, and they are vying for the remaining cards including Spades. Twenty-six cards were taken out of the deck by East and West.
</p>

<p>
<span class="math">\[{26 \choose 26} = 1\]</span>
</p></li>
<li><p>
The probability is therefore:
</p>

<p>
<span class="math">\[\frac{{39 \choose 13}\times{26 \choose 13}\times{26 \choose 13}\times{13 \choose 13}}{\frac{52!}{13!^4}}\]</span>
</p></li>
</ul></li>
<li><p>
(4 points) An urn contains <span class="math">\(n&gt;0\)</span> white balls and <span class="math">\(m&gt;0\)</span> black balls. Suppose we draw two balls without replacement. What is the probability that the balls are of the same color? What if we draw them with replacement? Show your work. Which of these probabilities is larger? Briefly explain some intuition for why one should be larger.
</p>

<ul>
<li><p>
Without replacement
</p>

<ul>
<li><p>
There are <span class="math">\(n + m\)</span> balls in all, and we’re interested in choosing 2 of them. The sample space is:
</p>

<p>
<span class="math">\[{m + n \choose 2}\]</span>
</p></li>
<li><p>
Call <span class="math">\(W\)</span> “drawing two white balls” and <span class="math">\(B\)</span> ``drawing two black balls. Then <span class="math">\(W \cup B\)</span> is either of those events happening or both.
</p></li>
<li><p>
The events are mutually exclusive, the drawings cannot all be the same for the same drawing.
</p>

<p>
<span class="math">\[P(W \cup B) = P(W) + P(B)\]</span> <span class="math">\[= \frac{{m \choose 2} + {n \choose 2}}{{m + n \choose 2}}\]</span>
</p></li>
</ul></li>
<li><p>
With replacement
</p>

<ul>
<li><p>
The events are, again, mutually exclusive.
</p></li>
<li><p>
Being as the balls are placed back in the urn, each event is equally likely to occur.
</p></li>
<li><p>
Count the number of either color, place it over total balls, and multiply it by itself to represent that the event has to happen twice.
</p></li>
<li><p>
Do the same for the other color, add the two values.
</p>

<p>
<span class="math">\[\left(\frac{m}{m + n}\right)^2 + \left(\frac{n}{m + n}\right)^2\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
(4 points) Again consider an urn with <span class="math">\(n&gt;0\)</span> white balls and <span class="math">\(m &gt;0\)</span> black balls. Suppose we draw <span class="math">\(r\geq 1\)</span> balls from the urn without replacement. What is the probability that we draw exactly <span class="math">\(k\)</span> white balls?
</p>

<ul>
<li><p>
The probability of getting a white, where <span class="math">\(i\)</span> is the number of total balls already drawn, is
</p>

<p>
<span class="math">\[\left(\frac{n - i}{m + n - i}\right)\]</span>
</p></li>
<li><p>
And the probability of getting a black under the same conditions is
</p>

<p>
<span class="math">\[\left(\frac{m - i}{m + n - i}\right)\]</span>
</p></li>
<li><p>
So draw <span class="math">\(k\)</span> white balls, and then transfer the counter to drawing the remaining balls to avoid over-counting
</p>

<p>
<span class="math">\[\left(\prod_{i = 0}^{k - 1} \frac{n - i}{m + n - i} \right) \times \left(\prod_{i = k}^{r - 1} \frac{m - (i - k)}{m + n - i} \right)\]</span>
</p></li>
</ul></li>
<li><p>
(4 points) A box contains a mixture of cubes and spheres and any of these objects can be either white or black. Suppose the box contains <span class="math">\(4\)</span> black cubes, <span class="math">\(6\)</span> black spheres, <span class="math">\(6\)</span> white cubes, and <span class="math">\(x\)</span> white spheres. Consider the experiment of drawing a random object from the box, and let <span class="math">\(A\)</span> be the event that a cube is drawn and <span class="math">\(B\)</span> be the event that a black object is drawn. If <span class="math">\(A\)</span> and <span class="math">\(B\)</span> are independent, what is <span class="math">\(x\)</span>?
</p>

<ul>
<li><p>
The probability of a cube being drawn is the total number of cubes over the total number of cubes and spheres.
</p>

<p>
<span class="math">\[\frac{4 + 6}{4 + 6 + 6 + x}\]</span>
</p></li>
<li><p>
The probability of a black object being drawn is similarly the total number of black objects divided by the total number of objects
</p>

<p>
<span class="math">\[\frac{6 + 4}{4 + 6 + 6 + x}\]</span>
</p></li>
<li><p>
Consider the complement of <span class="math">\(A \cup B\)</span>, it is the event that a non-black and non-cube item is draw, which is a white sphere.
</p>

<p>
<span class="math">\[P(x) = P((A \cup B)^c)\]</span>
</p></li>
</ul></li>
<li><p>
(10 points total) Two fair dice are rolled. Define the random variables <span class="math">\(X =\)</span> the sum of the two rolls, <span class="math">\(Y =\)</span> the maximum of the two rolls, <span class="math">\(Z =\)</span> the absolute value of the difference of the two rolls and <span class="math">\(W = XY\)</span> (i.e., the product of <span class="math">\(X\)</span> and <span class="math">\(Y\)</span>).
</p>

<ol>
<li><p>
(2 points) What are <span class="math">\({\mathrm{Range}}(X)\)</span>, <span class="math">\({\mathrm{Range}}(Y)\)</span>, <span class="math">\({\mathrm{Range}}(Z)\)</span> and <span class="math">\({\mathrm{Range}}(W)\)</span>?
</p>

<ul>
<li><p>
<span class="math">\({\mathrm{Range}}(X) = \lbrace 2, 3, 4, ... , 11, 12 \rbrace \)</span>
</p></li>
<li><p>
<span class="math">\({\mathrm{Range}}(Y) = \lbrace 1, 2, 3, 4, 5, 6 \rbrace \)</span>
</p></li>
<li><p>
<span class="math">\({\mathrm{Range}}(Z) = \lbrace 0, 1, 2, 3, 4, 5 \rbrace\)</span>
</p></li>
<li><p>
<span class="math">\({\mathrm{Range}}(W) = \lbrace i \times j &amp;#58; : &amp;#58; i \in \lbrace 2, 3, 4, ... , 11, 12 \rbrace, j \in \lbrace 1, 2, 3, 4, 5, 6 \rbrace\rbrace \)</span>
</p></li>
</ul></li>
<li><p>
(2 points) What are the partitions <span class="math">\({\mathcal{A}}_X\)</span> and <span class="math">\({\mathcal{A}}_Z\)</span>?
</p>

<ul>
<li><p>
<span class="math">\({\mathcal{A}}_X\)</span>
</p>

<ul>
<li><p>
<span class="math">\(2 = 1 + 1\)</span>
</p></li>
<li><p>
<span class="math">\(3 = 1 + 2\)</span>
</p></li>
<li><p>
<span class="math">\(4 = 1 + 3 = 2 + 2\)</span>
</p></li>
<li><p>
<span class="math">\(5 = 1 + 4 = 3 + 2\)</span>
</p></li>
<li><p>
<span class="math">\(6 = 1 + 5 = 2 + 4 = 3 + 3\)</span>
</p></li>
<li><p>
<span class="math">\(7 = 1 + 6 = 2 + 5 = 3 + 4\)</span>
</p></li>
<li><p>
<span class="math">\(8 = 2 + 6 = 3 + 5 = 4 + 4\)</span>
</p></li>
<li><p>
<span class="math">\(9 = 3 + 6 = 4 + 5\)</span>
</p></li>
<li><p>
<span class="math">\(10 = 4 + 6 = 5 + 5\)</span>
</p></li>
<li><p>
<span class="math">\(11 = 5 + 6\)</span>
</p></li>
<li><p>
<span class="math">\(12 = 6 + 6\)</span>
</p></li>
</ul></li>
<li><p>
<span class="math">\({\mathcal{A}}_Z\)</span>
</p>

<ul>
<li><p>
<span class="math">\(0 = 1 - 1 = 2 - 2 = 3 - 3 = 4 - 4 = 5 - 5 = 6 - 6\)</span>
</p></li>
<li><p>
<span class="math">\(1 = 6 - 5 = 5 - 4 = 4 - 3 = 3 - 2 = 3 - 2 = 2 - 1\)</span>
</p></li>
<li><p>
<span class="math">\(2 = 5 - 4 = 5 - 3 = 4 - 2 = 3 - 1\)</span>
</p></li>
<li><p>
<span class="math">\(3 = 6 - 3 = 5 - 2 = 4 - 1\)</span>
</p></li>
<li><p>
<span class="math">\(4 = 6 - 2 = 5 - 1\)</span>
</p></li>
<li><p>
<span class="math">\(5 = 6 - 1\)</span>
</p></li>
</ul></li>
</ul></li>
</ol></li>
</ol>

<h2>
March 13th, 2013 - Lecture: Linearity of expectation
</h2>

<h3>
Introduction
</h3>

<ul>
<li><p>
Topics:
</p>

<ul>
<li>
Linearity of expectation.
</li>
<li>
Expectation of binomial, geometric, and negative binomial frequency functions.
</li>
</ul></li>
<li><p>
Applications to computing expectations:
</p>

<ul>
<li>
Birthday problem,
</li>
<li>
balls into bins,
</li>
<li>
and coupon collecting.
</li>
</ul></li>
<li><p>
Reading: See notes part 3 on Sakai.
</p></li>
</ul>

<h2>
March 25th, 2013 - Lecture: Markov's Inequality; Begin Variance
</h2>

<h3>
Introduction
</h3>

<ul>
<li><p>
Topics:
</p>

<ul>
<li>
Markov's inequality for non-negative random variables.
</li>
<li>
Definition of variance and some calculations of variance.
</li>
</ul></li>
<li><p>
Reading: See notes part 4 on Sakai.
</p></li>
</ul>

<h2>
March 26th, 2013 - Office Hours
</h2>

<ol>
<li>
Norwa on number one
</li>
<li>
<p>Notes on number two</p>
<ul>
<li>
There are no tricks, apply the formula. <span class="math">\[E(X) = \sum_{a_i \in R(X)}a_i P(X = a_1)\]</span>
</li>
<li>
What is E(X) for X = minimum of two dice? <span class="math">\[P(x = 6) = P(\lbrace(6, 6) \rbrace) = \frac{1}{36} \]</span> <span class="math">\[P(x = 5) = P(\lbrace (5, 6), (6, 5), (5, 5) \rbrace) = \frac{3}{36}\]</span>
</li>
</ul></li>
<li>
<p>Notes on number three</p>
<ul>
<li>
<p>Find <span class="math">\(E(X)\)</span></p>
<ol>
<li>
Find some random variables <span class="math">\(X_1, ..., X_n\)</span> such that <span class="math">\(x = x_1 + ... + x_n\)</span>
</li>
<li>
Now <span class="math">\(E(X) = E(x_1) + ... E(x_n)\)</span>
</li>
</ol></li>
<li>
We're going to buy x boxes of cereal
</li>
<li>
Call x the number of unique toys
</li>
<li>
We're always stopping at n boxes.
</li>
<li>
<p>Imagine there six types of toys, red, green, blue, purple, orange, indigo.</p>
<ul>
<li>
When I open my n boxes, I check off the color I got until I get them all.
</li>
<li>
Call <span class="math">\(x_1\)</span> &quot;got a red toy indicator&quot;
</li>
<li>
There's another for each toy, 1 through 6.
</li>
</ul></li>
</ul></li>
<li><p>
Notes on number four
</p>

<ul>
<li>
There m men and w women in a line in a single line, random order, all orders equally likely.
</li>
<li>
You can pick any pair of seats, this pair has formed a couple, if a mw or a wm occur.
</li>
<li>
Use linearity of expectation
</li>
<li><p>
x = number of couples
</p>

<pre><code>    m w m
_ _ _ _ _ _ _ _
1 2 3 4 5 6 7 8
x_1
</code></pre></li>
<li><p>
<span class="math">\(E(X_1) = \)</span> P(couple in a seat 1 and 2)
</p></li>
</ul></li>
<li>
<p>Notes on five</p>
<ul>
<li>
What is a Bernoulli trial?
</li>
<li>
<p>So you see this problem there are <span class="math">\(2^12\)</span> different outcomes, different trials.</p>
<ul>
<li>
Big number, don't want to work it out by hand.
</li>
</ul></li>
<li>
We know something about this though, it's connected to something we've seen before. <span class="math">\[ E(\frac{S}{12}) \]</span>
</li>
</ul></li>
</ol>

<h2>
March 27th, 2013 - Homework 4
</h2>

<ol>
<li><p>
(10 points total) Two teams <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are playing each other in the World Series, which is a best-of-seven-game match that ends when one team wins 4 games. Assume that team <span class="math">\(x\)</span> wins each game with probability <span class="math">\(p\)</span>, and that the outcome of each game constitutes an independent trial.
</p>

<ol>
<li><p>
(0.5 points) What is the probability that <span class="math">\(x\)</span> wins the first four games?
</p>

<ul>
<li><p>
There is one way for x to win in four, and that’s winning four in a row.
</p></li>
<li><p>
By the multiplicity principle, we can multiply the chance of winning each game to get the probability of winning all games.
</p>

<p>
<span class="math">\[p^4\]</span>
</p></li>
</ul></li>
<li><p>
(2 points) What is the probability that <span class="math">\(x\)</span> wins four games after at most five game have been played?
</p>

<ul>
<li><p>
Sample space, strings, x character means x won, y character means y won.
</p>

<pre><code>        xxxx
        yxxxx
        xyxxx
        xxyxx
        xxxyx
</code></pre></li>
<li><p>
The team represented by y cannot win at the end because after 4 x wins, the games halt.
</p></li>
<li><p>
P(x wins in less than five games) = <span class="math">\(P(\mathrm{&amp;#123;xxxx, yxxxx, xyxxx, xxyxx, xxxyx&amp;#125;})\)</span>
</p></li>
<li><p>
There is a <span class="math">\((1 - p)\)</span> chance of y winning.
</p></li>
<li><p>
The x team still needs to win 4 games, which has a probability of <span class="math">\(p^4\)</span>.
</p></li>
<li><p>
Multiplying these two values gets you the likelihood that this happens for any given instance of x winning in less than five games.
</p></li>
<li><p>
To get all of the possible less than 5 games combinations, multiply by the number there are, which is 4.
</p></li>
<li><p>
Then, add the probability of just 4 straight victories. <span class="math">\[(1 - p) \times p^4 \times 4 + p^4\]</span>
</p></li>
</ul></li>
<li><p>
(2 points) What is the probability that <span class="math">\(x\)</span> will win four games before <span class="math">\(y\)</span> wins four games? (i.e., What is the probability that <span class="math">\(x\)</span> wins the Series?)
</p>

<ul>
<li><p>
To begin, lets enumerate some possibilities using strings.
</p>

<p>
<span class="math">\[&amp;#123;\mathrm{xxxx}&amp;#125;\]</span>
</p>

<p>
<span class="math">\[&amp;#123;\mathrm{yxxxx,
        xyxxx,
        xxyxx,
        xxxyx}&amp;#125;\]</span>
</p>

<p>
<span class="math">\[&amp;#123;\mathrm{yyxxxx,
        yxyxxx,
        ...,
        xxxyyx}&amp;#125;\]</span>
</p>

<p>
<span class="math">\[&amp;#123;\mathrm{yyyxxxx,
        yyxyxxx,
        ...,
        xxxyyyx}&amp;#125;\]</span>
</p></li>
<li><p>
These are all the possibilities for x winning. Name these <span class="math">\(X_1\)</span> through <span class="math">\(X_4\)</span> and notice the sum of their probabilities to be the probability that <span class="math">\(x\)</span> wins four games.
</p></li>
<li><p>
For <span class="math">\(X_3\)</span>, there are <span class="math">\({5 \choose 2,3}\)</span> ways of ordering x and y, y has to win twice, and x still has to win four times.
</p></li>
<li><p>
You have to subtract off the cases where there is a y at the end,
</p>

<p>
<span class="math">\[{4 \choose 1,3} \times (1 - y)^2 \times p^4\]</span>
</p></li>
<li><p>
For <span class="math">\(X_4\)</span>, there are <span class="math">\({6 \choose 3,3}\)</span> ways of ordering x and y, y has to win thrice, and x still has to win four times.
</p>

<p>
<span class="math">\[{5 \choose 2,3} \times (1 - y)^3 \times p^4\]</span>
</p></li>
<li><p>
Now add 4 straight victories:
</p>

<p>
<span class="math">\[{4 \choose 1,3} \times (1 - y)^2 \times p^4 + {5 \choose 2,3} \times (1 - y)^3 \times p^4 + p^4\]</span>
</p></li>
</ul></li>
<li><p>
(0.5 points) Calculate and simplify your answer in part (c) when <span class="math">\(p=1/2\)</span> and when <span class="math">\(p=2/3\)</span>.
</p>

<ul>
<li><p>
<span class="math">\(p = 1 / 2 : 1/2\)</span>
</p></li>
<li><p>
<span class="math">\(p = 2 / 3 : 1808/2187\)</span>
</p></li>
</ul></li>
<li><p>
(1 point) Let <span class="math">\(X\)</span> be the random variable that counts the number of games that are played. What is <span class="math">\({\mathrm{Range}}(X)\)</span>?
</p>

<p>
<span class="math">\[R(X) = &amp;#123;4, 5, 6, 7 &amp;#125;\]</span>
</p></li>
<li><p>
(2 points) What is <span class="math">\(P(X=7)\)</span>?
</p>

<p>
<span class="math">\[{6 \choose 3}(1 - p)^3 p^4 + {6 \choose 3} p^3 (1 - p)^3 p^4\]</span>
</p></li>
<li><p>
(2 points) What is <span class="math">\(P(X\geq 6)\)</span>?
</p>

<p>
<span class="math">\[{5 \choose 2}(1 - p)^2 p^4 + {5 \choose 2} p^2 (1 - p)^4 + {6 \choose 3} (1 - p)^3 p^4 + {6 \choose 3} p^3 (1 - p)^4\]</span>
</p></li>
</ol></li>
<li><p>
(4 points) Suppose we roll two fair dice. Let the random variable <span class="math">\(X=\)</span> “the minimum of the two dice” and <span class="math">\(Y =\)</span> “the absolute value of the difference of the two dice”. Find <span class="math">\(E(X)\)</span> and <span class="math">\(E(Y)\)</span>.
</p>

<ul>
<li><p>
<span class="math">\(E(X)\)</span>
</p>

<ul>
<li><p>
The formula to solve this is
</p>

<p>
<span class="math">\[E(X) = \sum_{a_i \in R(X)}a_i P(X = a_1)\]</span>
</p></li>
<li><p>
The sample space has a cardinality of 36, as there 6 choices for each of the two choices.
</p></li>
<li><p>
All rolls are equally likely in a fair dice.
</p></li>
<li><p>
Starting with the highest element in the range (which is the set containing 1 through 6), there is only one way 6 can be the minimum.
</p>

<p>
<span class="math">\[P(x = 6) = P(\lbrace(6, 6) \rbrace) = \frac{1}{36}\]</span>
</p></li>
<li><p>
There are 3 ways of “getting 5”, and that’s rolling two fives, and then both variations of a five and a six.
</p>

<p>
<span class="math">\[P(x = 5) = P(\lbrace(5, 5), (5, 6), (6,5) \rbrace) = \frac{3}{36}\]</span>
</p></li>
<li><p>
Apply the same pattern,
</p>

<p>
<span class="math">\[P(x = 4) = P(\lbrace(4, 4), (4, 5), (5,4), (4,6), (6,4) \rbrace) = \frac{5}{36}\]</span> <span class="math">\[P(x = 3) = P(\lbrace(3, 3), (3, 4), (4,3), (3,5), (5,3), (3,6), (6,3) \rbrace) = \frac{7}{36}\]</span> <span class="math">\[P(x = 2) = P(\lbrace(2, 2), (2, 3), (3,2), (2, 4), (4, 2), (2, 5), (5,2), (2,6), (6,2)) = \frac{9}{36}\]</span>
</p></li>
<li><p>
Notice that there 2 more each time.
</p>

<p>
<span class="math">\[P(x = 1) = \frac{11}{36}\]</span>
</p></li>
<li><p>
Now sum them and multiply them by their value (<span class="math">\(a_i\)</span>) to find expected value,
</p>

<p>
<span class="math">\[E(X) = \left(6 \times \frac{1}{36}\right) + \left(5 \times \frac{3}{36}\right) + \left(4 \times \frac{5}{36}\right) + \left(3 \times \frac{7}{36}\right) + \left(2 \times \frac{9}{36}\right) + \frac{11}{36} = 2.5277777778\]</span>
</p></li>
</ul></li>
<li><p>
<span class="math">\(E(Y)\)</span>
</p>

<ul>
<li><p>
The range is <span class="math">\(&amp;#123;0, 1, 2, 3, 4, 5&amp;#125;\)</span>.
</p></li>
<li><p>
This is another application of the formula <span class="math">\[E(X) = \sum_{a_i \in R(X)}a_i P(X = a_1)\]</span>
</p></li>
<li><p>
There are 6 ways of getting 0, <span class="math">\(&amp;#123;6 - 6, 5 - 5, ...&amp;#125;\)</span>. For <span class="math">\(i = 0\)</span>, <span class="math">\[0 \times P(X = 0) = 0 \times \frac{6}{36} = 0\]</span>
</p></li>
<li><p>
There are 10 ways of getting 1, <span class="math">\(&amp;#123;6 - 5, 5 - 6, 5 - 4, 4 - 5, 4 - 3, 3 - 4, 3 - 2, 2 - 3, 2 - 1, 1 - 2&amp;#125;\)</span>. For <span class="math">\(i = 1\)</span>, <span class="math">\[1 \times P(X = 1) = 1 \times \frac{10}{36}\]</span>
</p></li>
<li><p>
There are 8 ways of getting 2, <span class="math">\(&amp;#123;6 - 4, 4 - 6, 5 - 3, 3 - 5, 4 - 2, 2 - 4, 1 - 5, 5 - 1&amp;#125;\)</span>. For <span class="math">\(i = 2\)</span>, <span class="math">\[2 \times P(X = 2) = 2 \times \frac{8}{36} = \frac{4}{9}\]</span>
</p></li>
<li><p>
There are 6 ways of getting 3, <span class="math">\(&amp;#123;6 - 3, 3 - 6, 5 - 2, 2 - 5, 4 - 1, 1 - 4&amp;#125;\)</span>. For <span class="math">\(i = 3\)</span>, <span class="math">\[3 \times P(X = 3) = 3 \times \frac{6}{36} = \frac{2}{3}\]</span>
</p></li>
<li><p>
There are 4 ways of getting 4, <span class="math">\(&amp;#123;6 - 2, 2 - 6, 5 - 1, 1 - 5&amp;#125;\)</span>. For <span class="math">\(i = 4\)</span>, <span class="math">\[4 \times P(X = 4) = 4 \times \frac{4}{36} = \frac{4}{9}\]</span>
</p></li>
<li><p>
There are 2 ways of getting 5, <span class="math">\(&amp;#123;6 - 1, 1 - 6&amp;#125;\)</span>. For <span class="math">\(i = 5\)</span>, <span class="math">\[5 \times P(X = 5) = 5 \times \frac{2}{36} = \frac{5}{18}\]</span>
</p></li>
<li><p>
We know we’ve covered the sample space because the sum of the specific instances is the same as the cardinality of the sample space.
</p></li>
<li><p>
The expected value, <span class="math">\[E(X) = 0 + \frac{2}{9} + \frac{4}{9} + \frac{2}{3} + \frac{4}{9} +  \frac{5}{18} = 1.94 \approx 2\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
(4 points) Suppose boxes of cereal are filled with a random prize, each drawn from independently and uniformly from <span class="math">\(6\)</span> possible prizes. If we buy <span class="math">\(N\)</span> boxes of cereal, what is the expected number of distinct prizes we will collect?
</p>

<ul>
<li><p>
Let <span class="math">\(X_1 ... X_6\)</span> be the identifier for each of 6 unique toys
</p>

<p>
<span class="math">\[E\left(\sum I_{E_i}\right) = \sum_{i = 1}^6 E(I_{E_i}) = 6(1 - \left(5/6)\right)^n\]</span>
</p></li>
</ul></li>
<li><p>
(4 points) A group of <span class="math">\(m\)</span> men and <span class="math">\(w\)</span> randomly sit in a single row at a theater. If a man and woman are seated next to each other we say they form a couple. (Couples can overlap, meaning that one person can be a member of two couples.) What is the expected number of couples?
</p>

<ul>
<li><p>
Use linearity of expectation for each pair of seats.
</p></li>
<li><p>
Let <span class="math">\(x\)</span> equal the number of seats.
</p></li>
<li><p>
Then, <span class="math">\(E(X_1) = P(\)</span>couple in a seat 1 and 2<span class="math">\()\)</span>, <span class="math">\(E(X_2) = P(\)</span>couple in a seat 2 and 3<span class="math">\()\)</span>, etc.
</p></li>
<li><p>
For any given seat, there is a <span class="math">\(\frac{1}{2}\)</span> chance of their being a man or a woman in the seat.
</p></li>
<li><p>
The four possibilities are {mw, wm, mm, ww}.
</p></li>
</ul></li>
<li><p>
(3 points) Suppose an experiment tosses a fair coin twice; the experiment “succeeds” if both tosses were Heads. We repeat this experiment for 12 independent trials. Let <span class="math">\(N\)</span> be the random variable that counts the fraction of trials that are successful (so <span class="math">\(N = S/12\)</span>, where <span class="math">\(S\)</span> is the number of successful trials). Find <span class="math">\(E(N)\)</span>.
</p>

<p>
<span class="math">\[E(N) = E\left(\frac{S}{12}\right)\]</span> <span class="math">\[S = &amp;#123; x_1 + ... + x_12 &amp;#125;\]</span> <span class="math">\[\frac{1}{12} E(S)\]</span> <span class="math">\[E(S) = \sum_{a_i \in R(S)} a_i \times P(X_i)\]</span> <span class="math">\[P(X_i) = \frac{1}{4}\]</span> <span class="math">\[E(S) = 12 \times E(X_i)\]</span> <span class="math">\[\frac{1}{12}E(S) = 12 \times \frac{1}{4}\]</span> <span class="math">\[E(S) = \frac{1}{4}\]</span>
</p></li>
</ol>

<h2>
March 27th, 2013 - Lecture: Computing Variances
</h2>

<h3>
Introduction
</h3>

<ul>
<li><p>
Topics:
</p>

<ul>
<li>
Expectation of a function of a random variable.
</li>
<li>
Variance of a sum of independent random variables.
</li>
<li>
Variance of the bernoulli, binomial, geometric, and negative binomial distributions.
</li>
</ul></li>
<li><p>
Reading: See notes part 4 on Sakai.
</p></li>
</ul>

<table border=1 style="text-align:center;margin-left:auto; margin-right:auto;">
  <tbody>
    <!-- Results table headers -->
    <tr>
      <th></th>
      <th>
&quot;story&quot;
</th>
      <th>
<span class="math">\(R(X)\)</span>
</th>
      <th>
fx
</th>
      <th>
<span class="math">\(E\)</span>
</th>
      <th>
<span class="math">\(V\)</span>
</th>
    </tr>
    <tr>
      <td>
Bernoulli(P)
</td>
      <td>
s/f with prob p
</td>
      <td>
<span class="math">\(\lbrace 0, 1 \rbrace\)</span>
</td>
      <td>
<span class="math">\(p, 1 + p\)</span>
</td>
      <td>
<span class="math">\(p\)</span>
</td>
      <td>
<span class="math">\(p(1 - p)\)</span>
</td>
    </tr>
    <tr>
      <td>
Binom(n, p)
</td>
      <td>
#<span class="math">\(s\)</span> in <span class="math">\(n\)</span> trials
</td>
      <td>
<span class="math">\(\lbrace 0, ... , n \rbrace \)</span>
</td>
      <td>
<span class="math">\(f_x(i) = {n \choose i} p^i (1 - p)^{n - i}\)</span>
</td>
      <td>
<span class="math">\(pn\)</span>
</td>
      <td>
...
</td>
    </tr>
    <tr>
      <td>
Geom(p)
</td>
      <td>
# of trials until first s
</td>
      <td>
<span class="math">\(\lbrace 1, 2, ... \rbrace \)</span>
</td>
      <td>
<span class="math">\(f_x(i) = (1 - p)^{i - 1} p\)</span>
</td>
      <td>
<span class="math">\(\frac{1}{p}\)</span>
</td>
      <td>
...
</td>
    </tr>
    <tr>
      <td>
NegBiom(k,p)
</td>
      <td>
# trials until kth s
</td>
      <td>
<span class="math">\(\lbrace k, k + 1, ... \rbrace\)</span>
</td>
      <td>
<span class="math">\(f_x (i) = {i - 1 \choose k - 1} p^k (1 - p)^{i -k}\)</span>
</td>
      <td>
<span class="math">\(k \times \frac{1}{p}\)</span>
</td>
      <td>
...
</td>
    </tr>
  </tbody>
</table>

<h2>
April 1st, 2013 - Lecture: Covariance and Chebyshev's Inequality
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Variance of a sum of dependent random variables and the definition of covariance.
</li>
<li>
Properties of covariance. Independence implies correlation but the converse does not hold.
</li>
<li>
Statement of Chebyshev's inequality and a basic application.
</li>
</ul>

<h3>
Introduction
</h3>

<ul>
<li><p>
If <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are indepedant, then <span class="math">\[V(X + Y) = V(X) + V(Y)\]</span>
</p></li>
<li><p>
If not, then equality may or may not hold.
</p></li>
<li>
<p><strong>Example</strong>: A case where equality does not hold.</p>
<ul>
<li>
Take <span class="math">\(P(X = 1) = \frac{1}{2}\)</span>, <span class="math">\(P(X = 0) = \frac{1}{2}\)</span>, <span class="math">\(x = 1 \to Y = 0\)</span>, <span class="math">\(x = 1, Y = 1\)</span>. <span class="math">\[V(Y) = \frac{1}{4}\]</span>
</li>
<li>
So <span class="math">\[V(X) + V(Y) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}\]</span>
</li>
<li>
<span class="math">\(V(X + Y)\)</span> equals 0? <span class="math">\(X + Y\)</span> will always be 1. <span class="math">\[V(X + Y) \lt V(X) + V(Y)\]</span>
</li>
</ul></li>
</ul>

<h3>
Variance
</h3>

<ul>
<li><p>
&quot;Swinging together&quot; <span class="math">\[V(X + Y) \lt V(X) + V(Y)\]</span>
</p></li>
<li><p>
&quot;Swings unrelated&quot; <span class="math">\[V(X + Y)  =  V(X) + V(Y)\]</span>
</p></li>
<li><p>
&quot;Swings are opposite each other&quot; <span class="math">\[V(X + Y) \gt V(X) + V(Y)\]</span>
</p></li>
<li><p>
We want to measure &quot;how much they're together or opposite.&quot;
</p></li>
<li><p>
The <strong>covariance of <span class="math">\(X\)</span> and <span class="math">\(Y\)</span></strong> is defined to be: <span class="math">\[Cov(x, y) = E[E(x - E[x])(y - E[y])]\]</span>
</p></li>
<li><p>
Intuition
</p>

<ul>
<li><p>
&quot;Swing together&quot; <span class="math">\[Cov(X, Y) \gt 0\]</span>
</p></li>
<li><p>
&quot;Swings unrelated&quot; <span class="math">\[Cov(X, Y)  =  0\]</span>
</p></li>
<li><p>
&quot;Swings opposite&quot; <span class="math">\[Cov(X, Y) \lt 0\]</span>
</p></li>
</ul></li>
<li><p>
<strong>Example</strong> (non rigourous pictures)
</p>

<ul>
<li><p>
Examples, <span class="math">\(x\)</span> is height and <span class="math">\(y\)</span> is shoe size <span class="math">\[Cov(X, Y) \gt 0\]</span>
</p>

<pre><code>  v |               .
  a |             . .
  l |          . . .
  u |        . . .
  e |      . . .
    |    . .  .
  y |    . . 
    |  . 
  0 +-----------------
   0    value of X
</code></pre></li>
<li><p>
Examples, <span class="math">\(x\)</span> is temp and <span class="math">\(y\)</span> is sales of hot chocolate <span class="math">\[Cov(X, Y) \lt 0\]</span>
</p>

<pre><code>  v | .
  a |  . .     
  l |   .  . . 
  u |    .  .  . 
  e |     .  .  .
    |      .  .  .
  y |        .   .
    |            .
  0 +-----------------
   0    value of X
</code></pre></li>
</ul></li>
<li><p>
<strong>Formula</strong>: <span class="math">\[Cov(X, Y) = E(XY) - E(X) \times E(Y)\]</span>
</p></li>
<li><p>
<strong>Claim</strong>: If <span class="math">\(X, Y\)</span> are indepedant, <span class="math">\(Cov(X, Y) = 0\)</span>
</p>

<ul>
<li>
<strong>Proof</strong> <span class="math">\[Cov(X, Y) = E(XY) - E(X)E(Y)\]</span> <span class="math">\[= E(X)E(Y) - E(X)E(Y)\]</span> <span class="math">\[= 0\]</span>
</li>
</ul></li>
</ul>

<h3>
Tchbycheff's Inequility
</h3>

<ul>
<li>
Recall Markov's give tail bound base only on <span class="math">\(E(X)\)</span>
</li>
<li>
Chebyshev's give tail bound using <span class="math">\(E(X)\)</span> and <span class="math">\(V(X)\)</span>
</li>
<li><p>
<strong>Chebyshev's Theorem</strong>: Let <span class="math">\(X\)</span> be a random variable with <span class="math">\(E(X) = \mu\)</span>
</p>

<ul>
<li>
Then for any <span class="math">\(\epsilon \gt 0\)</span>.
</li>
</ul></li>
<li><p>
<strong>Example</strong>: Roll a fair die 100 times and let <span class="math">\(Z\)</span> be the sum and <span class="math">\(X_1\)</span>, ..., <span class="math">\(X_{100}\)</span> be the outcomes.
</p>

<ul>
<li>
What's the probability that <span class="math">\(Z\)</span> are within 50 of its mean?
</li>
</ul></li>
</ul>

<h2>
April 2nd, 2013 - Office Hours
</h2>

<h3>
Number 3
</h3>

<p>
<span class="math">\[Cov(X,Y) = E(XY) - E(X)E(Y)\]</span>
</p>

<ul>
<li>
Call <span class="math">\(z_1\)</span> the first roll, <span class="math">\(z_2\)</span> the second.
</li>
</ul>

<p>
<span class="math">\[E(XY) = \sum_{w \in S} X(w) Y(w) P(w)\]</span>
</p>

<ul>
<li>
We can do this in principle, it takes fifteen minutes.
</li>
<li>
More elegantly, however, take two random variables that represents the rolls, and it's nice because they're indepedant.
</li>
</ul>

<p>
<span class="math">\[X = z_1 + z_2\]</span> <span class="math">\[Y = z_1 - z_2\]</span>
</p>

<p>
<span class="math">\[XY = (z_1 + z_2)(z_1 - z_2)\]</span> <span class="math">\[ = z_1^2 - z_2^2 \]</span> <span class="math">\[E(XY) = E(z_1^2) - E(z_2^2) \]</span> <span class="math">\[ = 0 \]</span>
</p>

<h3>
Banach-Torski Paradox
</h3>

<ul>
<li>
There are lots of functions which are not integrable.
</li>
<li>
Take the function <span class="math">\(f(x)\)</span>, which is 0 if x is rational, 1 otherwise.
</li>
<li>
<span class="math">\([0, 1] \to \mathbb{R}\)</span> <span class="math">\[ \int_{0}^{1} f(x)dx\]</span>
</li>
<li>
Ball in 3-space, cut it up into peices and put them in sets, move the set around.
</li>
<li>
And you can double the volume! (???)
</li>
<li>
Axiom of choice.
</li>
</ul>

<blockquote>
  <p>
Physics is looking around you and drawing conclusions, but something really meaningful to me is that what you have infallible premises, you have an infallible conclusion. <cite>David Cash</cite>
</p>
</blockquote>

<h3>
Zero-knowledge proof
</h3>

<ul>
<li><p>
Zero knowledge proofs are a different way of thinking of proofs.
</p>

<ul>
<li>
A regular proof involves a prover and verifier.
</li>
<li>
If each step is right, the then conclusion is right, and you've proven something.
</li>
</ul></li>
<li><p>
If the statement &quot;<span class="math">\(x\)</span> is not prime&quot; (where <span class="math">\(x\)</span> is a hundred digit number).
</p></li>
<li>
You want to prove to a server that I know my password, you know I know it, but you don't know my password.
</li>
</ul>

<h4>
Journal of Craptology: Zero-Knowledge Proofs for Kids
</h4>

<ul>
<li>
There's a Waldo, you don't know where he is, but there is one.
</li>
<li>
Take a big piece of cardboard, and cut a Waldo sized hole, and show the person Waldo.
</li>
<li>
They know there's a Waldo, but have no knowledge about where the Waldo is.
</li>
</ul>

<h2>
April 3rd, 2013 - Lecture
</h2>

<h3>
Public polling
</h3>

<ul>
<li><p>
Say we want to estimate how much of the country will vote independent in the next presidential election.
</p>

<ul>
<li>
Call <span class="math">\(n\)</span> random people and ask what they'll do, with a country of <span class="math">\(m\)</span> people.
</li>
<li>
Average response to estimate fraction of country that will vote independent.
</li>
<li>
How big should <span class="math">\(n\)</span> be to be wintin <span class="math">\(\pm 3 \%\)</span> with probability <span class="math">\(\ge 95\%\)</span>
</li>
<li><p>
Let <span class="math">\(x_i = \)</span> &quot;ith person we call say will vote indepedant.&quot; <span class="math">\[ A_n = \frac{x_1 + ... + x_n}{n} \]</span>
</p></li>
<li><p>
Say a <span class="math">\(p\)</span> fraction of the country will vote independant (unknown).
</p></li>
<li><p>
Want to know: <span class="math">\[ P(|A_n - p| \gt 0.03) \le 0.05 \]</span>
</p></li>
<li><p>
Use Cheb. <span class="math">\[ P(|A_n - p| \gt 0.03) \le \frac{V(A_n)}{(0.03)^2} \le \frac{1}{n \times 0.03^2}\]</span>
</p>

<p>
<span class="math">\[ V(A_n) = V(\frac{x_1 + ... x_n}{n}) = \frac{1}{n} V(X_1) \]</span>
</p>

<p>
<span class="math">\[ = \frac{p(1 - p)}{n} \]</span>
</p>

<p>
<span class="math">\[ \le \frac{1}{n} \]</span>
</p></li>
<li><p>
Want <span class="math">\(\frac{1}{n \times 0.03^2} \le 0.05 \)</span>
</p></li>
</ul></li>
</ul>

<h3>
Back to counting! Trees
</h3>

<ul>
<li>
<p>Counting number of binary trees with <span class="math">\(n\)</span> nodes</p>
<ol>
<li>
<span class="math">\(n = 0\)</span>, <span class="math">\(a_0 = 1\)</span>
</li>
<li>
<span class="math">\(n = 1\)</span>, <span class="math">\(a_1 = 1\)</span>
</li>
<li>
<span class="math">\(n = 2\)</span>, <span class="math">\(a_2 = 2\)</span>
</li>
<li>
<span class="math">\(n = 3\)</span>, <span class="math">\(a_3 = 5\)</span>
</li>
<li>
<span class="math">\(n = 4\)</span>, <span class="math">\(a_4 = 14\)</span> <span class="math">\[a_n = \frac{1}{n + 1} {2n \choose n} \]</span>
</li>
</ol></li>
</ul>

<h3>
Generating functions
</h3>

<ul>
<li>
Want to find formula for <span class="math">\(a_0, a_1, a_2, ...\)</span>.
</li>
<li>
Write <span class="math">\(\lbrace a_i \rbrace_{i = 0}^\infty\)</span> or <span class="math">\(\lbrace a_i \rbrace\)</span> for the sequence.
</li>
<li><p>
<strong>Definition</strong>: The <em>generating function for</em> <span class="math">\(\lbrace a_i \rbrace\)</span> is the function <span class="math">\(A(x)\)</span> defined by <span class="math">\[ A(x) = \sum_{k = 0}^\infty a_k x^k \]</span>
</p></li>
<li><p>
(Treat as &quot;formula sum&quot;, meaning ignore convergence)
</p></li>
<li>
Problem sovling recipe for generating functions
</li>
<li>
<p>Want to find <span class="math">\(a_0, a_1, a_2, ... , a_n\)</span></p>
<ol>
<li>
Show that $A(x) has some simple &quot;closed form&quot;
</li>
</ol></li>
</ul>

<h2>
April 8th, 2013 - Lecture
</h2>

<h3>
Announcements
</h3>

<ul>
<li>
Ungraded HW on Sakai
</li>
<li>
New noes on generating functions
</li>
<li>
From <em>Applied Combinatorics</em> by Alan Tucker
</li>
<li>
HW07 online tonight, due next Monday
</li>
<li>
<p>Office hours this weeks:</p>
<ul>
<li>
3:00PM to 4:00PM on Thursday
</li>
<li>
2:00PM to 3:00PM on Friday
</li>
</ul></li>
</ul>

<h3>
Generating functions revisited
</h3>

<ul>
<li><p>
<strong>Question</strong>: How many solutions are there to <span class="math">\(z_1 + z_2 = 11\)</span>, with <span class="math">\(z_1, z_2\)</span> as non-negative integers.
</p>

<ul>
<li><p>
Some mediocre ways:
</p>

<ul>
<li>
You can list these by hand ...
</li>
<li>
There are 12, you can list them but I won't.
</li>
<li>
It can also be a stars and bars problem, 11 stars, 2 groups, empty allowed.
</li>
</ul></li>
<li><p>
But consider multipling out: <span class="math">\[(1 + x + x^2 + ... + x^{11})(1 + x + x^2 + ... + x^{11})\]</span>
</p></li>
<li>
This is going to be: <span class="math">\[\sum_{x = 0}^{22} a_k x^k\]</span>
</li>
<li>
Take a look at this pattern: <span class="math">\[a_0 = 1 | ()\]</span> <span class="math">\[a_1 = 2 | (x^0 x^1 + x^1 x^0 = 2x)\]</span> <span class="math">\[a_2 = 3 | (x^0 x^2 + x^1 x^1 + x^2 x^0 = 3x^2)\]</span>
</li>
<li>
Now consider that <span class="math">\(a_11 = 12\)</span>. This is not a coefficient.
</li>
<li>
Somehow counting the number of polynomials counted this other number we wanted.
</li>
<li>
Generating functions are about doing this counting with shortcuts.
</li>
</ul></li>
<li><p>
Considering multiplying out <span class="math">\((1 + x + x^2)^4 = \)</span> <span class="math">\[(1 + x + x^2)(1 + x + x^2)(1 + x + x^2)(1 + x + x^2)\]</span>
</p>

<ul>
<li>
<p>How to &quot;expand&quot;?</p>
<ul>
<li>
Pick <span class="math">\(x^0, x^1, x^2\)</span> from each group, multiplying choices.
</li>
<li>
Repeat for all possible choices (<span class="math">\(3^4\)</span> of them).
</li>
<li>
Add up results, collect like terms.
</li>
<li>
Each term is of the form: <span class="math">\[x^{e_1}x^{e_2}x^{e_3}x^{e_4}\]</span>
</li>
<li>
Each <span class="math">\(e_i = 0, 1, 2\)</span>
</li>
<li>
Expanded version collects for all possible allowed.
</li>
</ul></li>
</ul></li>
<li><p>
What is the coefficient of <span class="math">\(x^5\)</span> in <span class="math">\((1 + x + x^2)^4\)</span>.
</p>

<ul>
<li>
This equals the number of ways <span class="math">\(x^5\)</span> &quot;shows up&quot; in terms.
</li>
<li>
Coefficient is number of ways to wrte <span class="math">\(5 = e_1 + e_2 + e_3 + e_4, e_i = 0, 1\)</span>
</li>
</ul></li>
<li><p>
<strong>Examples</strong>: Find polynomial where <span class="math">\(a_k =\)</span> is equal to the number of ways to pick <span class="math">\(k\)</span> balls from a pile of 3 green, 2 white, 3 blue, and 3 gold.
</p>

<ul>
<li>
<p>Stated as &quot;equ problem&quot;, this means that <span class="math">\(a_k = \)</span> numer of solutions to <span class="math">\[e_1 + e_2 + e_3 + e_4 = k, e_i \in \lbrace 0 2 3 4 \rbrace\]</span></p>
<ul>
<li>
With grreen, white, blue, and gold (respectively)
</li>
</ul></li>
</ul></li>
<li><p>
<strong>Example</strong>: Same problem, but 7 green, 2 blue, 5 gold.
</p>

<ul>
<li>
The polynomial is <span class="math">\[(x^0 + x^1 + ... + x^7)(x^0 + x^1)(x^0 + x^1 + ... + x^5)\]</span>
</li>
</ul></li>
<li><p>
<strong>Example</strong>: Find the polynomial with <span class="math">\(a_k = \)</span> &quot;number of ways to pick 6 objects where each one is one of 3 types, but you can't have four of any one type.&quot;
</p></li>
</ul>

<h2>
April 15th, 2013 - Homework 7
</h2>

<ol>
<li><p>
(1.5 points each) Find the generating function for the sequence <span class="math">\(a_0, a_1, a_2, \ldots\)</span>, where <span class="math">\(a_k\)</span> is each of the following. Your solution does not need to be closed form.
</p>

<ol>
<li><p>
<span class="math">\(a_k = \)</span> the number of solutions to <span class="math">\(e_1 + e_2 + e_3 = k,\)</span> where <span class="math">\(0 \leq e_i \leq 4\)</span> for each <span class="math">\(i\)</span>.
</p>

<p>
<span class="math">\[(1 + x^1 + x^2 + x^3 + x^4)^3\]</span>
</p></li>
<li><p>
<span class="math">\(a_k = \)</span> the number of solutions to <span class="math">\(e_1 + e_2 + e_3 + e_4 = k,\)</span> where <span class="math">\(0 \leq e_i &lt; 4\)</span> for each <span class="math">\(i\)</span>, <span class="math">\(e_1\)</span> is odd, and <span class="math">\(e_2\)</span> is even.
</p>

<p>
<span class="math">\[(x^1 + x^3)(x^0 + x^2)(1 + x^1 + x^2 + x^3 + x^4)^2\]</span>
</p></li>
<li><p>
<span class="math">\(a_k = \)</span> the number of solutions to <span class="math">\(e_1 + e_2 + e_3 + e_4 = k,\)</span> where <span class="math">\(0 \leq e_i\)</span> for each <span class="math">\(i\)</span>.
</p>

<p>
<span class="math">\[\left(\sum_{i = 0}^{\infty} x^i \right)^4\]</span>
</p></li>
<li><p>
<span class="math">\(a_k = \)</span> the number of solutions to <span class="math">\(e_1 + e_2 + e_3 + e_4 + e_5= k,\)</span> where <span class="math">\(0 \leq e_i\)</span> for each <span class="math">\(i\)</span>, <span class="math">\(e_1\)</span> and <span class="math">\(e_3\)</span> are odd, and <span class="math">\(e_2\)</span> is even.
</p>

<p>
<span class="math">\[\left(\sum_{i = 0}^{\infty} x^{2i + 1} \right)^2 \times \left(\sum_{i = 0}^{\infty} x^{2i} \right) \times \left(\sum_{i = 0}^{\infty} x^{i} \right)^2\]</span>
</p></li>
</ol></li>
<li><p>
(3 points each) Model the following problems using a generation function, which does not need to be in closed form:
</p>

<ol>
<li><p>
Count the number of outcomes of rolling <span class="math">\(6\)</span> dice that sum to <span class="math">\(r\)</span>.
</p>

<p>
<span class="math">\[(x_1 + x_2 + x_3 + x_4 + x_5 + x_6)^6, &amp;#58; 1 \le x_i \le 6\]</span>
</p></li>
<li><p>
Count the number of outcomes of rolling <span class="math">\(6\)</span> dice that sum to <span class="math">\(r\)</span>, where the first three dice are odd and the last three are even.
</p>

<p>
<span class="math">\[(x_1 + x_3 + x_5)^3 \times (x_2 + x_4 + x_6)^3, &amp;#58; 1 \le x_{2i + 1} \le 3, 0 \le x_{2i} \le 2\]</span>
</p></li>
<li><p>
Count the number of outcomes of rolling <span class="math">\(6\)</span> dice that sum to <span class="math">\(r\)</span>, where for each <span class="math">\(i\)</span> the <span class="math">\(i\)</span>-th dice is not equal to <span class="math">\(i\)</span> (so the first die is not <span class="math">\(1\)</span>, the second is not <span class="math">\(2\)</span>, and so on).
</p>

<p>
<span class="math">\[= (x^2 + x^3 + x^4 + x^5 + x^6) \times (x^1 + x^3 + x^4 + x^5 + x^6) \times\]</span> <span class="math">\[(x^1 + x^2 + x^4 + x^5 + x^6) \times (x^1 + x^2 + x^3 + x^5 + x^6) \times\]</span> <span class="math">\[(x^1 + x^2 + x^3 + x^4 + x^6) \times (x^1 + x^2 + x^3 + x^4 + x^5)\]</span>
</p></li>
</ol></li>
<li><p>
(1.5 points each) Find the following coefficients. Show your work.
</p>

<ol>
<li><p>
The coefficient of <span class="math">\(x^{10}\)</span> in the series expansion of <span class="math">\((x^5+x^6 + x^7+ \cdots)^8\)</span>.
</p>

<p>
<span class="math">\[\mathrm{DNE}\]</span>
</p></li>
<li><p>
The coefficient of <span class="math">\(x^{20}\)</span> in the series expansion of <span class="math">\((x+x^2 + x^3+x^4+x^5) (x+x^2 + x^3+x^4+\cdots)^5 \)</span>.
</p>

<p>
<span class="math">\[18 \choose 14\]</span>
</p></li>
<li><p>
The coefficient of <span class="math">\(x^{12}\)</span> in the series expansion of <span class="math">\(x^2/(1+x)^8\)</span>.
</p>

<p>
<span class="math">\[8 + 10 - 1 \choose 10\]</span>
</p></li>
<li><p>
The coefficient of <span class="math">\(x^{12}\)</span> in the series expansion of <span class="math">\(1/(1+x^3)^2\)</span>.
</p></li>
</ol></li>
</ol>

<h2>
April 15th, 2013 - Lecture
</h2>

<ul>
<li><p>
<strong>Idea</strong>: To solve <span class="math">\(a_1, a_2, ... \)</span>
</p>

<ul>
<li><p>
Part 1: Define generating function <span class="math">\[A(x) = \sum_{k = 0}^\infty\]</span>
</p>

<ul>
<li>
&quot;Pull out&quot; the initial conditions
</li>
<li>
Substitute recurance relation formula
</li>
<li>
Expand, algebra, etc until you can substitute <span class="math">\(A(x)\)</span>
</li>
<li>
Get <span class="math">\(A(x) =\)</span> &quot;some formula of A(x)&quot;
</li>
<li>
Solve for A(x)
</li>
</ul></li>
<li><p>
Part 2: Find the series expansion
</p>

<ul>
<li>
Easiest case, like above, <span class="math">\(A(x)\)</span> is in the table.
</li>
<li>
Work harder, use partial fraction decomposition
</li>
</ul></li>
</ul></li>
</ul>

<h2>
April 17th, 2013 - Notes pt. 5
</h2>

<h3>
Generating functions
</h3>

<ul>
<li><p>
Let <span class="math">\(a_0, a_1, ...\)</span> (or more briefly <span class="math">\(\lbrace a_i \rbrace\)</span>) denote an infinite sequence of real numbers. Its <strong>generating function</strong> is defined by
</p>

<p>
<span class="math">\[ A(s) = \sum_{k = 0}^{\infty} a_k s^k = a_0 + a_1 s + ... + a_k s^k + ... \]</span>
</p>

<ul>
<li>
We are not claiming this series converges.
</li>
<li>
In some sense, you can view this as a formalism for infinite series.
</li>
<li>
When the series does converge with a function with algebraic properties, we will make use of this correspondance.
</li>
</ul></li>
<li><p>
Facts:
</p>

<ol>
<li><p>
$ A(0) = a_0$
</p>

<ul>
<li>
The first element in the sequence.
</li>
</ul></li>
<li><p>
$ A(1) = <em>{k = 0}^{} a</em>k$
</p>

<ul>
<li>
The sum of the elements.
</li>
</ul></li>
<li><p>
$ A`(1) = <em>{k = 1}{} k a</em>k s^{k - 1} |_{s = 1}$
</p>

<ul>
<li>
Differentiate each term of the sum in (1) and substitute.
</li>
</ul></li>
<li><p>
$ A + B = <em>{k = 0}^(a</em>k + b_k) s^k$
</p>

<ul>
<li>
Sum
</li>
</ul></li>
<li><p>
$ A(s)B(s) = a_0 b_0 + (a_0 b_1 + a_1 b_0) s + ... $ $ + (a_0 b_k + ... + a_k b_0) s^k $
</p>

<ul>
<li>
Multiplication, convolutions
</li>
</ul></li>
</ol></li>
</ul>

<h3>
Generating Functions and Recurrence Relations
</h3>

<ul>
<li><p>
Given a recurrence relation for the sequence <span class="math">\(\lbrace a_i \rbrace\)</span>,
</p>

<ol>
<li><p>
Deduce from it an equation satisfied by the generation function
</p>

<p>
<span class="math">\[ a(x) = \sum_{i} a_i x^i \]</span>
</p></li>
<li><p>
Solve this equation to get an explicit expression for the generating function.
</p></li>
<li>
Extract the coeeficient $ a_n $ of $ x^n $ from $ a(x) $ by expanding <span class="math">\(a(x)\)</span> as a power series.
</li>
</ol></li>
<li><p>
Alternate steps <a href="http://www.wikihow.com/Solve-Recurrence-Relations">wikiHow</a>
</p>

<ol>
<li><p>
Consider the sequence 2, 5, 14, 41, 122 ... given by this formula:
</p>

<p>
<span class="math">\[ a_0 = 2 \]</span> <span class="math">\[ a_n = 3 a_{n - 1} - 1 \]</span>
</p></li>
<li><p>
Write the genrating function of the sequence. A generating function is simply a formula power series where the coeeficient of $ x^n $ is the nth term of the sequence.
</p>

<p>
<span class="math">\[ A(x) = \sum_{k = 0}^{\infty} a_k x^k \]</span>
</p></li>
<li><p>
Manipulate the generating function. The objective in this step is to find an equation that will allow us to solve for the generating function $ A(x) $. User the formula for the sum a geomtric series.
</p>

<ul>
<li>
Original formula:
</li>
</ul>

<p>
<span class="math">\[ A(x) = \sum_{k = 0}^{\infty} a_k x^k \]</span>
</p>

<ul>
<li>
&quot;Take out&quot; when a is 0:
</li>
</ul>

<p>
<span class="math">\[ A(x) = 2 + \sum_{k = 1}^{\infty} a_k x^k \]</span>
</p>

<ul>
<li>
Make the first term addition by referring to the original definition of this series.
</li>
</ul>

<p>
<span class="math">\[ A(x) = 2 + \sum_{k = 1}^{\infty} (3 a_{k - 1} - 1) x^k \]</span>
</p>

<ul>
<li>
Split the sum
</li>
</ul>

<p>
<span class="math">\[ A(x) = 2 + \sum_{k = 1}^{\infty} 3 a_{k - 1} x^k -  \sum_{k = 1}^{\infty} x^k \]</span>
</p>

<ul>
<li>
Now recognize that if you take out the 3 as well as <em>an</em> <span class="math">\(x\)</span>, you have the original formula.
</li>
</ul>

<p>
<span class="math">\[ = 2 + 3x A(x) - \frac{x}{1 - x} \]</span>
</p></li>
<li><p>
Solve for <span class="math">\(A(x)\)</span>:
</p>

<p>
<span class="math">\[ A(x) = \frac{ 3x - 2 }{ (3x - 1)(1 - x)} \]</span>
</p></li>
<li><p>
Find the coefficient of <span class="math">\(x^n\)</span> using partial fractions or some other method.
</p></li>
<li><p>
Write the formual for <span class="math">\(a_n\)</span> by identifying the coefficient of <span class="math">\(x^n\)</span> in <span class="math">\(A(x)\)</span>.
</p>

<p>
<span class="math">\[ a_n = \frac{3^{n + 1} + 1}{2} \]</span>
</p></li>
</ol></li>
</ul>

<h2>
April 17th, 2013 - Lecture
</h2>

<ul>
<li>
<p>Counting number of binary trees on n nodes</p>
<ul>
<li>
Let <span class="math">\(b_n\)</span> be the number of binary trees given <span class="math">\(n\)</span> nodes.
</li>
<li>
$ b_0 = 1<span class="math">\(, \)</span>b_1 = 1<span class="math">\(, \)</span>b_2 = 2$, etc
</li>
<li>
What does an n node ree &quot;look like&quot;?
</li>
</ul></li>
</ul>

<h2>
Practice Final Exam
</h2>

<ol>
<li>
<p>What is the coefficient of <span class="math">\(x^{12} x^{9}\)</span> when <span class="math">\((x + y)^{21}\)</span> is expanded? How many monomials are in the expansion?</p>
<ul>
<li>
This requires the binomial theorem.
</li>
<li>
You plug in <span class="math">\(21\)</span> for <span class="math">\(n\)</span> and <span class="math">\(9\)</span> for <span class="math">\(k\)</span>. <span class="math">\[22 \choose 12\]</span>
</li>
</ul></li>
<li>
<p>If <span class="math">\(P(A \cap B) \lt P(A)\)</span>, is it always true that <span class="math">\(P(A|B) \lt P(A)\)</span>? Either prove it, or disprove it by finding a counterexample.</p>
<ul>
<li>
<p>You can disprove this with the values <span class="math">\(P(A) = .5\)</span>, P(B) = .25<span class="math">\(.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;If we throw \)</span>n$ balls into <span class="math">\(m\)</span> urns, what is the probability that all of the balls land in exactly 1 urn? At most 2 urns?</p>
<ul>
<li>
The number of possible ways to distribute <span class="math">\(n\)</span> balls into <span class="math">\(m\)</span> possible urns is, according to the partition formula, <span class="math">\[n + m - 1 \choose m - 1\]</span>
</li>
<li>
Let this be our sample space.
</li>
<li>
For exactly one urn, how many ways are there to distribute all the balls into one urn? <span class="math">\[m\]</span>
</li>
<li>
Making our answer <span class="math">\(m\)</span> divided by the sample space.
</li>
<li>
How many ways are there to distribute <span class="math">\(n\)</span> balls into at most <span class="math">\(2\)</span> urns? This means that <span class="math">\(1\)</span> urn is possible, but we're interested in <span class="math">\(2\)</span> urns <em>as well</em>.
</li>
<li>
The number of ways to divide <span class="math">\(n\)</span> indistinguishable balls into <span class="math">\(r\)</span> possibly empty bins is: <span class="math">\[n + r - 1 \choose r - 1\]</span>
</li>
<li>
Our answer, therefore is, <span class="math">\[\frac{m + {n + 2 - 1 \choose 2 - 1}}{n + m - 1 \choose m - 1}\]</span>
</li>
</ul></li>
<li><p>
Suppose we shuffle a standard deck of 52 cards and deal a 5 card hand. What is the probability we get a straight? What about a straight flush?
</p>

<ul>
<li>
Our sample space is the beyond astronmically large: <span class="math">\[56 \choose 5\]</span>
</li>
<li><p>
Let's look at how many straights there are:
</p>

<pre><code>A  2  3  4  5  6  7  8  9  10  J  Q  K  A
+-----------+  +-----------+
   +-----------+  +------------+
      +-----------+  +------------+
         +-----------+  +------------+
                +----------+    +-----------+
</code></pre></li>
<li>
I count 10 different ways of getting a straight.
</li>
<li>
So compared to our sample space, you must select 1 of 10 straights, then for each card, you must pick a suit. <span class="math">\[\frac{{10 \choose 1}{4 \choose 1}^5}{56 \choose 5}\]</span>
</li>
<li>
If we're only interested in the case where all of the suits are the same, you can simply take of the <span class="math">\(5\)</span>th power on the suit value.
</li>
</ul></li>
<li><p>
We roll a die 10 times. Let <span class="math">\(X\)</span> be the number of time a roll of 1, 2, or 3 comes up. Find the range of <span class="math">\(X\)</span>, <span class="math">\(P(X = 3)\)</span>, <span class="math">\(E(X)\)</span>, and <span class="math">\(V(X)\)</span>.
</p>

<ul>
<li>
So we're interested in the frequnecy function where we get &quot;the number of trials until the first success&quot; which is &quot;geometric.&quot;
</li>
<li>
Our range, therefore, is from <span class="math">\(0\)</span> to potentially inifinite.
</li>
<li>
The probability of success for any given trial, because of linearity of expectation, is: <span class="math">\[\frac{3}{6}\]</span> because there are <span class="math">\(3\)</span> values that represent a sucess and <span class="math">\(6\)</span> total possible values. <span class="math">\[P(X = n) = \frac{1}{2}(1 - \frac{1}{2})^{n - 1}\]</span> <span class="math">\[E(X) = \frac{1}{\frac{1}{2}}\]</span> <span class="math">\[V(X) = \frac{1 - \frac{1}{2}}{(\frac{1}{2})^2}\]</span>
</li>
</ul></li>
<li><p>
Suppose again we shuffle a standard deck of 52 cards and deal a five card hand. Let <span class="math">\(A_i\)</span> be the event that the <span class="math">\(i\)</span>-th card is red.
</p>

<ol>
<li><p>
What is the probability of the event <span class="math">\(E = \)</span> &quot;exactly two cards are red&quot;?
</p>

<ul>
<li>
You can exploit symettry and linearity of expectation here.
</li>
<li>
There are <span class="math">\(56\)</span> cards in the first drawing, <span class="math">\(55\)</span> cards in the second drawing.
</li>
<li>
There are <span class="math">\(26\)</span> red cards in the first drawing, <span class="math">\(25\)</span> red cards in the second (because we already &quot;got one&quot; in this calculation).
</li>
<li>
Now the rest of the cards have to be not red out of <span class="math">\(26\)</span>, <span class="math">\(25\)</span>, <span class="math">\(24\)</span> non-red cards in a deck of <span class="math">\(54\)</span>, <span class="math">\(53\)</span>, <span class="math">\(52\)</span>.
</li>
<li>
Every fraction must be multiplied by the multiplicative principle. <span class="math">\[{5 \choose 2}\left(\frac{26}{56} \times \frac{25}{55} \times \frac{26}{54} \times \frac{25}{53} \times \frac{24}{52}\right)\]</span>
</li>
<li>
Please note that there are in fact <span class="math">\(52\)</span> cards in a deck.
</li>
</ul></li>
<li><p>
What is the probability of the event <span class="math">\(F = \)</span> &quot;the first two cards are red&quot;?
</p>

<ul>
<li>
This is like the first question, but we just don't care about the cards after the first two. They could be red, they might not be, whatever the case, we do not have to account for it. <span class="math">\[\frac{26}{56} \times \frac{25}{55}\]</span>
</li>
</ul></li>
<li>
<p>Are <span class="math">\(E\)</span> and <span class="math">\(F\)</span> indepedant? Explain your answer.</p>
<ul>
<li>
Two events are indepedant if and only if the probability of their intersection is equal to the product of their individual probabilities. Formally, <span class="math">\[P(A \cap B) = P(A) \times P(B)\]</span>
</li>
<li>
What is <span class="math">\(A \cap B\)</span>? It is both that &quot;the first two cards are red&quot; and &quot;exactly two cards are red.&quot; Which is the event that only the first two cards are red.
</li>
<li>
The probability that both of these events happen is not equal to the product of the individual probilities, and therefore this particular tuple of events are dependent on one another.
</li>
</ul></li>
</ol></li>
<li><p>
Consider an experiment where we flip three coins. Suppose we repeat this experiment until we get all Heads. Let <span class="math">\(X\)</span> be the random variable that is the number of experiments needed. Find <span class="math">\(E(X)\)</span> and <span class="math">\(V(X)\)</span>.
</p>

<ul>
<li>
Again, we are dealing with a geometric frequency function because we're interested in the number of trials until the first success.
</li>
<li>
The success of any given trial is going to be the cube of the success of a single coin toss, <span class="math">\[\left(\frac{1}{2}\right)^3\]</span>
</li>
<li>
Therefore, based on the formula for the geometric frequency function, <span class="math">\[P(X = n) = \left(\frac{1}{2}\right)^3 \left(1 - \left(\frac{1}{2}\right)^3\right)^{n - 1}\]</span> <span class="math">\[E(X) = \frac{1}{\left(\frac{1}{2}\right)^3}\]</span> <span class="math">\[V(X) = \frac{1 - \left(\frac{1}{2}\right)^3}{\left(\left(\frac{1}{2}\right)\right)^3)^2}\]</span>
</li>
</ul></li>
<li><p>
Consider the same experiment as before, except now we flip the coins 100 times. Let <span class="math">\(W\)</span> be the random variable representing the number of time you get all Heads. Is the frequency function of <span class="math">\(W\)</span> Bernoulli, binomial, geometric, or negative binomial? Find <span class="math">\(E(W)\)</span> and <span class="math">\(V(W)\)</span>. Use Chebyshev's inequality to bound the probability that $|W - E(W)| is more than 10.
</p>

<ul>
<li>
This is a binomial frequency function, as it represents the number of successes in <span class="math">\(n\)</span> trials.
</li>
<li>
The range is limited by <span class="math">\(n\)</span>.
</li>
<li>
<span class="math">\(n\)</span> is equal to <span class="math">\(100\)</span>, as we are performing the trial that many times.
</li>
<li>
The probability of success in any given trial is still: <span class="math">\[ \left(\frac{1}{2}\right)^3\]</span> <span class="math">\[P(X = k) = {100 \choose k} \left(\frac{1}{2}\right)^3 \left( 1 - \left( \frac{1}{2} \right)^3\right)^k\]</span> <span class="math">\[E(X) = \left(\frac{1}{2}\right)^3 \times 100 \]</span> <span class="math">\[V(X) =  \left(\frac{1}{2}\right)^3 \left(1 - \left(\frac{1}{2}\right)^3\right)\times 100\]</span>
</li>
</ul></li>
<li>
Consider a group of <span class="math">\(n\)</span> married couples which are seated at a rectangular table with <span class="math">\(n\)</span> seats on each side. Let <span class="math">\(X\)</span> be a random variable that counts the number of married couples that are seated next to each other. Find <span class="math">\(E(X)\)</span>.
</li>
<li>
<p>Suppose that 51 percent of babies are born girls. Suppose also that there is a prenatal test such that 98 percent of the baby girls come back positive. Use Bayes Theorem to compute the probability that the baby is a girl.</p>
<ul>
<li>
Bayes's theorem: <span class="math">\[ P(F|E) = \frac{P(E|F)P(F)}{P(E|F)P(F) + P(E|F^c)P(F^c)} \]</span>
</li>
<li>
<span class="math">\(E = \)</span> &quot;the test for girl is positive&quot;
</li>
<li>
<span class="math">\(F = \)</span> &quot;the baby is a girl&quot; = <span class="math">\(.51\)</span>
</li>
<li>
<span class="math">\(P(E|F) = \)</span> &quot;the prenatal test is right&quot; $ = .98$ <span class="math">\[ \frac{.98 \times .51}{.98 \times .51 + .02 \times .49} \]</span>
</li>
</ul></li>
<li><p>
Give the generating function with the <span class="math">\(n\)</span>-coefficient equal to the number of ways to solve <span class="math">\(e_1 + e_2 + e_3 = n\)</span> with <span class="math">\(e_1, e_2 \ge 0\)</span> and <span class="math">\(e_3\)</span> a multiple of <span class="math">\(3\)</span>. Give a close form version of your function. <span class="math">\[e_1 = x^0 + x^1 + x^2 + x^3 ...\]</span> <span class="math">\[e_2 = x^0 + x^1 + x^2 ... \]</span> <span class="math">\[e_3 = x^0 + x^3 + x^6 + x^9 ... \]</span>
</p>

<ul>
<li>
Referring the &quot;the chart&quot;, not that <span class="math">\(e_1\)</span> and <span class="math">\(e_2\)</span> are of the form: <span class="math">\[ \frac{1}{x - 1} \]</span>
</li>
<li>
And similarly, <span class="math">\(e_3\)</span> is of the form: <span class="math">\[ \frac{1}{x^3 - 1} \]</span>
</li>
<li>
Yielding the closed form: <span class="math">\[ \left( \frac{1}{1 - x} \right)^2 \left( \frac{1}{1-x^3} \right)\]</span>
</li>
</ul></li>
<li><p>
Find a formula for <span class="math">\(a_n\)</span>, which is defined by the following recurrence relation for all <span class="math">\(n \gt 0\)</span>: <span class="math">\[a_0 = 9\]</span> <span class="math">\[a_n = 2a_{n - 1} + 2\]</span> <span class="math">\[2a_{n - 1} 2 + 9 = a_n + a_0 \]</span> <span class="math">\[\sum_{n = 0}^{\infty} a_n x^n \]</span> <span class="math">\[x^n (2a_{n - 1} \]</span>
</p></li>
<li><p>
Consider the coin flipping game, where player <span class="math">\(A\)</span> pays <span class="math">\(B\)</span> one dollar for each Heads, and vice versa for each Tails. (The coin is unbiased here.) Let <span class="math">\(X_1\)</span> be the random variable recordin the first time player <span class="math">\(A\)</span> is &quot;ahead.&quot; Find <span class="math">\(P(X_1 \le 7)\)</span>. What is the probability that <span class="math">\(X\)</span> is odd? Even?
</p></li>
<li>
Continuing with the coin flipping game, also define <span class="math">\(X_2\)</span> to record the first time <span class="math">\(A\)</span> is up two dollars, <span class="math">\(Z_1\)</span> be the first time any player is up one dollar, and <span class="math">\(Z_2\)</span> the first time any player is up two dollars. Which pairs of random variables from <span class="math">\(\lbrace X_1, X_2, Z_1, Z_2 \rbrace\)</span> are indepedant? Let <span class="math">\(W = Z_2 - Z_1\)</span>. Is <span class="math">\(W\)</span> indepedant of <span class="math">\(Z_1\)</span>? Explain your answers carefully, but explicit calculations are not necessary.
</li>
</ol> 
{% endraw %}