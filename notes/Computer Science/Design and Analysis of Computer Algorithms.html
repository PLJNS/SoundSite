---
layout: page
title: Notes
subtitle: From Paul Jones at Rutgers University
---
{% raw %}
<h1>
Design and Analysis of Computer Algorithms <small>with Professor Kostas Bekris</small>
</h1>

<h2>
Syllabus
</h2>

<h3>
Topics
</h3>

<p>
We will cover a large subset of the following and possibly some new algorithmic topics and applications, as time permits:
</p>

<ul>
<li>
Mathematical tools. Review of mathematical background, concepts of algorithm design, complexity, asymptotics, induction, and randomization. Fibonacci numbers. Euclidean gcd algorithms. Universal hashing.
</li>
<li>
Divide and conquer. Fast integer multiplication; recurrences; the master theorem; mergesort; randomized median and selection algorithms; quicksort; fast matrix multiplication.
</li>
<li>
Sorting. Lower bounds for comparison-based sorting; binsort and radix sort.
</li>
<li>
Dynamic programming; Paradigm of SPs in DAGs; longest increasing subsequence; approximate string matching; integer and (0,1) knapsack problems; chain matrix multiplication; single-pair reliable SPs, all-pairs SPs; independent sets.
</li>
<li>
Graph search. Graph classes and representations; depth first search in undirected and directed graphs; topological search; strongly connected components. Breadth first search and layered DAGs.
</li>
<li>
Shortest Paths (SPs) in digraphs. Single-source SPs for nonnegative edge weights; priority queues and Dijkstra; SPs in DAGs; single-source SPs for general edge weights. Maximum adjacency search.
</li>
<li>
Greedy algorithms. Spanning trees and cuts, analysis of union-find and path compression; MST algorithms; randomized algorithm for global minimum cuts; approximate set cover.
</li>
<li>
Network flows. Max flow min cut theorem and integrality; fast algorithms; disjoint (s,t)-dipaths; maximum bipartite matching &amp; minimum vertex cover. Global minimum cuts.
</li>
<li>
Elements of NP-completeness &amp; problem reductions.
</li>
<li>
NP-hard problems. Search and selected approximation algorithms.
</li>
</ul>

<h3>
Prerequisites
</h3>

<p>
Courses:
</p>

<ul>
<li>
CS 112 Data Structures
</li>
<li>
CS 206 Introduction to Discrete Structures II
</li>
</ul>

<p>
We assume (and briefly review early on in the class) elements of discrete mathematics, such as logarithms, proofs by induction, series and sums, permutations, asymptotics (big-Oh, big-Omega notation), basics of solving recurrences, as well as concepts of programming and data structures, e.g., linked lists, stacks, queues, trees, binary search, recursion, hashing, priority queues, graph algorithms, sorting.
</p>

<h3>
Reading Material
</h3>

<p>
The class will primarily draw upon material from the following book:
</p>

<ul>
<li>
&quot;Algorithms&quot; by Dasgupta, Papadimitriou &amp; Vazirani, McGraw Hill, 2008.
</li>
</ul>

<p>
The following book may also be used as reference:
</p>

<ul>
<li>
&quot;Introduction to Algorithms&quot; by Cormen, Leiserson, Rivest &amp; Stein, McGraw Hill (The chapters in the calendar below refer to the 2nd edition)
</li>
</ul>

<p>
The books are not required for the class. Students are expected to take notes during the presentation of the material in the classroom and the recitations. Homeworks and exams will be based on the presented material.
</p>

<h3>
Exams
</h3>

<p>
There will be three exams: two midterms and one final. The first midterm will cover the material of the first third of the course, and the second midterm will cover the second third of the course. The final exam will cover material from the entire class. Check the tentative schedule for updates. All exams will be in-class on a date arranged and announced ahead of time.
</p>

<p>
A missed exam draws zero credit. Emergencies will be considered upon submitting a University-issued written verification to the Instructor; for assistance contact your Dean's Office. Also, check the definition of <a href="http://sasundergrad.rutgers.edu/forms/final-exam-conflict">Final Exam</a> by SAS.
</p>

<h3>
Homework Assignments
</h3>

<p>
There will be 4 to 5 homeworks. You will be informed in advance when an assignment is due. A tentative scheduled is available on the course's website. The homeworks consist of practice questions which are intended to assist students in mastering the course content. They may also potentially involve limited programming effort.
</p>

<p>
Homeworks should be completed by teams of students - three at most. No additional credit will be given for students that complete a homework individually. Please inform Athanasios Krontiris about the members of your team (email: tdk.krontir/AT/gmail.com).
</p>

<p>
Students will receive 10% extra credit if they typeset their report using LaTeX or 5\% extra credit if they typewrite their answers (e.g., using Word). Submit only PDF documents. For instance, if a pair was to receive a score of 62/100 and they typeset their report, then their score will be 68/100, i.e,. they receive a bonus of +10\% of 62 points. Resources on how to use LaTeX are available below.
</p>

<h3>
Submission Rules
</h3>

<p>
No late submission is allowed. If you don't submit a homework on time, you get 0 points for that homework. The deadline will typically correspond to the beginning of a lecture. Students can submit their homeworks electronically via Sakai.
</p>

<h3>
Grading System
</h3>

<p>
The final grade will be computed according to the following rule **(this is tentative and can change)**:
</p>

<p>
final grade = max(Case A: With Homeworks, Case B: Without Homeworks)
</p>

<p>
Case A: With Homeworks
</p>

<ul>
<li>
Homeworks: 20 points total
</li>
<li>
First midterm: 25 points
</li>
<li>
Second midterm: 25 points
</li>
<li>
Final exam: 30 points
</li>
<li>
Participation: +/- 5 points (this is up to the discretion of the instructor and the TAs)
</li>
</ul>

<p>
Case B: Without Homeworks
</p>

<ul>
<li>
First midterm: 30 points
</li>
<li>
Second midterm: 30 points
</li>
<li>
Final exam: 40 points
</li>
<li>
Participation: +/- 5 points (this is up to the discretion of the instructor and the TAs)
</li>
</ul>

<p>
On any assignment (homework or exam), you can either attempt to answer the question, in which case you will receive between 0 and 100% credit for that question, or you can write &quot;I don't know&quot;, in which case you receive 25% credit for that question. Leaving the question blank is the same as writing &quot;I don't know.&quot; You can and will get less than 25% credit for a question that you answer erroneously.
</p>

<p>
Finally, the first exam is a make-or-break situation. If your score on the first exam is 26% or less (which amounts to a blank exam) then you fail the class. The first exam will be early enough for you to drop the class.
</p>

<p>
Your participation grade can be positive or negative. By default your participation grade is 0..., e.g., if you typically come to the lectures/recitations but you rarely answer questions during the lectures or the recitations, your participation grade will be 0. Positive participation grades will be given to students that actively participate in lectures and recitations. You can also receive a negative participation grade depending on the level of your involvement in the course lectures and recitations (or lack there of) or because of issues related to collusion or cheating in homeworks and exams.
</p>

<p>
The mapping of scores to letter grades will be determined at the end of the semester. As a <strong>rough</strong> guide, the following rule may be used for the final grade **(it will be adapted close to the end of the semester)**:
</p>

<ul>
<li>
A: &gt; 89
</li>
<li>
B+: 80-89
</li>
<li>
B: 70-79
</li>
<li>
C+: 60-69
</li>
<li>
C: 50-59
</li>
<li>
D: 40-49
</li>
<li>
F: less than 40
</li>
</ul>

<p>
Students interested in a recommendation letter by the instructor will be offered one only if they achieve a score above 95 after the completion of the course.
</p>

<h3>
Questions about Grading
</h3>

<p>
If you have a question or complaint regarding the points you received on specific parts of a HW assignment, or an exam, staple a sheet of paper on the graded item, stating specifically but very briefly what parts of that document you wish to have reviewed and forward it to Athanasios Krontiris, who will handle the process of communicating with the instructor and the other TAs. Please refrain from verbal arguments about grades with the instructor or with any of the TAs. We will try to get back to you within two weeks. The deadline for submitting such requests is the last lecture.
</p>

<h3>
Academic Standards
</h3>

<p>
Exams are to be treated as individual efforts. Homeworks are not to be treated as collective efforts beyond the participation of the team members! Discussions are not allowed on how to solve specific questions in homeworks. Do not discuss assignments with students that are not currently taking the class.
</p>

<p>
A severe penalty will be given to any assignment which indicates collusion or cheating. The usual penalty for cheating on an assignment or an exam is failure in the course. At a minimum your participation grade will be influenced negatively. Stealing another person's listing or having another person &quot;ghost write&quot; an assignment will be considered cheating.
</p>

<p>
Turning in work without properly citing the sources of the content included in your work is plagiarism. All kinds of sources are included in this definition, even those downloaded from the web, in which case an operable link must be cited. Plagiarism from the web or other sources is considered cheating and has the same effects. Even with a reference, submitting an answer to a homework question, verbatim from any source and without any contribution on your part, draws zero credit.
</p>

<p>
You should carefully study the website of Rutgers University on <a href="http://academicintegrity.rutgers.edu/">Academic Integrity</a> and the corresponding <a href="http://academicintegrity.rutgers.edu/policy-on-academic-integrity">policy</a>, as well as the corresponding <a href="http://www.cs.rutgers.edu/policies/academicintegrity/">policy</a> from the department of Computer Science. Your continued enrollment in this course implies that you have read these policies, and that you subscribe to the principles stated therein.
</p>

<h3>
LaTeX Resources
</h3>

<p>
General info on what you can do with LaTeX: <a href="http://www.maths.tcd.ie/%7Edwilkins/LaTeXPrimer/">Getting Started with</a> <a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/lshort.pdf">The Not So Short Introduction to</a> <a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/comprehensive.pdf">Comprehensive List of Latex</a> <a href="http://www.logicmatters.net/latex-for-logicians/">Latex for Logicians</a>
</p>

<p>
Mac <a href="http://ii2.sourceforge.net/tex-index.html">Tex on Mac OS X</a> <a href="http://www.tug.org/mactex/">MacTex</a>
</p>

<p>
The first link describes many alternatives that are available for installing Tex on a Mac. The second link forwards to the MacTex package, one of the alternatives mentioned in the first website. MaxTex provides everythink that you need to use Latex on Mac except from a text editor. It is, however, compatible with a wide variety of popular editors (e.g., Alpha, BBEdit, Emacs, VIM, iTeXMac, TeXShop). Note that MaxTex is a large package.
</p>

<p>
Carbon Emacs has been succesfully tested with MacTex. After installing MacTex, it is possible to directly compile and view *.tex files from Carbon Emacs's UI.
</p>

<p>
Note for Mac users: You will probably have problems previewing your PDF output when using the postscript images provided by the instructor for developing the notes. Nevertheless, the PDF file can be printed properly. Prepare your document without the images and then add them. You will probably still be able to preview the intermediate .dvi output file with the &quot;xdvi&quot; program.
</p>

<p>
Linux (Ubuntu) <a href="https://help.ubuntu.com/community/LaTeX">Latex on Ubuntu</a> <a href="http://www.tug.org/texlive/">Tex Live</a>
</p>

<p>
You just have to download and install the proper packages described above (e.g., through apt-get), use your favorite editor (e.g., emacs) to prepare a *.tex file and then you compile (run at least two times: &quot;latex filename.tex&quot;) to get the *.dvi output. You can go from dvi to postscript with the command &quot;dvips&quot; and you can convert postscript to pdf with the command &quot;ps2pdf&quot;.
</p>

<p>
Windows <a href="http://faculty.smu.edu/barr/latex/">Latex for Windows help</a> <a href="http://miktex.org/">MikTex (Latex for Windows)</a>
</p>

<p>
If you follow the instructions on the first link you should be able to get it working on a Windows system.
</p>

<p>
Below you can find Windows executables (32 bit) for the following programs (follow the order when installing):
</p>

<p>
<a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/setup-2.7.3092.exe">MiKTeX</a>
</p>

<p>
<a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/gs863w32.exe">Ghostscript</a>
</p>

<p>
<a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/gsv49w32.exe">Ghostview</a>
</p>

<p>
<a href="http://www.cse.unr.edu/robotics/bekris/cs482_f09/sites/cse.unr.edu.robotics.bekris.cs482_f09/files/winedt55.exe">WinEdt</a>
</p>

<h2>
January 24th, 2014 - Lecture
</h2>

<h3>
Fibonacci
</h3>

<ul>
<li><p>
He was an Italian mathematician in the 13th century who designed a famous sequence of numbers.
</p>

<p>
<span class="math">\[0, 1, 2, 3, 5, 8, 13, ...\]</span>
</p>

<ul>
<li><p>
We can design an algorithm to compute this.
</p>

<p>
<span class="math">\[F_n = F_{n - 1} + F_{n - 2}\]</span>
</p></li>
<li><p>
And this can be expressed as a psuedocode function
</p>

<pre><code>fib(n) {
    if (n == 0 | n == 1) {
        return n;
    } else {
        return fib(n - 1) + fib(n - 2)
    }
}
</code></pre></li>
<li><p>
<span class="math">\(T(n)\)</span> grows at least as much as the value of <span class="math">\(Fn\)</span>.
</p>

<p>
<span class="math">\[F_n \approx 2^{0.694n}\]</span>
</p>

<ul>
<li>
This grows exponetially as a function of n.
</li>
</ul></li>
</ul></li>
<li><p>
If today, you can compute the 100th number, then after a year, hardware will allow you to computer the 101th number in a sequence.
</p></li>
<li><p>
A new function for Fibonacci:
</p>

<pre><code>fib(n) {
    if (n &lt;= 1) {
        return n;
    }

    int F[n];
    F[0] = 0;
    F[1] = 1;

    for (int i = 2; ; n++) {
        F[i] = F[i - 1] + F[1 - 2];

        return F[n];
    }
}
</code></pre></li>
<li><p>
Eventually, the cost is not exponetial, as now our function is linear, it's in the order of n.
</p></li>
<li><p>
Now, it's quite feasible to get to 200,000th number in the sequence.
</p></li>
</ul>

<h3>
Computations
</h3>

<ul>
<li><p>
So far, all computations were treated as equal cost operations.
</p>

<ul>
<li>
This is a convinient simplication, but it is not true.
</li>
<li><p>
For instance, addition.
</p>

<ul>
<li><p>
For numbers that can fit within your computer's register, say, 32-bits or 64-bits, then it takes only steps.
</p></li>
<li><p>
But for big numbers like the Fibonacci numbers, like <span class="math">\(0.69 / n\)</span> bits.
</p></li>
</ul></li>
</ul></li>
<li><p>
Arithemetic operations on large numbers cannot be treated as a single step. You need to think about their representation.
</p>

<ul>
<li>
<p>We need to think in terms of the representation of numbers.</p>
<ul>
<li>
For example, a number <em>N</em> in base <em>b</em>.
</li>
<li>
You need <span class="math">\(\lceil log_b (N+1) \rceil\)</span> digits.
</li>
</ul></li>
</ul></li>
<li><p>
If cost of addition is linear to the size of bit representation, what is the cost of <code>fib(n)</code>?
</p></li>
</ul>

<h3>
Running Time Simplification
</h3>

<ul>
<li><p>
If you have two functions that represent two running times for algorithms,
</p>

<p>
<span class="math">\[f(n) = O(g(n))\]</span>
</p>

<p>
<span class="math">\[\exists c, n_0, f(n) \le c \times g(n) \forall n \ge n_0\]</span>
</p></li>
</ul>

<h2>
January 29th, 2013 <small>Lecture</small>
</h2>

<h3>
Multiplication
</h3>

<ul>
<li><p>
Example: 13 times 11
</p>

<pre><code>    1101
    1011
    ----
    1101
   1101
  0000
 1101
 10001111
</code></pre></li>
<li><p>
To compute each row, eitther &quot;X&quot; or &quot;0&quot;, left-shifted.
</p>

<ul>
<li>
The rows are in the order of &quot;2N&quot;
</li>
<li>
You have to sum them up, and you can do this pairwise.
</li>
</ul></li>
<li><p>
If you do <em>n</em> times an operation which costs <em>n</em>, your runtime is going to be <span class="math">\(n^2\)</span>.
</p>

<ul>
<li>
Multiplication is more expensive than addition.
</li>
<li>
Multiplication is quadratic, where addition is linear (with respect to the size of the input).
</li>
</ul></li>
</ul>

<h3>
Alternative Multiplication
</h3>

<ul>
<li><p>
If you have two decimal numbers, <em>x</em> and <em>y</em>, write them next to each other.
</p>

<pre><code>11           13
5            26
2   IGNORE   52
1            104
----------------
            143 (= 13 + 26 + 104)
</code></pre></li>
<li><p>
Notice that the third row is ignored in both varieties of multiplication.
</p></li>
</ul>

<!--    $$x \times y =\begin{cases} 2 (x \times \lfloor \frac{y}{2} \rfloor, & \text{if $y$ is even}.\\ x + 2(x \times \lfloor \frac{y}{2} \rfloor, & \text{if $y$ is odd}. \end{cases}$$-->

<ul>
<li>
<p>The expensive operation is the addition if <span class="math">\(y\)</span> is odd. So O(n) due to addition.</p>
<ul>
<li>
Again, big-O is quadratic.
</li>
</ul></li>
</ul>

<h3>
Modula Arithmetic
</h3>

<p>
<span class="math">\[O(n)\]</span>
</p>

<h3>
Modulo Multiplication
</h3>

<ul>
<li>
<p>The product can be in the order of <span class="math">\((N - 1)^2\)</span>.</p>
<ul>
<li>
The product will be at most <em>2N</em> bits long.
</li>
</ul></li>
</ul>

<p>
<span class="math">\[O(n^2)\]</span>
</p>

<h3>
Problems
</h3>

<dl>
<dt>
Primality
</dt>
<dd>
<p>
Given a number N, determine whether it is prime.
</p>
</dd>

<dt>
Factorning
</dt>
<dd>
<p>
Given a number N, express it as a product of prime numbers.
</p>
</dd>
</dl>

<h2>
January 29th, 2014 <small>Recitation</small>
</h2>

<h3>
Asymptotic Bounds
</h3>

<ul>
<li>
<p>There are three types we'll use</p>
<ol>
<li>
Tight bound
</li>
<li>
Lower bound
</li>
<li>
Upper bound, &quot;big O&quot;
</li>
</ol></li>
</ul>

<h4>
Upper Bound (Big-O)
</h4>

<p>
<span class="math">\[f(n) = O(g(n))\]</span>
</p>

<blockquote>
  <p>
G is upper upper bound for F if and only if there exists a constant and <span class="math">\(n_0\)</span> such that:
</p>
  
<p>
<span class="math">\(0 \le f(n) \le c g(n)\)</span>
</p>
</blockquote>

<p>
<strong>O(f) grows no faster than ..</strong>
</p>

<h4>
Lower Bound (Big Omega)
</h4>

<p>
<span class="math">\[f(n) = \Omega(g(n))\]</span> <span class="math">\[\exists c, n_0 (0 \le c g(n) \le f(n) \forall n \ge n_o)\]</span>
</p>

<p>
<strong>O(f) grows faster than ...</strong>
</p>

<h4>
Tightly Bound (Big Theta)
</h4>

<p>
<strong>O(f) grows equal to</strong>
</p>

<h3>
Runtimes
</h3>

<table>
<thead>
<tr>
  <th>
Name
</th>
  <th>
Function
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
Logarithmic
</td>
  <td>
<em>O(log(n))</em>
</td>
</tr>
<tr>
  <td>
Constant
</td>
  <td>
<em>O(n)</em>
</td>
</tr>
<tr>
  <td>
Power
</td>
  <td>
<span class="math">\(O(n^a)\)</span>
</td>
</tr>
<tr>
  <td>
Exponential
</td>
  <td>
<span class="math">\(O(a^n)\)</span>
</td>
</tr>
</tbody>
</table>

<h2>
January 31st, 2014 <small>Lecture</small>
</h2>

<ul>
<li>
<p>All cryptography is based on number theory and number theoretic properties.</p>
<ul>
<li>
Think of message as modulo <em>N</em>.
</li>
<li>
Longer message can be broken into smaller pieces.
</li>
<li>
What is a good value for <em>N</em>?
</li>
</ul></li>
</ul>

<h3>
Property
</h3>

<ul>
<li><p>
Pick any two primes, and lets call them <em>p</em> and <em>q</em>.
</p>

<ul>
<li>
Then let <em>N</em> be their product, <em>N</em> = <em>p</em> * <em>q</em>.
</li>
<li>
For any <em>e</em> so that gcd(e, (p - 1)(q - 1)) = 1
</li>
<li><p>
Then the following are true:
</p>

<ol>
<li><p>
Take a number x and think of it as your message in your communication, then <span class="math">\(x \to x^e\)</span> is a bijection.
</p>

<ul>
<li>
An a bijection is a one-to-one mapping.
</li>
</ul></li>
<li><p>
Let <span class="math">\(d = e^{-1} mod (p - 1)(q - 1)\)</span>, then,
</p>

<p>
<span class="math">\[\forall x \in [0, N - 1] (x^e)^d \equiv x mod N\]</span>
</p></li>
</ol></li>
</ul></li>
</ul>

<h4>
Example
</h4>

<ul>
<li><p>
Lets say our two prime numbers are p = 5, q = 11.
</p>

<ul>
<li>
Our N = pq = 55.
</li>
<li><p>
Now we need to find e such that gcd(e, (5 - 1)(11 - 1)).
</p>

<ul>
<li>
So the <abbr title="Greatest common divisor">GCD</abbr> is 1.
</li>
</ul></li>
<li><p>
So lets pick e = 3 is sufficient??
</p>

<ul>
<li><p>
So what is the valye of d?
</p>

<p>
<span class="math">\[d \equiv e^{-1} \bmod (p - 1)(q - 1) \equiv 3^{-1} \bmod 40\]</span>
</p></li>
</ul></li>
<li><p>
This means that 3 times d is equal to modulo 40, which is the modular inverse.
</p>

<ul>
<li>
For d = 27, we have 3 times 27 which equals 81 or modulo 40.
</li>
</ul></li>
<li><p>
For any message, x modula 55, ecryption is
</p>

<p>
<span class="math">\[y = x^3 \bmod 55\]</span>
</p></li>
<li><p>
For any message, decryption is
</p>

<p>
<span class="math">\[x = y^{27} \bmod 55\]</span>
</p></li>
</ul></li>
</ul>

<h3>
Proof of property
</h3>

<ul>
<li><p>
We have to show that
</p>

<p>
<span class="math">\[(x^e)^d \bmod N \equiv x \bmod N\]</span>
</p>

<ul>
<li><p>
The following are true if e and d have been selected as specific
</p>

<p>
<span class="math">\[e \times d \equiv 1 \bmod (p - 1)(q - 1) \equiv e \times d = k (p - 1)(q - 1) + 1\]</span>
</p>

<blockquote>
  <p>
<strong>Fermat's Little Theorem</strong>: If <em>p</em> is prime, then <span class="math">\(\forall 1 \le a \le p\)</span>:
</p>
  
<p>
<span class="math">\(a^p \equiv a \bmod p\)</span> <span class="math">\(a^{p -1} \equiv 1 \bmod p\)</span>
</p>
</blockquote></li>
</ul></li>
</ul>

<h3>
Summary of <abbr title="Rivest-Shamir-Adelman">RSA</abbr>
</h3>

<h4>
Bob
</h4>

<ol>
<li><p>
Pick two prime numbers, this is the <em>p</em> and <em>q</em>
</p>

<ul>
<li>
And for the security of the system, pick two <em>large</em> primes.
</li>
</ul></li>
<li><p>
You announce to the world that everyone should be sending your message, that is, publish (N, e) where N equals p times q and e relative prime to (p - 1)(q - 1).
</p></li>
<li>
<p>Internally compute the private key, which is d equal the inverse of e module (p - 1)(q - 1).</p>
<ul>
<li>
If you mulitply the two and modulo product, the value should be one.
</li>
</ul></li>
</ol>

<h4>
Alice
</h4>

<ol>
<li>
Generate encrypted message <span class="math">\(x^e \bmod N\)</span> where e and N come from Bob public key.
</li>
<li>
When Bob receives this message <span class="math">\(x = y^d \bmod N\)</span>.
</li>
</ol>

<h3>
Why is this secure?
</h3>

<ul>
<li><p>
To break the system, Eve must be able to compute x that has never left Alice given the publically available information, and the public key (N, e).
</p>

<ul>
<li>
<p>How do you do this?</p>
<ul>
<li>
Try to guess x so that <span class="math">\(y = x^e\)</span>
</li>
</ul></li>
</ul></li>
<li><p>
Alternatively, she can try to factor out p and q from N, hence, the intractable factoring problem.
</p></li>
</ul>

<h3>
Analysis
</h3>

<blockquote>
  <p>
What are the operations of <abbr title="Rivest-Shamir-Adelman">RSA</abbr> and what is the running time?
</p>
</blockquote>

<ul>
<li>
Modular exponentiation, plus the decoding.
</li>
<li>
We have to select e, a small integer, but it has to be a relative prime
</li>
<li>
Compute <span class="math">\(d = e^{-1} \bmod (p - 1)(q - 1)\)</span> which always exists when <span class="math">\(e\)</span> is a relative prime.
</li>
<li>
Pick 2 large prime numbers.
</li>
</ul>

<h2>
February 7th, 2013 <small>Lecture</small>
</h2>

<h3>
<abbr title="Rivest-Shamir-Adelman">RSA</abbr>
</h3>

<ul>
<li><p>
Pick 2 large <em>n</em>-bit primes.
</p>

<p>
<span class="math">\[N = pq\]</span> <span class="math">\[e : \gcd(e, (p - 1), (q - 1)) = 1\]</span> <span class="math">\[d : d = e^{-1} \bmod (p - 1) (q - 1)\]</span>
</p></li>
</ul>

<h3>
Greatest Common Divisor
</h3>

<ul>
<li>
<p>If you could perform factoring efficiently, the you could solve the problem.</p>
<ul>
<li>
But we do not have one. So <abbr title="Rivest-Shamir-Adelman">RSA</abbr> is safe.
</li>
</ul></li>
</ul>

<dl>
<dt>
Euclid's observation
</dt>
<dd>
<p>
<span class="math">\(\gcd(x, y) = \gcd(x \bmod  y, y)\)</span>
</p>

<pre><code>function euclid(a, b) {
    if (b == 0) {
        return a;
    } else {
        return euclid(b, a \bmod b);
    }
}
</code></pre>
</dd>
</dl>

<h2>
February 12th, 2014 <small>Lecture</small>
</h2>

<h3>
Review of <abbr title="Rivest-Shamir-Adelman">RSA</abbr>
</h3>

<ul>
<li>
<p>Sender</p>
<ul>
<li>
Picks two random primes <em>p</em> and <em>q</em>.
</li>
<li>
Sets <em>N</em> equal to <em>pq</em>.
</li>
<li>
<code>gcd(e, (p - 1)(q - 1))</code>
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Fermat</strong>: If a number <em>p</em> is prime, then for all smaller numbers, it is the case that <span class="math">\(a^{p - 1} \equiv 1 \mod p\)</span>
</p>
</blockquote>

<ul>
<li><p>
How probable is it that this test succeeds? Less than half.
</p></li>
<li><p>
A way of generating random primes
</p>

<ul>
<li>
Pick a random number.
</li>
<li>
Apply the primality test accounrd to Fermat.
</li>
<li>
If it succeeds, then return it.
</li>
</ul></li>
<li><p>
Good news:
</p>

<ul>
<li>
Prime numbers are abundant, frequently arising.
</li>
<li>
A random <em>n</em>-bit number has roughly <span class="math">\(\frac{1}{n}\)</span> chance of being prime.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Lagrange's Prime Number Theorem</strong>: Let <span class="math">\(\pi(x)\)</span> be the number of primes <span class="math">\(\le x\)</span>.
</p>
  
<p>
<span class="math">\[\pi(x) \approx \frac{x}{\ln(x)}\]</span>&gt; Or more precisely,
</p>
  
<p>
<span class="math">\[\lim_{x \to \infty} \frac{\pi(x)}{\frac{x}{\ln(x)}} = 1\]</span>
</p>
</blockquote>

<h3>
Hashing <small>Another Application of Number-Theoretic Algorithms</small>
</h3>

<ul>
<li><p>
<strong>Objective</strong>: Store and efficiently retrieve IP addresses of the form <code>128.32.168.80</code>.
</p>

<ul>
<li>
There are <span class="math">\(2^{32}\)</span> possibilities.
</li>
</ul></li>
<li><p>
Let's say you have <em>n</em> of them (<span class="math">\(n &amp;lt;&amp;lt; 2^{32}\)</span>).
</p></li>
<li><p>
One possible way of storing them is an <em>array</em> of 2 to the 32 size that indicates whether an IP address is in the set <em>n</em> IP addresses.
</p></li>
<li><p>
Another possibility is a <em>linked list</em>, where you only store the <em>n</em> IP addresses.
</p>

<ul>
<li>
But <em>O(n)</em> time to access them.
</li>
</ul></li>
<li><p>
Hash tables try to provide a trade-off.
</p>

<ul>
<li>
Create an array in the order of <em>n</em>.
</li>
</ul></li>
<li><p>
We need hash function that can return an index to the array and have the following properties:
</p>

<ul>
<li>
<p>The function will scatter the elements in the array, it will be &quot;random&quot; where they're going to be placed.</p>
<ul>
<li>
If your given the same placement, you should get the same value.
</li>
<li>
Consistency for the same input, it should always return the same index in the array.
</li>
</ul></li>
</ul></li>
<li><p>
Picking one of the four numbers can work as a hash function under the assumption that that input is uniformly distributed.
</p></li>
<li>
<strong>Objective</strong>: Regardless of the input, we need the &quot;random&quot; property.
</li>
<li>
<strong>Realization</strong>: There is no single hash function that behaves well on all input data.
</li>
<li><p>
<strong>Idea</strong>: Pick from a family of hash functions randomly so that the probability of 2 elements to be mapped to the same index is <span class="math">\(\frac{1}{257}\)</span> size of the array.
</p></li>
<li><p>
Every IP address is a tuple : <span class="math">\(\lbrace x_1, x_2, x_3, x_4 \rbrace\)</span> where <span class="math">\(x_1 \in [1, 256]\)</span>.
</p>

<ul>
<li><p>
Consider the has function
</p>

<p>
<span class="math">\[h_\alpha (x_1, x_2, x_3, x_4) = \sum_{i = 1}^{4} \alpha_1 \times x_i \mod N\]</span> If you pick the numbers alpha-1 at random, then <span class="math">\(h_\alpha\)</span> is likely to be good.
</p></li>
</ul></li>
<li><p>
<strong>Property</strong>: Consider any 2 elements <em>x</em>-1 through <em>x</em>-4 and corresponding <em>y</em>s. If the coefficients are chosen and uniformly at random mod <em>N</em>, then
</p></li>
</ul>

<h2>
February 12th, 2014 <small>Recitation</small>
</h2>

<h3>
Probability Theorem
</h3>

<ul>
<li>
<p>If you have event <em>A</em>, and you want to measure how <em>probable</em> this event is.</p>
<ul>
<li>
We have some axioms. The axioms of probability.
</li>
</ul></li>
</ul>

<blockquote>
  <ol>
  <li>
<span class="math">\(1 \ge P(A) \ge 0\)</span>
</li>
  <li>
<span class="math">\(P(S) = 1\)</span>
</li>
  <li>
<span class="math">\(P(A \bigcup B) = P(A) + P(B)\)</span> if <em>A</em> and <em>B</em> are mutually exclusive.
</li>
  </ol>
</blockquote>

<h4>
Event of throwing a dice
</h4>

<ul>
<li>
<strong>Output</strong>: {1, 2, 3, 4, 5, 6}
</li>
<li>
The probability of any individual role is one in six.
</li>
<li>
The events are independant.
</li>
<li>
The event can be mutually exclusive (with themselves).
</li>
</ul>

<h4>
Bayes Theorem
</h4>

<blockquote>
  <p>
<strong>Bayes' Theorem</strong>:
</p>
  
<p>
<span class="math">\[P(A | B) = \frac{P(B | A) \times P(A)}{P(B)}\]</span>
</p>
</blockquote>

<h2>
February 14th, 2014 <small>Lecture</small>
</h2>

<h3>
Divide and Conquer Algorithms
</h3>

<ul>
<li><p>
What is the characteristic here?
</p>

<ul>
<li>
Break your problem into smaller instances.
</li>
<li>
In the case of the number-theoretic algorithms, the way we were evaluating the run-time was with bits.
</li>
<li>
Then, recursively solve smaller sub-problems and then combine these answers.
</li>
</ul></li>
<li><p>
What was the recursively algorithm for multiplication? It was already an instance of <em>divide and conquer</em>.
</p>

<ul>
<li>
Before for multiplcation, the running time had a big-O of &quot;n-squared&quot;
</li>
</ul></li>
</ul>

<h4>
New Multiplication
</h4>

<ul>
<li><p>
Now we'll get a better running time for multiplication, with <em>another idea for multiplcation.</em>
</p>

<ul>
<li><p>
Cnosider the following way of writing numbers:
</p>

<ul>
<li><p>
<em>x</em> is written as two numbers, with <em>x</em> &quot;sub left&quot; and <em>x</em> &quot;sub right&quot;, where if it's 10 bits, you have two five bit numbers.
</p>

<p>
<span class="math">\[x = 2^{\frac{n}{2}} \times x_L + x_R\]</span> <span class="math">\[y = 2^{\frac{n}{2}} \times y_L + y_R\]</span> <span class="math">\[x \times y = (2^{\frac{n}{2}} \times x_L + x_R)(2^{\frac{n}{2}} \times y_L + y_R)\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
Lets think about the new operations and how much they cost.
</p>

<ul>
<li><p>
In the above representation, we have:
</p>

<ul>
<li>
Additions (linear)
</li>
<li>
Multiplecation with powers of 2 (linear)
</li>
<li>
Four multiplications between &quot;n over two&quot; bit numbers where we call recursively the same operation.
</li>
</ul></li>
<li><p>
We can define a recursive relation:
</p>

<p>
<span class="math">\[T(n) = 4 \times T(\frac{n}{2}) + O(n)\]</span>
</p></li>
</ul></li>
<li>
Gauss observation reowkred the above expression so as to make use of only 3 of the &quot;n over two&quot; multiplications.
</li>
<li><p>
At the <span class="math">\((log_2 n\)</span>)^{2n}$ level, we get down to size-1.
</p>

<ul>
<li>
At each level we have <span class="math">\(3^k\)</span> subproblems, each of them of size <span class="math">\(\frac{n}{2^k}\)</span>
</li>
<li>
At each level you have a linear cost for combining the subproblems.
</li>
<li><p>
At depth <em>k</em>,
</p>

<p>
<span class="math">\[3^k \times O(\frac{n}{2^k}) = (\frac{3}{2})^k \times O(n)\]</span>
</p></li>
<li><p>
Now, we've managed to decrease the run-time to something like <em>n</em> to-the 1.59 as opposed to <em>n</em>-squared.
</p></li>
</ul></li>
<li><p>
We are solving multiplication with a divide-and-conquer approach.
</p>

<ul>
<li>
By decreasing the number of recursive calls, we managed to get a running time that is <em>better</em>, i.e. the branching factor in terms of recursive calls matters.
</li>
<li><p>
As a matter of fact, when you have something like ...
</p>

<p>
<span class="math">\[T(n) = \alpha \times T(\frac{n}{b}) + O(n^d)\]</span>
</p></li>
</ul></li>
</ul>

<h3>
Sorting Problems
</h3>

<blockquote>
  <p>
<strong>Master theorem</strong>: If you have a recurisve cost-function of the form <span class="math">\(T(n) = a \times T(\frac{n}{b}) + O(n^2)\)</span> then:
</p>
</blockquote>

<!-- $$T(n) =   \begin{cases} O(n^d) & \text{if } d < log_b a \\ O(n^d \log(n)) & \text{if } d = \log_b a \\ O(n^{log_b a} & \text{if } d < log_b a \end{cases}$$-->

<ul>
<li><p>
Assume that <em>n</em> is a power of <em>b</em> for convinience.
</p>

<ul>
<li>
The size of the problem decreases by <em>b</em> at every level.
</li>
</ul></li>
<li><p>
We need <span class="math">\(log_b n\)</span> level to stop the recursion.
</p>

<ul>
<li><p>
At level <em>k</em> we have <span class="math">\(a^k\)</span> subproblems of size <span class="math">\(\frac{n}{b^k}\)</span>
</p>

<ul>
<li><p>
Work at level <em>k</em>:
</p>

<p>
<span class="math">\[a^k \times O((\frac{n}{b^k})^d) = O(n^d) \times (\frac{a}{b^d})^k\]</span>
</p></li>
</ul></li>
</ul></li>
</ul>

<h4>
Three cases
</h4>

<ol>
<li><p>
If <span class="math">\(\frac{a}{b^d} &amp;lt; 1\)</span>, series is decreasing.
</p>

<ul>
<li>
The &quot;first term&quot; dominates.
</li>
<li><p>
Running time:
</p>

<p>
<span class="math">\[O(n^d)\]</span>
</p></li>
</ul></li>
<li><p>
If <span class="math">\(\frac{a}{b^d} &gt; 1\)</span>, series is increasing
</p>

<ul>
<li>
The &quot;last term dominates&quot;
</li>
<li><p>
Running time:
</p>

<p>
<span class="math">\[O(n^{\log_b a})\]</span>
</p></li>
</ul></li>
<li><p>
If <span class="math">\(\frac{a}{b^d} = 1\)</span>, all terms are equivilent.
</p>

<p>
<span class="math">\[O(n^d) = O(n^{\log_b a})\]</span>
</p></li>
</ol>

<h2>
February 16th, 2014 <small>Reading</small>
</h2>

<h3>
Chapter 1 <small>Algorithms with numbers</small>
</h3>

<dl>
<dt>
Factoring
</dt>
<dd>
<p>
Given a number <em>N</em>, express it as a product of its prime factors.
</p>
</dd>

<dt>
Primality
</dt>
<dd>
<p>
Given a number <em>N</em>, determine whether it is a prime.
</p>
</dd>
</dl>

<ul>
<li><p>
Factoring is hard.
</p>

<ul>
<li>
Despite lots of effot, the fastest method is exponetial to the number of bits.
</li>
</ul></li>
<li><p>
We <em>can</em> efficiently test whether something <em>is</em> prime!
</p></li>
</ul>

<h4>
Basic arithmetic
</h4>

<h5>
Addition
</h5>

<blockquote>
  <p>
<strong>Basic property of decimal numbers</strong>: The sum of any three single-digit numbers is <em>at most</em> two digits long.
</p>
</blockquote>

<ul>
<li><p>
This simple rule gives us a way to add two numbers in any base:
</p>

<ul>
<li><p>
Align their right-hand ends, and then perform single right-to-left pass in which the sum in which the sum is computed digit-by-digit, maintaining the overflow as a carry.
</p>

<ul>
<li><p>
Since we know each individual sum is a two-digit number, <em>the carry is always a single digit</em>, and so at any given step, three single digit numbers are added.
</p>

<pre><code>Carry: 1        1  1  1     
          1  1  0  1  0  1  (53)
          1  0  0  0  1  1  (35)
       -------------------
       1  0  1  1  0  0  0  (88)
</code></pre></li>
</ul></li>
</ul></li>
<li><p>
<em>Given two binary numbers, how long does our algorithm take to add them?</em>
</p>

<ul>
<li><p>
We want the answer expressed as a function of <em>the size of input</em>.
</p>

<ul>
<li>
The number of bits.
</li>
</ul></li>
<li><p>
Suppose that <em>x</em> and <em>y</em> are <em>n</em> bits longs.
</p>

<ul>
<li>
The the sum of <em>x</em> and <em>y</em> is <em>n + 1</em> bits <em>at most</em>.
</li>
<li>
Each bit of the sum is computed in a fixed amount of time.
</li>
<li>
<p>The total running time for addition:</p>
<ul>
<li>
Of the form <span class="math">\(c_0 + c_1 n\)</span> where &quot;c-zero&quot; and &quot;c-one&quot; are some constant.
</li>
<li>
In other words, it is <em>linear</em>.
</li>
<li>
The running time is <em>O(n)</em>.
</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>
<em>Is there a faster algorithm?</em>
</p>

<ul>
<li>
In order to add two <em>n</em>-bit numbers, we must at least read them and write down the answer, and even that requires <em>n</em> operations.
</li>
<li>
The algorithm is optimal, up to multiplicative constants!
</li>
</ul></li>
<li><p>
<em>Why O(n) operations? Isn't binary addition done in one instruction?</em>
</p>

<ol>
<li><p>
Only addition operations that are within a computers word-length, often 32 or 64.
</p>

<ul>
<li>
It is often useful and necessary to handle number much larger than this, several thousand bits long.
</li>
<li>
Operations on these big numbers is often like operating bit-by-bit.
</li>
</ul></li>
<li><p>
When we want to understand algorithms, it makes sense to study the basic algorithms that encoded to the hardware.
</p>

<ul>
<li>
Focus on <em>bit complexity</em>.
</li>
</ul></li>
</ol></li>
</ul>

<h5>
Multiplication and division
</h5>

<ul>
<li><p>
The grade-school algorithm for multiplying to numbers is to create an array of intermiediate sums, each representing the product of the first number by a single digit of the second.
</p>

<ul>
<li>
These values are left-shifted and right-shifted and added.
</li>
</ul></li>
<li><p>
Thirteen times eleven in binary:
</p>

<pre><code>            1  1  0  1  (binary 13)
         x  1  0  1  1  (binary 11)
----------------------
            1  1  0  1  (1101 times 1, shifted no)
         1  1  0  1     (1101 times 1, shifted once)
      0  0  0  0        (1101 times 0, shifted twice)
+  1  1  0  1           (1101 times 1, shifted thrice)
----------------------
1  0  0  0  1  1  1  1  (binary 143)
</code></pre></li>
<li><p>
Psuedo code:
</p>

<pre><code>function multiply(x, y) {
    if (y == 0) {
        return 0
    }

    z = multiply(x, floor(y / 2));

    if (y is even) {
        return 2z;
    } else {
        return x + 2z;
    }
}
</code></pre></li>
<li><p>
<em>How long does this take?</em>
</p>

<ul>
<li>
If <em>x</em> and <em>y</em> are both <em>n</em>-bits, then there are <em>n</em> intermediate rows, with lengths of up to <em>2n</em> bits (taking the shiting into account).
</li>
<li><p>
The total time to add up these rows, doing two numbers at a time:
</p>

<p>
<span class="math">\[O(n) + O(n) + ... + O(n)\]</span> <em>(n - 1)</em> times.
</p>

<ul>
<li>
This is <span class="math">\(O(n^2)\)</span>, or <em>quadratic</em> in the size of the inputs.
</li>
<li>
Still polynomial but much slower than addition.
</li>
</ul></li>
</ul></li>
<li><p>
<em>Can this be improved?</em>
</p>

<ul>
<li>
Al Khwarizmi knew another way to multiple.
</li>
<li><p>
To multiply two decimal numbers, write them next to each other.
</p>

<ul>
<li>
Then, repeat: Divide the first number by 2, rounding down the result, and double the second number. Keep going until the first number gets down to 1.
</li>
<li><p>
Then, strike out all the rows in which the first number is even, and add up whatever remains in the second column.
</p>

<pre><code>11  13
 5  26
 2  52  (strike out)
 1 104
------
   143  (answer)
</code></pre></li>
</ul></li>
</ul></li>
<li><p>
Pseudocode!
</p>

<pre><code>function divide(x, y) {
    if (x == 0) {
        return (q, r) = (0, 0);
    }

    (q, r) = divide(floor(x / 2), y);
    q = 2 * q;
    r = 2 * r;

    if (x is odd) {
        r = r + 1;
    } 

    if (r &gt;= y) {
        r = r - y;
        q = q + 1;
    }

    return (q, r)
}
</code></pre></li>
<li><p>
<em>Is this algorithm correct?</em>
</p>

<ul>
<li>
Verify that it mimics the description of the rules.
</li>
</ul></li>
<li><p>
<em>How long does this algorithm take?</em>
</p>

<ul>
<li><p>
It must terminate after <em>n</em> recursive calls, because at each call <em>y</em> is halved.
</p>

<ul>
<li>
That is, the number of bits is decreased by one.
</li>
</ul></li>
<li><p>
Each recursive call requires these operations:
</p>

<ul>
<li>
A division by 2 (right shift)
</li>
<li>
A test for odd/even (looking up the last bit)
</li>
<li>
A multiplication by 2 (left shift)
</li>
<li>
Possible on addition, O(n).
</li>
</ul></li>
<li><p>
The total time is <span class="math">\(O(n^2)\)</span> therefore.
</p></li>
</ul></li>
<li><p>
<em>Can we do better?</em>
</p>

<ul>
<li>
Intuitively, you think that you need to, at most, do <em>n</em> operations <em>n</em> times to add.
</li>
<li>
But no! Chapter 2 will show you can do better.
</li>
</ul></li>
</ul>

<h4>
Modular arithmetic
</h4>

<ul>
<li><p>
With repeated addition or multiplication, numbers get very big.
</p>

<ul>
<li>
We &quot;reset to zero&quot; whenever time reaches 24.
</li>
<li>
Similarly, the built-in arithmetic operations of computer processors, numbers are restricted to a size, say 32 or 64, which is generous for most purposes.
</li>
</ul></li>
<li><p>
For primality testing and cryptography, it is necessary to deal with numbers that are significantly bigger.
</p></li>
<li>
<em>Modular arithmetic</em> is a system for dealing with restricted ranges of integers.
</li>
</ul>

<dl>
<dt>
<em>x</em> <em>modulo</em> <em>N</em>
</dt>
<dd>
<p>
The remainder when <em>x</em> is divided by <em>N</em>.
</p>

<p>
If <em>x = qN + r</em> with <em>0 &lt;= r &lt; N</em>, then <em>x</em> modulo <em>N</em> is equal to <em>r</em>.
</p>
</dd>
</dl>

<ul>
<li><p>
One way to think of modular arithmetic deals with all the integers, but divides them in <em>N</em> <em>equivilence classes</em>, each of the form <span class="math">\(\lbrace i + kN : k \in \mathbb{Z} \rbrace\)</span> for some <em>i</em> between ) and <em>N - 1</em>.
</p>

<ul>
<li><p>
For example, there are three equivilence classes of modulo 3:
</p>

<pre><code>...  -9  -6  -3  0  3  6  9  ...
...  -8  -5  -2  1  4  7  10 ...
...  -7  -4  -1  2  5  8  11 ...
</code></pre>

<p>
Any member of the class is substituable for any other.
</p></li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Substitution rule</strong>: If <span class="math">\(x \equiv x&#39; ( \bmod N)\)</span> and <span class="math">\(y \equiv y&#39; (\bmod N)\)</span>, then:
</p>
  
<p>
<span class="math">\[x + y \equiv x&#39; + y&#39; (\bmod N)\]</span>&gt; <span class="math">\[xy \equiv x&#39; y&#39; (\bmod N)\]</span> - It is not hard to check that in modular arithmetic, the usual associate, commutative, and distributive properties of addition and multiplication continue to apply. - For instance: - Associativity - Commutativity - Distributivity
</p>
</blockquote>

<pre><code>-   You can simplify big numbers with this. Witness:

    $$2^{345} \equiv (2^5)^{69} \equiv 32^{69} \equiv 1^69 \equiv 1 (\bmod 31)$$
</code></pre>

<h5>
Modular addition and multiplication
</h5>

<ul>
<li><p>
To <em>add</em> two numbers &quot;<em>x</em> and <em>y</em> modulo <em>N</em>&quot;, we start with regular addition.
</p>

<ul>
<li>
Since <em>x</em> and <em>y</em> are both in the range of 0 to <em>N - 1</em>, their sum is between <em>0</em> and <em>2(N - 1)</em>.
</li>
<li>
If the sum exceeds <em>N - 1</em>, we merely need to subtract of <em>N</em> to bring it back to the required range.
</li>
<li>
The overal computation therefore consists of an addition, and possibly a substraction, of numbers that never exceed <em>2N</em>.
</li>
<li>
<p>Its running time is linear in the sizes of these numbers.</p>
<ul>
<li>
In other words, <em>O(n)</em>, where <em>n = ceil(log N)</em>, is the size of <em>N</em>.
</li>
</ul></li>
</ul></li>
<li><p>
To <em>multiply</em> two mod-<em>N</em> numbers <em>x</em> and <em>y</em>, we again just start with regular multiplication and then reduce the answer modilo <em>N</em>.
</p>

<ul>
<li><p>
The produce can be as large as <span class="math">\((N - 1)^2\)</span>.
</p>

<ul>
<li>
But this is still at most <em>2n</em> bits long.
</li>
</ul></li>
<li><p>
To recude the answer modulo <em>N</em>, we compute the remainder upon dividing it by <em>N</em>, using our quadratic-time division algorith,
</p></li>
<li>
Multiplication remains a quadratic operation.
</li>
</ul></li>
<li><p>
<em>Division</em> is not so easy.
</p>

<ul>
<li>
Will be do later.
</li>
</ul></li>
<li><p>
To complete the suit of modular arithmetic primitives we need for cryptography, we next turn to <em>modular exponentiation</em>, and then to <em>greatest common divisor</em>, which is the key to division.
</p></li>
</ul>

<h5>
Modular exponentiation
</h5>

<ul>
<li><p>
<em>What is the algorithm?</em>
</p>

<pre><code>function modular-exponentiation(x, y, N) {
    if (y == 0) {
        return 1;
    }

    z = modulor-exponentiation(x, floor(y / 2), N);

    if (y is even) {
        return (z ** 2) % N;
    } 

    else {
        return (x * (z ** 2)) % N;
    }
}
</code></pre></li>
<li><p>
In the cryptosystem we are working towards, it is necessary to compute &quot;<em>x</em> to the <em>y</em> mod <em>N</em>&quot; for the values of <em>x</em>, <em>y</em>, and <em>N</em> that are several hundred bits long. <em>How can this be done quickly?</em>
</p></li>
<li><p>
The result is some number modulo <em>N</em> and is therefore itself a few hundred bits long.
</p>

<ul>
<li>
<p>However, the raw value of &quot;<em>x</em> to the <em>y</em>&quot; could be much, much longer than this.</p>
<ul>
<li>
Even when <em>x</em> and <em>y</em> are just 20-bit numbers, &quot;<em>x</em> to the <em>y</em>&quot; is <em>at least</em> <span class="math">\((2^{19})^{2^{19}} = 2^{(19)(524288)}\)</span>, about 10 million bits long!
</li>
</ul></li>
</ul></li>
<li><p>
To make sure the numbers we are dealing with never grow too large, we need to perform all intermediate computations modulo <em>N</em>.
</p>

<ul>
<li><p>
Here's the idea:
</p>

<ul>
<li><p>
Calculate <span class="math">\(x^y \bmod N\)</span> by repeatedly multiplying by <em>x</em> modulo <em>N</em>. The resulting sequence of intermediate products,
</p>

<p>
<span class="math">\[x \bmod N \to x^2 \bmod N \to x^3 \bmod N \to ... \to x^y \bmod N\]</span> consists of number that are smaller than <em>N</em>, and so the individual multiplications do not take too long.
</p></li>
<li>
But there's a problem! If <em>y</em> is 500 bits long, we need to perform &quot;2 to the 500&quot; multiplication! The algorithm is clearly exponetial in the size of <em>y</em>.
</li>
</ul></li>
</ul></li>
<li><p>
Luckily, we can do better.
</p>

<ul>
<li><p>
Starting with <em>x</em> and <em>squaring repeatedly</em> modulo <em>N</em>, we get:
</p>

<p>
<span class="math">\[x \bmod N \to x^2 \bmod N \to \cdots \to x^{2^{\log y}} \bmod N\]</span>
</p></li>
<li><p>
Each takes <span class="math">\(O(log^2 N)\)</span> to compute, with only <em>log y</em> multiplications.
</p>

<ul>
<li><p>
To determine this, multupli together these powers.
</p>

<p>
<span class="math">\[x^{25} = x^{11001_2} = x^{10000_2} \times x^{1000_2} \times x^{1_2} = x^{16} \times x^8 \times x^1\]</span>
</p></li>
<li>
A polynomial time algorithm
</li>
</ul></li>
</ul></li>
<li><p>
We can package this idea in a simply recusrive algorithm described at this begninng of this section.
</p>

<ul>
<li><p>
Let <em>n</em> be the size of bits <em>x</em>, <em>y</em>, and <em>N</em> (whichever of the three is largest)
</p>

<ul>
<li>
Like multiplication, the algorithm will halt after <em>at most</em> <em>n</em> recursive calls.
</li>
<li>
During each call it multiplies <em>n</em>-bit numbers
</li>
<li>
Doing computation modulo <em>N</em> saves us here.
</li>
</ul></li>
<li><p>
Running time of <span class="math">\(O(n^3)\)</span>
</p></li>
</ul></li>
</ul>

<h5>
Euclids algorithms for greatest common divisor
</h5>

<ul>
<li>
Given to integerns <em>a</em> and <em>b</em>, find the largest integer that divides both of them, known as their <em>greatest common divisor</em> (gcd).
</li>
<li><p>
The most obvious approach is to first factor <em>a</em> and <em>b</em>, and the multiply together their common factors.
</p>

<ul>
<li>
For instance, <span class="math">\(1035 = 3^2 \cdot 5 \cdot 23\)</span> and <span class="math">\(759 = 3 \cdot 11 \cdot 23\)</span>, so their <abbr title="Greatest common divisor">GCD</abbr> is <span class="math">\(3 \cdot 23 = 69\)</span>.
</li>
<li>
<p>However, we have no efficient algorithm for factoring.</p>
<ul>
<li>
<em>Is there some other way?</em>
</li>
</ul></li>
</ul></li>
<li><p>
Euclid's algorithm uses the following simple formula.
</p>

<blockquote>
  <p>
<strong>Euclid's rule</strong>: If <em>x</em> and <em>y</em> are positive integers with <span class="math">\(x \ge y\)</span>, then <span class="math">\(\gcd(x, y) = \gcd(x \bmod y, y)\)</span>.
</p>
</blockquote></li>
<li><p>
Euclid's rule allows us to write down an elegant recursive algorithm:
</p>

<pre><code>function Euclid(a, b) {
    if (b == 0) {
        return a;
    }

    return Euclid(b, a % b);

}
</code></pre>

<ul>
<li>
<p>This means that after any two consective rounds, both arguments <em>a</em> and <em>b</em> are <em>at very least</em> halved in value - the length of each decreases by at least on bit.</p>
<ul>
<li>
If they are initially <em>n</em>-bit integers, then the base case will be reached within <em>2N</em> recursive calls.
</li>
<li>
And since each call involves a quadratic-time division, the total time is <span class="math">\(O(n^3)\)</span>.
</li>
</ul></li>
</ul></li>
</ul>

<h5>
An extension of Euclid's algorithm
</h5>

<ul>
<li>
A small extensions is the key to dividing in the modular world.
</li>
<li><p>
Suppose someone claims that <em>d</em> is the <abbr title="Greatest common divisor">GCD</abbr> of <em>a</em> and <em>b</em>:
</p>

<ul>
<li>
How can we check this?
</li>
</ul>

<blockquote>
  <p>
<strong>Lemma</strong>: If <em>d</em> divides both <em>a</em> and <em>b</em>, and <em>d = ax + by</em> for some integers <em>x</em> and <em>y</em>, then necessarily <em>d = gcd(a, b)</em>.
</p>
</blockquote></li>
<li><p>
So, if we can supply two numbers <em>x</em> and <em>y</em> such that <em>d = ax + by</em>, then we can be sure that <em>d = gcd(a, b)</em>.
</p>

<ul>
<li>
What is even better is that those <em>x</em> and <em>y</em>s can be found by a small extension of Euclid's algorithm.
</li>
</ul>

<blockquote>
  <p>
<strong>Lemma</strong>: For any positive integers <em>a</em> and <em>b</em>, the extended Euclid algorithm returns integers <em>x</em>, <em>y</em>, and <em>d</em> such that <em>gcd(a, b) = d = ax + by</em>.
</p>
</blockquote></li>
</ul>

<h5>
Modular division
</h5>

<ul>
<li>
<p>In real arithmetic, every number that doesn't equal zero has an inverse, that is, one over itself.</p>
<ul>
<li>
<p>Dividing by a number is the same as multiplying by its inverse.</p>
<ul>
<li>
In <em>modular</em> arithmeric, we can make a similar definition.
</li>
</ul></li>
</ul></li>
</ul>

<dl>
<dt>
Multiplicative inverse
</dt>
<dd>
<p>
We say <em>x</em> is the <em>multiplicative inverse</em> of <em>a</em> modulo <em>N</em> if <span class="math">\(ax \equiv 1 (mod N)\)</span>.
</p>
</dd>
</dl>

<ul>
<li><p>
There can only be one, and we denote it with <span class="math">\(a^{-1}\)</span>.
</p>

<ul>
<li>
However, it doesn't <em>always</em> exist.
</li>
<li>
<p>For instance, 2 is not invertible modulo 6.</p>
<ul>
<li>
2 multiplied by <em>x</em> is not equivilient to 1 modulo 6 for any and every number.
</li>
<li>
In this case, <em>a</em> and <em>N</em> are both even and thus <em>a mod N</em> is alwaus even, since <em>a mod N = a - kN</em> for some <em>k</em>.
</li>
</ul></li>
</ul></li>
<li><p>
In fact, this is the only circumstance in which <em>a</em> is not invertible.
</p>

<ul>
<li>
<p>The <em>gcd(a, N) = 1</em> (this is called <em>relatively prime</em>), the extended Euclid algorithm gives us integers <em>a</em> and <em>y</em> such that <em>ax + Ny = 1</em>, which means that <span class="math">\(ax \equiv 1 (mod N)\)</span>.</p>
<ul>
<li>
This, <em>x</em> is <em>a</em>'s sought inverse.
</li>
</ul></li>
</ul></li>
</ul>

<dl>
<dt>
Example
</dt>
<dd>
<ul>
<li><p>
Continuing with our previous example, supose we computer <span class="math">\(1^{-1} \bmod 25\)</span>.
</p>

<ul>
<li><p>
Using the extended Euclid algorithm we find that
</p>

<p>
<span class="math">\[15 \cdot 25 - 34 \cdot 11 = 1\]</span>
</p></li>
<li><p>
Reducing both sides modulo 25, wh have
</p>

<p>
<span class="math">\[-34 \cdot 11 \equiv 1 \bmod 25\]</span>
</p></li>
<li>
Therefore, <span class="math">\(-34 \equiv 16 \bmod 25\)</span> is the inverse of <span class="math">\(11 \bmod 25\)</span>.
</li>
</ul></li>
</ul>
</dd>

<dt>
Modular division theorem
</dt>
<dd>
<p>
For any <span class="math">\(a \bmod N\)</span>, <em>a</em> has the a multiplicative inverse modulo <em>N</em> <abbr title="if and only if">iff</abbr> it is relatively prime to <em>N</em>. When this inverse exists, it can be found in time <span class="math">\(O(n^3)\)</span> by running the extended Euclid algorith,
</p>
</dd>
</dl>

<ul>
<li><p>
This resolve the issue of modular division.
</p>

<ul>
<li><p>
When working with modulo <em>N</em>, we can divide by numbers realtively prime to <em>N</em>.
</p>

<ul>
<li>
And <em>only by</em> these.
</li>
</ul></li>
<li><p>
To carry out the division, we multiply the inverse.
</p></li>
</ul></li>
</ul>

<h4>
Primality testing
</h4>

<dl>
<dt>
Fermat's little theorem
</dt>
<dd>
<p>
If <span class="math">\(p\)</span> is prime, then for every <span class="math">\(1 \le a &amp;lt; p\)</span>
</p>

<p>
<span class="math">\[a^{p - 1} \equiv 1 (\bmod p)\]</span>
</p>
</dd>

<dd>
<pre><code>function primality(N) {
    a = random(less than N); 

    if ((a ** (N - 1)) == (1 % N)) {
        return YES;
    }

    else {
        return NO;
    }
}
</code></pre>
</dd>
</dl>

<ul>
<li><p>
The problem is that Fermat's theorem is not an <abbr title="if and only if">iff</abbr> condition.
</p>

<ul>
<li>
It doesn't say what happens when <em>N</em> is <em>not</em> prime.
</li>
<li>
In fact, it <em>is</em> possible for a composite number <em>N</em> to pass Fermat's test.
</li>
</ul></li>
<li><p>
In analyzing the bahvior of this algorithm, we first need to get a minor bad case out the way.
</p>

<ul>
<li>
<p>There are extremely rare composite numbers called <em>Carmichael numbers</em>, and they pass Fermat's test for all <em>a</em> relatively prime to <em>N</em>.</p>
<ul>
<li>
On such numbers our algorithm will fail.
</li>
</ul></li>
</ul></li>
<li><p>
In a Carmichael-free universe, our algirothms work well.
</p></li>
</ul>

<dl>
<dt>
Lemma
</dt>
<dd>
<p>
If <span class="math">\(a^{N - 1} \not\equiv 1 \bmod N\)</span> for some <em>a</em> relatively prime to <em>N</em>, then it must hold for at least half the choices of <span class="math">\(a &amp;lt; N\)</span>.
</p>
</dd>
</dl>

<ul>
<li><p>
We are ignoring Charmichael numbers, so we now assert,
</p>

<ul>
<li>
If <em>N</em> is prime, then <span class="math">\(a^{N - 1} \equiv 1 \bmod N\)</span> for all <em>a</em> less than <em>N</em>.
</li>
<li>
If <em>N</em> is not prime, then <span class="math">\(a^{N - 1} \equiv 1 \bmod N\)</span> for at most half of the values of <span class="math">\(a &amp;lt; N\)</span>.
</li>
</ul></li>
<li><p>
The probability of our primality algorithm returning yes when a number is prime is 1, that is it happens 100% percent of the time.
</p>

<ul>
<li>
On the other hand, the probability of reutrning yes with a number is <em>not prime</em> is one half, that is, false positives happen half of the time.
</li>
<li>
If you run the test <em>k</em> times, the probability of <em>N</em> not being prime is <span class="math">\(\le \frac{1}{2^k}\)</span>.
</li>
</ul></li>
</ul>

<pre><code>function primarilty2(N) {
    pick positive integers a1, a2, ..., ak &lt; N at random;

    if ((a[i] ** N - 1) == 1 (mod N)) for all i ... k {
        return yes;
    else
        return no;
    }
</code></pre>

<ul>
<li>
<p>This probability of error drops exponentially fast, and can be driven <em>arbitrarily low</em> by choosing a large enough <em>k</em>.</p>
<ul>
<li>
If you do it a 100 times, the probability of a cosmic ray ruining your computation is higher than the probability of error.
</li>
</ul></li>
</ul>

<h5>
Generating random primes
</h5>

<ul>
<li><p>
We are now close to having everything we need for cryptography.
</p>

<ul>
<li><p>
We need a fast algorithm for choosing random primes.
</p>

<ul>
<li>
Hundred bits long.
</li>
</ul></li>
<li><p>
What makes this aks quite easy is that prime are abundant
</p>

<ul>
<li>
A random, <em>n</em>-bit number has a roughly one-on-<em>n</em> chance of being prime.
</li>
<li>
For instance, about 1 in 20 social security numbers are prime!
</li>
</ul></li>
</ul></li>
</ul>

<dl>
<dt>
Lagrange's prime number theorem
</dt>
<dd>
<p>
Let <span class="math">\(\pi(x)\)</span> be the number of primes <span class="math">\(\le x\)</span>. Then, <span class="math">\(\pi(x) \approx \frac{x}{\ln n}\)</span>, or more precisely,
</p>

<p>
<span class="math">\[\lim_{x \to \infty} \frac{\pi(x)}{(x / \ln x)} = 1\]</span> Such abundance makes it simple to generate a random <em>n</em>-bit prime:
</p>

<ul>
<li>
Pick a random <em>n</em>-bit number <em>N</em>.
</li>
<li>
Run a primality test on <em>N</em>.
</li>
<li>
If it passes the test, output <em>N</em>, else repeat the test.
</li>
</ul>
</dd>
</dl>

<ul>
<li><p>
<em>How fast is this algoritm?</em>
</p>

<ul>
<li>
If the randomly chosen <em>N</em> is truly prime, which happen with probability at least <span class="math">\(\frac{1}{n}\)</span>, then it will certainly pass the test.
</li>
<li>
So on each iteration, this procedure has at least a <span class="math">\(\frac{1}{n}\)</span> chance of halting.
</li>
<li>
Therefore, on average it will halt within <span class="math">\(O(n)\)</span> rounds.
</li>
</ul></li>
<li><p>
Next, exactly which primality test should be used?
</p>

<ul>
<li>
<p>In this application, since the number we are testing for primality are chosen at random rather than by adversary, it is sufficient to perform the Fermat test with <em>a</em> as 2.</p>
<ul>
<li>
For random numbers the Fermat test has a mihc samller failur probability than the worst-case one-half bound proven earlier.
</li>
<li>
The resulting algorithm is quite fast, generating primes that are hundreds of bits long in a fraction of a second on a PC.
</li>
</ul></li>
</ul></li>
<li><p>
The important question: <em>What is the probability that the output of the algorithm is really prime?</em>
</p>

<ul>
<li>
To answer, we must first understnad how discerning the Fermat test is.
</li>
<li>
As a concrete example, perform the operation on base <em>a</em> equal to 2 for all numbers less than or equal to 25 times 10 to the 9th power.
</li>
<li><p>
In this range, there are about <span class="math">\(10^9\)</span> primes, and about 20,000 composites that pass the test.
</p>

<ul>
<li><p>
Thus, the chance of erronesouly outputting a composite numbers is
</p>

<p>
<span class="math">\[20,000 / 10^9 = 2 \times 10^{-5}\]</span>
</p></li>
</ul></li>
</ul></li>
</ul>

<h4>
Cryptography
</h4>

<ul>
<li><p>
The next topic is the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> cryptosystem.
</p>

<ul>
<li>
It derives very strong guarantees of security by ingeniously exploiting the wide gulf between the polynomial-time computability of certain number-theoretic tasks and the intractability of others.
</li>
</ul></li>
<li><p>
The typical setting for cryptography is described with a cast of three characters: Alice and Bob, who want to communicate in private, and Eve, the eavesdropper who will go to great length to find out what they're saying.
</p>

<ul>
<li>
<p>Imagine Alice sends her message &quot;<em>x</em>&quot; which is written in binary.</p>
<ul>
<li>
She encodes it as <em>e(x)</em>, sends it over, and then Bob applies his decryption function <em>d()</em> to decode it: <em>d(e(x)) = x</em>.
</li>
</ul></li>
</ul></li>
</ul>

<pre><code>Alice                                                Bob
            +----------+  e(x)   +----------+
  x  ----&gt;  | Encoder  |  ----&gt;  | Decoder  |  ----&gt; x = d(e(x))  
            +----------+    |    +----------+
                            |
                            +---&gt;  Eve
</code></pre>

<ul>
<li><p>
Alice and Bob are worried that the eavesdropper, Eve, will intercept <em>e(x)</em>.
</p>

<ul>
<li>
She might be a sniffer on the network.
</li>
<li>
<p>The function <em>e()</em> is chosen so that without knowing <em>d()</em>, Even cannot do anything with the information she has picked up.</p>
<ul>
<li>
<em>e(x)</em> tells you little or nothing about <em>x</em>.
</li>
</ul></li>
</ul></li>
<li><p>
For centuries, cryptography was based on <em>private-key protocols</em>.
</p>

<ul>
<li>
<p>In such a scheme, Alice and Bob would have to actually meet before hand and agree on the scheme.</p>
<ul>
<li>
Eve's only hope is to get the codebook and a message.
</li>
</ul></li>
</ul></li>
<li><p>
<em>Public-key protocols</em> such as <abbr title="Rivest-Shamir-Adelman">RSA</abbr> are significantly more subtle and tricky.
</p>

<ul>
<li>
They allow Alice to send a message to Bob without ever having to meet him.
</li>
<li><p>
The central idea behind <abbr title="Rivest-Shamir-Adelman">RSA</abbr> is the dramatic constrast between facting and primality.
</p>

<ul>
<li>
Bob is able to implement a <em>digital lock</em>, to which only he has the key.
</li>
</ul></li>
<li><p>
By making his digital lock public, he gives Alice a way to send him a secure message that only he can decrypt.
</p>

<ul>
<li>
This is how the Internet sends private and sensitive information.
</li>
</ul></li>
</ul></li>
<li><p>
In the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol, Bob need only perform the simplest of calculations, such as multiplcation, for a digital log.
</p>

<ul>
<li>
By contrast, Eve must perform operations like factoring large numbers that would require more than the world's most powerful computers combined.
</li>
</ul></li>
</ul>

<h5>
<abbr title="Rivest-Shamir-Adelman">RSA</abbr>
</h5>

<ul>
<li><p>
<abbr title="Rivest-Shamir-Adelman">RSA</abbr> is <em>public-key cryptography</em>.
</p>

<ul>
<li>
Anybody can send a message to anybody else using publically available information.
</li>
<li>
Each person has a public key known to the whole world and a secret key known only to themselves.
</li>
<li>
<p>When Alice wants to send a message to Bob, this happens:</p>
<ol>
<li>
She encodes it using his public key.
</li>
<li>
He decrypts it with his secret key.
</li>
</ol></li>
</ul></li>
<li><p>
The <abbr title="Rivest-Shamir-Adelman">RSA</abbr> scheme is based heavily on number theory
</p>

<ul>
<li><p>
Think of messages from Alice to Bob as numbers modulo <em>N</em>.
</p>

<ul>
<li>
Messages larger than <em>N</em> are broken into smaller peices.
</li>
</ul></li>
<li><p>
The encryption function will the be a bijection on <span class="math">\(\lbrace 0, 1, \cdots N - 1\rbrace\)</span>
</p>

<ul>
<li>
The decryption function will be its inverse.
</li>
</ul></li>
</ul></li>
</ul>

<dl>
<dt>
Property
</dt>
<dd>
<p>
Pick any two primes <em>p</em> and <em>q</em> and let <em>N = pq</em>. For any <em>e</em> relatively prime to <em>(p - 1)(q - 1)</em>:
</p>

<ol>
<li><p>
The mapping <span class="math">\(x \mapsto x^e \bmod N\)</span> is a bijection on
</p>

<p>
<span class="math">\[\lbrace 0, 1, \cdots , N - 1 \rbrace\]</span>
</p></li>
<li><p>
Moreover, the inverse mapping is easily realized: led <em>d</em> be the inverse of <em>e</em> modulo <em>(p - 1)(q - 1)</em>. Then for <span class="math">\(x \in \lbrace 0, 1, \cdots N - 1 \rbrace\)</span>,
</p>

<p>
<span class="math">\[(x^e)^d \equiv x \bmod N\]</span>
</p>

<blockquote>
  <p>
<strong>Example</strong>
</p>
  
<ul>
  <li><p>
<strong>Bob chooses his public and secret keys</strong>
</p>
  
<ul>
  <li>
He starts by picking two large (<em>n</em>-bit) random primes <em>p</em> and <em>q</em>.
</li>
  <li><p>
His public key is <span class="math">\((N, e)\)</span> where <span class="math">\(N = pq\)</span> and <span class="math">\(e\)</span> is a <span class="math">\(2n\)</span>-bit number relatively prime to <span class="math">\((p - 1)(q - 1)\)</span>.
</p>
  
<ul>
  <li>
A common choise is <span class="math">\(e = 3\)</span> because it permits fast encoding.
</li>
  </ul></li>
  <li><p>
His secret key is <span class="math">\(d\)</span>, the inverse of <span class="math">\(e\)</span> modulo <span class="math">\((p - 1)(q - 1)\)</span>, computed using the extended Euclid algorithm.
</p></li>
  </ul></li>
  <li><p>
<strong>Alice wishes to send message <span class="math">\(x\)</span> to Bob</strong>.
</p>
  
<ul>
  <li>
She looks up his public key <span class="math">\((N, e)\)</span> and sends him <span class="math">\(y = (x^e \bmod N)\)</span>, computed using an efficient modular exponentiation algorith,
</li>
  <li>
He decodes the message by computing <span class="math">\(y^d \bmod N\)</span>.
</li>
  </ul></li>
  </ul>
</blockquote></li>
</ol>
</dd>
</dl>

<ul>
<li><p>
This <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol is convinient.
</p>

<ul>
<li>
The computations for Alice and Bob are easy.
</li>
<li><p>
But what about Eve?
</p>

<ul>
<li><p>
The security of <abbr title="Rivest-Shamir-Adelman">RSA</abbr> hinges upon a simple assumption:
</p>

<blockquote>
  <p>
Given <span class="math">\(N\)</span>, <span class="math">\(e\)</span>, and <span class="math">\(y = x^e \bmod N\)</span>, it is computationally intractable to determine <span class="math">\(x\)</span>.
</p>
</blockquote></li>
</ul></li>
</ul></li>
<li><p>
This assumption is quite plausible
</p>

<ul>
<li><p>
She coould try all possible values of <span class="math">\(x\)</span>, each time checking <span class="math">\(x^e \equiv y \bmod N\)</span>
</p>

<ul>
<li>
This would take exponetial time.
</li>
</ul></li>
<li><p>
She could try to factor <span class="math">\(N\)</span> to retrieve <span class="math">\(p\)</span> and <span class="math">\(q\)</span>, and then figure our <span class="math">\(d\)</span> by inverting <span class="math">\(e\)</span> modulo <span class="math">\((p - 1)(q - 1)\)</span>.
</p>

<ul>
<li>
This factoring would be hard.
</li>
</ul></li>
<li><p>
Intractibility is normally a source of dismay, <abbr title="Rivest-Shamir-Adelman">RSA</abbr> uses it as an advantage.
</p></li>
</ul></li>
</ul>

<h2>
February 19th, 2014 <small>Recitation</small>
</h2>

<h3>
Classification
</h3>

<ul>
<li><p>
Sorting algorithms are often classified by:
</p>

<ul>
<li><dl>
<dt>
Computational complexity
</dt>
<dd>
<p>
Worst, average, and best best-behavior of element comparisions in terms of the size of the list.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Memory usage
</dt>
<dd>
<p>
Some sorting algorithms are &quot;in place&quot; and they do not need any memory input beyond the input array, which stores the elemts to be sorted.
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Stability
</dt>
<dd>
<p>
Stable sorting algorithms maintain the same order of records with equal keys in the input array (values)
</p>
</dd>
</dl></li>
<li><dl>
<dt>
Whether or not they are a comparisons sort
</dt>
<dd>
<p>
A comparision sort examines the data only by comparing two elements with a comparision operator.
</p>
</dd>
</dl></li>
</ul></li>
</ul>

<h4>
Stability
</h4>

<ul>
<li><p>
If all keys are different than the stability distinction is not necessary.
</p>

<ul>
<li>
But if there are equal keys, the a sorting algorithm is stable if whenever there are two records.
</li>
</ul></li>
<li><p>
Unstable sorting algorithms may change the relative order of records with equal keys, but stable sorting algorithms never do so.
</p>

<ul>
<li>
<p>Unstable sorting algorithms can be specially implemented to be stable.</p>
<ul>
<li>
Additional computational cost.
</li>
</ul></li>
</ul></li>
</ul>

<table>
<thead>
<tr>
  <th>
Algorithm
</th>
  <th>
Best time
</th>
  <th>
Average time
</th>
  <th>
Worst time
</th>
  <th>
Memory
</th>
  <th>
Stable
</th>
  <th>
Method
</th>
  <th>
Notes
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
Bubble sort
</td>
  <td>
<span class="math">\(n\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(1\)</span>
</td>
  <td>
Yes
</td>
  <td>
Swapping elements
</td>
  <td>
Simple implementation
</td>
</tr>
<tr>
  <td>
Selection sort
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(1\)</span>
</td>
  <td>
No
</td>
  <td>
Selection
</td>
  <td>
Stable with <span class="math">\(O(n)\)</span>
</td>
</tr>
<tr>
  <td>
Insertion sort
</td>
  <td>
<span class="math">\(n\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(n^2\)</span>
</td>
  <td>
<span class="math">\(1\)</span>
</td>
  <td>
Yes
</td>
  <td>
Insertion
</td>
  <td>
<span class="math">\(O(n + d)\)</span> where <span class="math">\(d\)</span> inversions
</td>
</tr>
<tr>
  <td>
Heapsort
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(1\)</span>
</td>
  <td>
No
</td>
  <td>
Selection
</td>
  <td></td>
</tr>
<tr>
  <td>
Merge sort
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
Worst is <span class="math">\(n\)</span>
</td>
  <td>
Yes
</td>
  <td>
Merging
</td>
  <td>
Highly parralelizable
</td>
</tr>
<tr>
  <td>
In-place Merge sort
</td>
  <td></td>
  <td></td>
  <td>
<span class="math">\(n (\log n)^2\)</span>
</td>
  <td>
<span class="math">\(1\)</span>
</td>
  <td>
Yes
</td>
  <td>
Merging
</td>
  <td>
Implemented in STL
</td>
</tr>
<tr>
  <td>
Quicksort
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(n \log n\)</span>
</td>
  <td>
<span class="math">\(\log n\)</span>
</td>
  <td>
<span class="math">\(\log n\)</span>
</td>
  <td>
Depends
</td>
  <td>
Partitioning
</td>
  <td>
Usually done in place
</td>
</tr>
</tbody>
</table>

<h2>
February 21st, 2014 <small>Lecture</small>
</h2>

<h3>
Linear Time Sorting Algorithms
</h3>

<ul>
<li><p>
Counting sort
</p>

<ul>
<li><p>
N input elements that are integers in a range from 0 to <em>k</em>.
</p>

<pre><code>     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
A  | 2 | 5 | 3 | 0 | 2 | 3 | 0 | 3 |
   +---+---+---+---+---+---+---+---+

     0   1   2   3   4   5
   +---+---+---+---+---+---+
C  | 2 | 0 | 2 | 3 | 0 | 1 |
   +---+---+---+---+---+---+
// C[1] says how many elements in array 
// A have value i


C` +---+---+---+---+---+---+
   | 1 | 2 | 2 | 4 | 7 | 7 |
   +---+---+---+---+---+---+
// C`[1] specifies # of elements less than or
// greater to i in array A

   +---+---+---+---+---+---+---+---+
O  | 0 | 0 | 2 | 2 | 3 | 3 | 3 | 5 |
   +---+---+---+---+---+---+---+---+
</code></pre></li>
</ul></li>
</ul>

<h4>
Running time
</h4>

<ul>
<li>
Initialize empty array <span class="math">\(G \Omega(k)\)</span>
</li>
<li>
Count the occurences of elements in array <span class="math">\(A : \Omega(n)\)</span>
</li>
<li>
Identify the sport of each mumber in the output array.
</li>
</ul>

<h3>
Radix Sort
</h3>

<ul>
<li><p>
What is the idea?
</p>

<ul>
<li>
We'll assume a fixed number of bits.
</li>
<li>
<p>Let's say we have <span class="math">\(d\)</span>-bit numbers.</p>
<ul>
<li>
The idea is check one bit at a time and sort them from the right-most bit to the left-most bit.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>Lemma</strong>: For <em>n</em> <em>d</em>-digit numbers where each digit can take up to <em>k</em> values, radix sort correctly sorts these numbers in <span class="math">\(O(d(n + k))\)</span> time assuming you use a stable sort for sorting per digit.
</p></li>
<li><p>
<strong>Base case</strong>: Sort accorind to the least significant digit.
</p></li>
<li>
<strong>Assumption</strong>: Sorted up to (i - 1)th digit.
</li>
<li><p>
<strong>Step</strong>: Every number that has a lower (1 + i)th digit than any other number will appear earlier.
</p></li>
<li><p>
If they have the same ith giti, these numbers already appear in the correct order in the array.
</p>

<ul>
<li>
If I use a stable sort, the correct order is retained.
</li>
<li>
Cost of sorting per digit up to range k: <span class="math">\(\Theta(n + k)\)</span>
</li>
<li>
How many sorts? <em>d</em> as number of digits.
</li>
</ul></li>
</ul>

<h3>
Bucket Sort
</h3>

<ul>
<li>
Asumme that the input is drawn from a uniform distribution.
</li>
<li><p>
wlog assume input is in a range.
</p></li>
<li><p>
<strong>Idea</strong>: Divide (0, 1) into n buckets and distribute the numbers into these buckets.
</p>

<ul>
<li>
Sort the number in each bucket and combine the results.
</li>
<li>
How to sort each bucket?
</li>
</ul></li>
</ul>

<h4>
Running time
</h4>

<p>
<span class="math">\[T(n) = \Theta(n) + \sum_{i = 0}^{n  - 1} O(O_i^2)\]</span>
</p>

<ul>
<li>
The theta is the cost of generating the bueckets and assigning elements to buckets.
</li>
</ul>

<h2>
February 25th, 2014 <small>Practice Questions and Reading Material</small>
</h2>

<h3>
Introduction to Concepts of Algorithmic Design, Computing Running Times
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapters 0.1, 0.2, 0.3 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
<li>
Chapters 1.2, 3.1, 3.2 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>You may be provided with an algorithm for computing the <span class="math">\(n\)</span>th Fibonacci number and then be asked to compute its running time (think in terms of it complexity). Alternatively, you may be asked to provide an efficient algorithm for computing Fibonacci numbers.</strong>
</p>

<pre><code>function fib1(n) 
if n = 0: return 0
if n = 1: return 1
return fib1(n - 1) + fib1(n - 2)
</code></pre>

<pre><code>(n) function fib2
if n=0 return 0
create an array f[0 . . . n] f[0] =0,f[1] =1
for i = 2 . . . n:
    f[i] = f[i − 1] + f[i − 2] 
return f[n]
</code></pre></li>
<li><p>
<strong>Give a justification why imporovement in hardware are typically not sufficient to make an algorithm with exponential running time reasonably tractable.</strong>
</p>

<blockquote>
  <p>
But technology is rapidly improving—computer speeds have been doubling roughly every 18 months, a phenomenon sometimes called Moore’s law. With this extraordinary growth, perhaps fib1 will run a lot faster on next year’s machines. Let’s see—the running time of fib1(n) is proportional to <span class="math">\(2^{0.694n} \approx (1.6)^n\)</span>, so it takes 1.6 times longer to compute <span class="math">\(F_{n+1} than F_n\)</span>. And under Moore’s law, computers get roughly 1.6 times faster each year. So if we can reasonably compute <span class="math">\(F_{100}\)</span> with this year’s technology, then next year we will manage <span class="math">\(F_{101}\)</span>. And the year after, <span class="math">\(F_{102}\)</span>. And so on: just one more Fibonacci number every year! Such is the curse of exponential time.
</p>
</blockquote></li>
<li><p>
<strong>We have 3 algorithms for the same problem where the first runs in exponential time, the second in logarithmic, and the thir in linear time as a function of the same input. Which algorith do you prefer to use?</strong>
</p>

<table>
<thead>
<tr>
  <th>
Name
</th>
  <th>
Running time
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
Exponential
</td>
  <td>
<span class="math">\(O(a^n)\)</span>
</td>
</tr>
<tr>
  <td>
Logarithmic
</td>
  <td>
<span class="math">\((\log(n))\)</span>
</td>
</tr>
<tr>
  <td>
<strong>Linear</strong>
</td>
  <td>
<span class="math">\(O(n)\)</span>
</td>
</tr>
</tbody>
</table></li>
<li><p>
<strong>You may be provided running times of different algorithms and asked to show which running time dominates asymptotically.</strong>
</p></li>
<li><p>
<strong>Provide the definition of <em>O</em>-notation (or Theta or Omega).</strong>
</p>

<blockquote>
  <p>
Let <span class="math">\(f(n)\)</span> and <span class="math">\(g(n)\)</span> be functions from positive integers to positive reals. We say f = O(g) (which means that “<span class="math">\(f\)</span> grows no faster than <span class="math">\(g\)</span>”) if there is a constant <span class="math">\(c &gt; 0\)</span> such that <span class="math">\(f(n) \le c \times g(n)\)</span>.
</p>
</blockquote>

<ul>
<li>
<p>Just as <span class="math">\(O(\cdot)\)</span> is an analog of <span class="math">\(\le\)</span>, you can define the other analyses as such:</p>
<ul>
<li>
<span class="math">\(f = \Omega(g)\)</span> means <span class="math">\(g = O(f)\)</span>.
</li>
<li>
<span class="math">\(f = \Theta(g)\)</span> means <span class="math">\(f = O(g)\)</span> and <span class="math">\(f = \Omega(g)\)</span>.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>You may be provided certain statements about asymptotic analysis of running times and asked to show if they are correct or not. For instance, &quot;n to the a dominates n to the b if a is greater than b.</strong>
</p></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 0.1, 0.2, 0.3
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 3.1-1, 3.1-2, 3.1-3, 3.1-4, 3.2-4
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 1.1, 3-2, 3-3, 3-6
</li>
</ul>

<h3>
Number Theoretic Algorithms <small>Complexity of adding and multiplying 2 n-bit numbers, modulo arithmetic</small>
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapters 1.1,1.2 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
</ul>

<h5>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr> 31.2 <small>Greatest common divisor</small>
</h5>

<ul>
<li><p>
In this section, Euclid's algorithm for computing the <abbr title="Greatest common divisor">GCD</abbr> of two integers efficiently.
</p>

<ul>
<li>
<p>Suprising connection with Fibonacci numbers.</p>
<ul>
<li>
This yields the worst case.
</li>
</ul></li>
</ul></li>
<li><p>
Restricted to nonegative integers. <span class="math">\[\gcd(a, b) = \gcd(|a|, |b|)\]</span>
</p></li>
<li><p>
One way of characterizing the problem:
</p>

<p>
<span class="math">\[a = p_{1}^{e_1}p_{2}^{e_2} \cdots p_{r}^{e_r}\]</span> <span class="math">\[b = p_{1}^{f_1}p_{2}^{f_2} \cdots p_{r}^{f_r}\]</span> <span class="math">\[\gcd(a, b) = p_{1}^{\min(e_1, f_1)} p_{2}^{\min(e_2, f_2)} \cdots p_{r}^{\min(e_r, f_r)}\]</span>
</p></li>
<li><p>
Proof
</p>

<ul>
<li><p>
We first show that the <abbr title="Greatest common divisor">GCD</abbr> of <span class="math">\(a\)</span> and <span class="math">\(b\)</span> divide each other.
</p>

<ul>
<li>
If <span class="math">\(d = \gcd(a, b)\)</span>, then <span class="math">\(d | a\)</span> and <span class="math">\(d | b\)</span>.
</li>
</ul></li>
<li><p>
Because <span class="math">\(a \bmod b\)</span> is a linear combination of <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, <span class="math">\(d | (a \bmod b)\)</span>.
</p></li>
<li>
Because <span class="math">\(d | b\)</span> and <span class="math">\(d | (a \bmod b)\)</span>, <span class="math">\(d | \gcd(b, a \bmod b)\)</span>
</li>
<li><p>
Equivilently,
</p>

<p>
<span class="math">\[\gcd(a, b) | \gcd(b, a \bmod b)\]</span>
</p></li>
<li><p>
You can show that the reversed is true to, and because if you can invert the divisible operator, the two things are plus or minus equal
</p></li>
</ul></li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>What is the &quot;primality&quot; problem and what is the &quot;factoring&quot; problem? Which one of the two is tractable? What are the advantages of the intractability of the other problem?</strong>
</p>

<dl>
<dt>
Factoring problem
</dt>
<dd>
<p>
Given a number <span class="math">\(N\)</span>, express it as a product of prime factors.
</p>
</dd>

<dd>
<p>
Computationally intractable, fastest is exponential. This makes <abbr title="Rivest-Shamir-Adelman">RSA</abbr> work because keys and factoring large multiples.
</p>
</dd>

<dt>
Primality problem
</dt>
<dd>
<p>
Given a number <span class="math">\(N\)</span>, determine whether it is a prime.
</p>
</dd>

<dd>
<p>
Computationally tractable.
</p>
</dd>
</dl></li>
<li><p>
<strong>How many digits do you need to represent a number <em>N</em> in base <em>b</em>?</strong>
</p>

<ul>
<li><p>
With <span class="math">\(k\)</span> digits in base <span class="math">\(b\)</span> we can express numbers up to <span class="math">\(b^k - 1\)</span>.
</p>

<ul>
<li>
For instance, in decimal, three digits get us all the way up to <span class="math">\(999 = 10^3 - 1\)</span>.
</li>
</ul></li>
<li><p>
By solving for <span class="math">\(k\)</span>, we find that <span class="math">\(\lceil \log_b(N + 1) \rceil\)</span> are need to write <span class="math">\(N\)</span> in base <span class="math">\(b\)</span>.
</p></li>
</ul></li>
<li><p>
<strong>How much does the size of the representation of a number changes when we change bases?</strong>
</p>

<ul>
<li><p>
Recall the rule for converting logarithms from base <span class="math">\(a\)</span> to base <span class="math">\(b\)</span>:
</p>

<p>
<span class="math">\[\log_b N = (log_a N) / (log_a b)\]</span>
</p>

<ul>
<li>
So the size of integer <span class="math">\(N\)</span> in base <span class="math">\(a\)</span> is the same as its size in base <span class="math">\(b\)</span>, times a constant factor <span class="math">\(log_a b\)</span>.
</li>
<li><p>
In big-O, the base is irrelevant, and we write the size simply as
</p>

<p>
<span class="math">\[O(\log N)\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
<strong>What is the running time of the addition operation of a number for two <em>n</em>-bit numbers (think in terms of bit complexity)?</strong>
</p>

<ul>
<li>
Suppose <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are <span class="math">\(n\)</span> bits long.
</li>
<li>
The sume of <span class="math">\(x\)</span> and <span class="math">\(y\)</span> is <em>at most</em> <span class="math">\(n + 1\)</span> bits long.
</li>
<li>
Each bit of the sume is computed in a fixed amount of time.
</li>
<li><p>
The total running time for addition will be
</p>

<p>
<span class="math">\[c_0 + c_1 n\]</span>
</p>

<p>
In other words, <em>linear</em>.
</p>

<p>
<span class="math">\[O(n)\]</span>
</p></li>
</ul></li>
<li><p>
<strong>What is the running time of the tradition multiplication operation taught in grade school for two <em>n</em>-bit numbers (think in terms of <em>n</em>-bit complexit)?</strong>
</p>

<ul>
<li>
To compute each row, either &quot;X&quot; or &quot;0&quot;, left-shifted
</li>
<li>
The rows are in the order of <span class="math">\(2n\)</span>.
</li>
<li>
You have to sum them up, and you do this pairwise.
</li>
<li>
If you do <span class="math">\(n\)</span> times an operation which costs <span class="math">\(n\)</span>, your running time is going to be <span class="math">\(n^2\)</span>.
</li>
</ul></li>
<li><p>
<strong>Provide a recursive formula for the multiplication of two numbers.</strong>
</p>

<pre><code>function multiplication(x, y) {
    if (y == 1) {
        return x;
    }

    else {
        return x + multiplication(x, y - 1);
    }
}
</code></pre></li>
<li><p>
<strong>What is the complexity of modular addition and modular multiplication for two <em>n</em> bit numbers?</strong>
</p>

<ul>
<li>
<strong>Addition</strong>: <span class="math">\(O(n)\)</span>
</li>
<li>
<strong>Multiplication</strong>: <span class="math">\(O(n^2)\)</span>
</li>
</ul></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 1.1, 1.2, 1.3, 1.4, 1.6, 1.7, 1.8, 1.9, 1.10, 1.31
</li>
</ul>

<h3>
<abbr title="Rivest-Shamir-Adelman">RSA</abbr> Cryptosystem, Fermat’s Little Theorem and Modular Exponentiation
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapters 1.2,1.4 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
<li>
Chapter 31.3, 31.6, 31.7 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>Give a private key protocol for a cryptography application. Given an example it can be compromised.</strong>
</p>

<ul>
<li>
There is the classic &quot;codebook&quot; private-key scheme, where Alice and Bob meet before hand and agree on a secret code.
</li>
<li><p>
The way that this is compromised is if Eve gets the codebook or knows a given message and backtraces the code from it.
</p></li>
<li><p>
Alternatively, the answer you're looking for might be that you can encode and decode with public keys like:
</p>

<p>
<span class="math">\[x = d(e(x))\]</span>
</p>

<p>
Where <span class="math">\(x\)</span> is a message, <span class="math">\(d(\cdot)\)</span> is a decoder, and <span class="math">\(e(\cdot)\)</span> is an encoder. Bob can
</p></li>
<li>
&quot;Network sniffer&quot;
</li>
</ul></li>
<li><p>
<strong><abbr title="Rivest-Shamir-Adelman">RSA</abbr> is based on which basic property of modulo arithmetic?</strong>
</p>

<ul>
<li><p>
The basic feature of modulo arithmetic that <abbr title="Rivest-Shamir-Adelman">RSA</abbr> exploits is the dramatic contrast in the tractibility of factoring and primality testing.
</p>

<ul>
<li>
Bob and Alice only need to make simple calculations.
</li>
<li>
Eve needs to make computations that would be struggle for the world's largest computers.
</li>
</ul></li>
<li><p>
Pick any two prime <span class="math">\(p\)</span> and <span class="math">\(q\)</span> and let <span class="math">\(N = pq\)</span>. For any <span class="math">\(e\)</span> relatively prime to <span class="math">\((p - 1)(q - 1)\)</span>:
</p>

<p>
<span class="math">\[(x^e)^d \equiv x \mod N\]</span>
</p></li>
</ul></li>
<li><p>
<strong>Describe the steps of the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol. The security of the protocol is based</strong> on which assumption?
</p>

<blockquote>
  <p>
Given <span class="math">\(N\)</span>, <span class="math">\(e\)</span>, and <span class="math">\(y = x^e \mod N\)</span>, it is computationally intractable to determine <span class="math">\(x\)</span>.
</p>
</blockquote></li>
<li><p>
<strong>What are the basic operations that need to be performed accords to the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol and what is their running time?</strong>
</p>

<ul>
<li>
Pick two large primes
</li>
<li>
Multiply two large primes
</li>
<li>
Extended Euclid algorithm
</li>
<li>
Efficient modular exponentian algorithm
</li>
<li>
<span class="math">\(y^d \mod N\)</span>
</li>
</ul></li>
<li><p>
<strong>You may be provided an example message and asked to described the operations of the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol on it.</strong>
</p></li>
<li><p>
<strong>What does Fermat's Little Theorem specify? Prove it.</strong>
</p>

<ul>
<li>
<strong>Fermat's Little Theorem</strong>: If <span class="math">\(p\)</span> is a prime number, then for any integer <span class="math">\(a\)</span>, the number <span class="math">\(a^p - a\)</span> is an integer multiple of <span class="math">\(p\)</span>.
</li>
<li><p>
<strong>Proof</strong>
</p>

<ul>
<li><p>
Let us assume that <span class="math">\(a\)</span> is positive and not divisible by <span class="math">\(p\)</span>. The idea is that if we write down the sequence of numbers
</p>

<p>
<span class="math">\[a 2a, 3a, \cdots, (p - 1)a\]</span>
</p>

<p>
and reduce each one modulo <span class="math">\(p\)</span>, the result sequences turns out to be an arrangment of
</p>

<p>
<span class="math">\[1, 2, 3, \cdots, p - 1\]</span>
</p>

<p>
Therefore, if we multiple together the numbers in each sequences, the results must be indetifical modulo <span class="math">\(p\)</span>:
</p>

<p>
<span class="math">\[a \times 2a \times 3a \times \cdots \times (p - 1)\]</span>
</p>

<p>
Which is equivilent to
</p>

<p>
<span class="math">\[1 \times 2 \times 3 \times \cdots \times (p - 1)\]</span>
</p>

<p>
Collecting the <span class="math">\(a\)</span> terms yields
</p>

<p>
<span class="math">\[a^{p - 1} (p - 1)! \equiv (p - 1)! (\mod p)\]</span>
</p>

<p>
Finally, we may &quot;cancel out&quot; the numbers <span class="math">\(1, 2, \cdots, p - 1\)</span> from both sides, obtains,
</p>

<p>
<span class="math">\[a^{p -1} \equiv 1 (\mod p)\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
<strong>What is the importance of Fermat's little theorem in the <abbr title="Rivest-Shamir-Adelman">RSA</abbr> protocol?</strong>
</p>

<ul>
<li><p>
It's used to prove it.
</p>

<p>
The proof of the correctness of <abbr title="Rivest-Shamir-Adelman">RSA</abbr> is based on Fermat's little theorem. This theorem states that if <span class="math">\(p\)</span> is prime and <span class="math">\(p\)</span> does not divide an integer <span class="math">\(a\)</span> then
</p>

<p>
<span class="math">\[a^{p - 1} \equiv 1 \pmod{p}\]</span>
</p>

<p>
We want to show that <span class="math">\((m^e)^d \equiv m (\mod pq)\)</span> for every integer <em>m</em> when <em>p</em> and <em>q</em> are distinct prime numbers and <em>e</em> and <em>d</em> are positive integers satisfying
</p>

<p>
<span class="math">\[e d \equiv 1 \pmod{(p - 1)(q - 1)}\]</span>
</p>

<p>
We can write
</p>

<p>
<span class="math">\[ed - 1 = h(p - 1)(q - 1)\]</span>
</p>

<p>
for some nonnegative integer <em>h</em>.
</p>

<p>
To check two numbers, like <span class="math">\(m^{ed}\)</span> and <em>m</em>, are congruent mod <em>pq</em> it suffices (and in fact is equivalent) to check they are congruent mod <em>p</em> and mod <em>q</em> separately. (This is part of the <a href="/wiki/Chinese_remainder_theorem" title="Chinese remainder theorem">Chinese remainder theorem</a>, although it is not the significant part of that theorem.) To show <span class="math">\(m^{ed} \equiv m (\bmod p)\)</span>, we consider two cases: <span class="math">\(m ≡ 0 (mod p)\)</span>
</p>

<p>
In the first case <em>m<sup>ed</sup></em> is a multiple of <em>p</em>, so <em>m<sup>ed</sup></em> ≡ 0 ≡ <em>m</em> (mod <em>p</em>). In the second case
</p>

<p>
<span class="math">\[m^{e d} = m^{(ed - 1)}m = m^{h(p - 1)(q - 1)}m = \left(m^{p - 1}\right)^{h(q - 1)}m \equiv 1^{h(q - 1)}m \equiv m\pmod{p}\]</span>
</p>

<p>
where we used <a href="/wiki/Fermat%27s_little_theorem" title="Fermat's little theorem">Fermat's little theorem</a> to replace <em>m</em><sup><em>p</em>−1</sup> mod <em>p</em> with 1.
</p>

<p>
The verification that <em>m<sup>ed</sup></em> ≡ <em>m</em> (mod <em>q</em>) proceeds in a similar way, treating separately the cases <em>m</em> ≡ 0 (mod <em>q</em>) and <em>m</em> 0 (mod <em>q</em>), using Fermat's little theorem for modulus <em>q</em> in the second case.
</p>

<p>
This completes the proof that, for any integer <em>m</em>,
</p>

<p>
<span class="math">\[\left(m^e\right)^d \equiv m \pmod{pq}\]</span>
</p></li>
</ul></li>
<li><p>
<strong>How can we efficiently perform modular exponentiantion? What is the running time of the approach?</strong>
</p>

<ul>
<li>
Recursion, repeated squaring.
</li>
<li>
Let <span class="math">\(n\)</span> be the size of bits <span class="math">\(x\)</span>, <span class="math">\(y\)</span>, and <span class="math">\(N\)</span> (whichever is largest).
</li>
<li>
Like multiplication, there are <em>at most</em> <span class="math">\(n\)</span> recurisve calls.
</li>
<li>
During each call, it multiples <span class="math">\(n\)</span>-bit numbers.
</li>
<li>
Doing computation modulo <span class="math">\(N\)</span> saves us here.
</li>
<li>
Running time of <span class="math">\(O(n^3)\)</span>
</li>
</ul></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 1.17, 1.27, 1.28, 1.35, 1.39, 1.42, 1.43, 1.44
</li>
</ul>

<h3>
Greatest Common Divisor algorithms: Euclid’s test, Modulo Multiplicative Inverse
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapters 1.2 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
<li>
Chapter 31.2 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>What does Euclid’s rule specify? Prove it.</strong>
</p>

<ul>
<li>
<strong>Euclid's rule</strong>: If <span class="math">\(x\)</span> and <span class="math">\(y\)</span> are positive integers where <span class="math">\(x \le y\)</span>, then <span class="math">\(\gcd(x, y) = \gcd(x \bmod y, y)\)</span>
</li>
<li><p>
<strong>Proof</strong>
</p>

<ul>
<li>
Suppose <span class="math">\(a, b \in \mathbb{Z}\)</span> and <span class="math">\(a \lor B \neq 0\)</span>
</li>
<li>
From the *Division Theorem<span class="math">\(, \)</span>a = qb + r$ where <span class="math">\(0 \le r &amp;lt; |b|\)</span>.
</li>
<li>
From <em><abbr title="Greatest common divisor">GCD</abbr> with Remainder</em>, the <abbr title="Greatest common divisor">GCD</abbr> of <span class="math">\(a\)</span> and <span class="math">\(b\)</span> is also the <abbr title="Greatest common divisor">GCD</abbr> of <span class="math">\(b\)</span> and <span class="math">\(r\)</span>.
</li>
<li>
Therefore, we may search instead for <span class="math">\(\gcd(b, r)\)</span>.
</li>
<li>
Since <span class="math">\(|r| &amp;lt; |b|\)</span> and <span class="math">\(b \in \mathbb{Z}\)</span>, we will reach <span class="math">\(r = 0\)</span> after finitely many steps.
</li>
<li>
At this point, <span class="math">\(\gcd(r, 0) = r\)</span> from <em><abbr title="Greatest common divisor">GCD</abbr> with Zero</em>.
</li>
</ul></li>
<li><p>
<strong>Proof <em>from the book</em></strong>:
</p>

<ul>
<li><p>
It is enough to show the slightly simpler rule:
</p>

<p>
<span class="math">\[\gcd(x, y)= \gcd(x - y, y)\]</span>
</p>

<p>
from which the one stated can be derived by repeatedly subtracting <span class="math">\(y\)</span> from <span class="math">\(x\)</span>.
</p></li>
<li><p>
Any integer that divides both <span class="math">\(x\)</span> and <span class="math">\(y\)</span> must also divide <span class="math">\(x - y\)</span>, so
</p>

<p>
<span class="math">\[\gcd(x, y) \le \gcd(x - y, y)\]</span>
</p></li>
<li><p>
Likewise, any integer that divides both <span class="math">\(x - y\)</span> and <span class="math">\(y\)</span> must also divide both <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, so
</p>

<p>
<span class="math">\[\gcd(x, y) \ge gcd(x - y, y)\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
<strong>What is Euclid’s algorithm for finding the greatest common divisor? What is its running time and why?</strong>
</p>

<pre><code>function euclid(a, b) {
    if (b == 0) {
        return a;
    }

    else {
        return euclid(b, a % b);
    }
}
</code></pre>

<ul>
<li>
It is, &quot;If <span class="math">\(d\)</span> divides both <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, and <span class="math">\(d = ax + by\)</span> for some integers <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, then necessarily <span class="math">\(d = \gcd(a, b)\)</span>.&quot;
</li>
<li><p>
<strong>Run time</strong>
</p>

<ul>
<li>
Let <span class="math">\(T(a,b)\)</span> be the number of steps taken in the Euclidean algorithm.
</li>
<li>
Let <span class="math">\(h = \log_{10} b\)</span> be the number of digits in <span class="math">\(b\)</span>.
</li>
<li>
Assume that the modulo function is <span class="math">\(O(1)\)</span>.
</li>
<li><p>
The worst case is consecutive Fibonacci numbers.
</p>

<p>
<span class="math">\[a = F_{n + 1}, b = F_{n}\]</span>
</p></li>
<li><p>
The algorith will calcuate the following until <span class="math">\(n = 0\)</span>,
</p>

<p>
<span class="math">\[\gcd(F_{n + 1}, F_n) = \gcd(F_n, F_{n - 1})\]</span>
</p>

<p>
So,
</p>

<p>
<span class="math">\[T(F_{n + 1}, F_n) = \Theta(n)\]</span> <span class="math">\[T(a, F_n) = O(n)\]</span>
</p></li>
<li><p>
Since <span class="math">\(F_n = \Omega(\Phi^n)\)</span>, this implies that
</p>

<p>
<span class="math">\[T(a, b) = O(\log_{\Phi} b)\]</span>
</p></li>
<li><p>
Note that <span class="math">\(h \approx log_{10} b\)</span> and <span class="math">\(log_b x = \frac{\log x}{\log b}\)</span>, this implies <span class="math">\(\log_b x = O(\log x)\)</span>.
</p></li>
<li><p>
So the worst case for Euclid's algorith is
</p>

<p>
<span class="math">\[O(\log_{\Phi}b) = O(h) = \log(b)\]</span>
</p></li>
</ul></li>
</ul></li>
<li><p>
<strong>Show that if <span class="math">\(d\)</span> divides both <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, and <span class="math">\(d = a\times x+b \times y\)</span> for some integers <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, then <span class="math">\(d = \gcd(a, b)\)</span></strong>.
</p>

<ul>
<li>
By the first two condition, <span class="math">\(d\)</span> is a common divisor of <span class="math">\(a\)</span> and <span class="math">\(b\)</span> so it cannot exceed the <abbr title="Greatest common divisor">GCD</abbr>, that is, <span class="math">\(d \le \gcd(a, b)\)</span>.
</li>
<li>
On the other hand, <span class="math">\(\gcd(a, b)\)</span> is a common divisor of <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, it must also divide <span class="math">\(ax + by = d\)</span>, which implies that <span class="math">\(\gcd(a, b) \le d\)</span>.
</li>
<li>
Therefore, <span class="math">\(d = \gcd(a, b)\)</span>.
</li>
</ul></li>
<li><p>
<strong>What is the extended Euclid’s algorithm for finding the greatest common divisor d of two numbers a and b, as well as numbers <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, so that <span class="math">\(d = a\times x+b \times y\)</span>? Prove its correctness (you will need to prove Euclid’s algorithm first and then the extension).</strong>
</p>

<blockquote>
  <p>
<strong>EEA</strong>: If <span class="math">\(d\)</span> divides both <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, and <span class="math">\(d = ax + by\)</span> for some integers <span class="math">\(x\)</span> and <span class="math">\(y\)</span>, then <span class="math">\(d = \gcd(a, b)\)</span>
</p>
</blockquote>

<ul>
<li>
<p><em>Proof</em></p>
<ul>
<li>
By the first two condition, <span class="math">\(d\)</span> is a common divisor of <span class="math">\(a\)</span> and <span class="math">\(b\)</span> so it cannot exceed the <abbr title="Greatest common divisor">GCD</abbr>, that is, <span class="math">\(d \le \gcd(a, b)\)</span>.
</li>
<li>
On the other hand, <span class="math">\(\gcd(a, b)\)</span> is a common divisor of <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, it must also divide <span class="math">\(ax + by = d\)</span>, which implies that <span class="math">\(\gcd(a, b) \le d\)</span>.
</li>
<li>
Therefore, <span class="math">\(d = \gcd(a, b)\)</span>.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>When does the multiplicative inverse of a number <span class="math">\(x\)</span> exists modulo <span class="math">\(N\)</span> and why?</strong>
</p></li>
<li>
<strong>How can you compute the multiplicative inverse modulo <span class="math">\(N\)</span> for two relative prime numbers? What is the running time of the corresponding algorithm?</strong>
</li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 1.18, 1.19, 1.20, 1.21, 1.22, 1.23, 1.24, 1.33, 1.37
</li>
</ul>

<h3>
Primality Testing and Universal Hashing
</h3>

<h4>
Reading material
</h4>

<h5>
Chapter 1.3 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</h5>

<ul>
<li>
Is there a test to tell us whether a number is prime? We place our home in a theorom from 1640.
</li>
</ul>

<dl>
<dt>
Fermat's Little Theorem
</dt>
<dd>
<p>
If <span class="math">\(p\)</span> is prime, then <span class="math">\(\forall 1 \le a &amp;lt; p\)</span>, <span class="math">\[a^{p - 1} \equiv 1 (\bmod p)\]</span>
</p>
</dd>
</dl>

<h5>
Chapter 1.5 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</h5>

<ul>
<li>
Chapters 31.8, 11.2, 11.3 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>Describe a test for evaluating whether a number is prime. What is the probability of this test returning the correct answer and why? Can you increase the probability of success for this test?</strong>
</p>

<ul>
<li><p>
Fermat's little theorem states that if <span class="math">\(p\)</span> is prime and <span class="math">\(1 \le a &amp;lt; p\)</span>, then, <span class="math">\[a^{p - 1} \equiv 1 (\bmod p)\]</span>
</p></li>
<li><p>
If we want to test if <span class="math">\(p\)</span> is prime, then we can pick random <span class="math">\(a\)</span>'s in the interval and see if the equality holds.
</p>

<ul>
<li>
If the equlaity <em>doesn't</em> hold for a value, then <span class="math">\(p\)</span> is composite.
</li>
<li>
If the equality <em>does</em> hold for <em>many</em> values of <span class="math">\(a\)</span>, the <span class="math">\(p\)</span> is <em>probable prime</em>.
</li>
</ul></li>
<li><p>
Using fast algorithms for modular exponentiation, the running time of this algorith is
</p>

<p>
<span class="math">\[O(k \times \log^2 n \times \log \log n \times \log \log \log n)\]</span>
</p>

<p>
Where <span class="math">\(k\)</span> is the number of time we test a random <span class="math">\(a\)</span>, and <span class="math">\(n\)</span> is the value we want to test for primality.
</p></li>
<li><p>
You increase the probability with a larger <span class="math">\(k\)</span>.
</p></li>
</ul></li>
<li><p>
<strong>What does Lagrange's Prime Number Theorem specify and why is it helpful for primality testing?</strong>
</p>

<dl>
<dt>
Lagrange's prime number theorem
</dt>
<dd>
<p>
Let <span class="math">\(\pi(x)\)</span> be the number of prime <span class="math">\(\le x\)</span>. Then <span class="math">\(\pi(x) \equiv x / (\ln x)\)</span>, or more precisely,
</p>

<p>
<span class="math">\[\lim_{x \to \infty} \frac{\pi(x)}{(x / \ln x)} = 1\]</span>
</p>
</dd>
</dl>

<ul>
<li>
<p>Such abundance makes it simple to generate a random <span class="math">\(n\)</span>-bit prime:</p>
<ul>
<li>
Pcik a random <span class="math">\(n\)</span>-bit number <span class="math">\(N\)</span>.
</li>
<li>
Run a primality test on <span class="math">\(N\)</span>.
</li>
<li>
If it passes the test, output <span class="math">\(N\)</span>; else repeat the process.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>What properties should a good hash function provide?</strong>
</p>

<ul>
<li>
<p>Wikipedia</p>
<ul>
<li>
Determinism, same input same output
</li>
<li>
Uniformity, map the expected inputs as evenly as possible.
</li>
<li>
Variable range, can be expanded or contracted in size.
</li>
<li>
Continuity, two similar inputs should be mapped to nearly equal hashes
</li>
</ul></li>
<li>
<p><abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr></p>
<ul>
<li>
A good hash function satisifes the assumption of simple uniform hasing: each key is equally likely to hash to any of the <span class="math">\(m\)</span> slots, indepedantly of where any other key has hashed to.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>What is the property of universal hashing families of functions? Provide a universal hashing family of functions for an example problem and prove the corresponding property.</strong>
</p>

<ul>
<li>
Universal hashing is designed to stop the effectiveness of a attacker who knows the hash function and wants to put every input in the same bin.
</li>
<li>
To avoid this, at the beginning of the lifecycle of a hash table, a function is picked <em>at random</em>.
</li>
</ul>

<dl>
<dt>
Property
</dt>
<dd>
<p>
Consider any pair of distinct IP addresses <span class="math">\(x = (x_1, \cdots, x_4)\)</span> and <span class="math">\(y = (y_1, \cdots, y_4)\)</span>. If the coefficients <span class="math">\(a = (a_1, a_2, a_3, a_4)\)</span> are chosen uniformly at random from <span class="math">\(\lbrace 0, 1, \cdots, n - 1\)</span> then
</p>

<p>
<span class="math">\[\Pr \lbrace h_a (x_1, \cdots, x_2) = h_a (y_1, \cdots, y_4) \rbrace = \frac{1}{n}\]</span>
</p>
</dd>
</dl>

<ul>
<li><p>
<em>Proof</em>
</p>

<ul>
<li>
Since <span class="math">\(x = (x_1, \cdots x_4)\)</span> and <span class="math">\(y = (y_1, \cdots, y_4)\)</span> are distinct, these quadruples must differ in some component, assume their not equal.
</li>
<li>
We wish to compute the probability <span class="math">\(\Pr[h_a(x_1, \cdots, x_4) = h_a (y_1 \cdots y_4)]&lt;/li&gt; &lt;li&gt;That is, that  \)</span>$ <em>{i = 1}^{4} a</em>i x_i <em>{i = 1}^4 a</em>i y_i n <span class="math">\[&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This last equation can be rewritten
\]</span><em>{i = 1}^{3} a</em>i (x_i - y_i) a_4 (y_4 - x_4) n <span class="math">\($&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Suppose that we draw a random hash function \)</span>h_a$ by picking <span class="math">\(a = (a_1, a_2, a_3, a_4)\)</span> at random.
</p></li>
<li>
We start by drawing <span class="math">\(a_1, a_2, a_3\)</span>, and then pause to think: What is the probability that the last drawn number <span class="math">\(a_4\)</span> is such that the equation 2 bullet points back holds (call it (1))?
</li>
<li>
So far the left-hand side of equation (1) evaluates to some number, say <span class="math">\(c\)</span>.
</li>
<li>
And since <span class="math">\(n\)</span> is prime and <span class="math">\(x_4 \dne y_4\)</span>, <span class="math">\((y_4 - x_4)\)</span> has a unique inverse modulo <span class="math">\(n\)</span>.
</li>
<li>
Thus for equation (1) to hold, the last number <span class="math">\(a_4\)</span> must be precisely <span class="math">\(c \times (y_4 - x_4)^{-1} \bmod n\)</span>, out of its <span class="math">\(n\)</span> possible values.
</li>
<li>
The probability of this happening is <span class="math">\(\frac{1}{n}\)</span>, and the proof is complete.
</li>
</ul></li>
</ul></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 1.29, 1.34, 1.35
</li>
</ul>

<h3>
Divide and Conquer Algorithms and Recurrence Functions, Mergesort
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapter 2.1-2.2-2.3 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
<li>
Chapters 2.3,4.1,4.2,4.3 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>What are the basic principles of divide-and-conquer algorithms?</strong>
</p>

<ol>
<li>
Breaking it into <em>subproblems</em> that are themselves smaller instancs of the same type of problem.
</li>
<li>
Recursively solving these subproblems.
</li>
<li>
Appropriately combining their answers.
</li>
</ol></li>
<li><p>
<strong>Describe an approach for multiplication of two n-bit numbers that has a better running time than <span class="math">\(O(n^2)\)</span>. Prove its running time.</strong>
</p>

<pre><code>function multiply(x, y) {
    n = max(sizeof(x), sizeof(y));

    if (n == 1) {
        return xy;
    }

    x_l = leftmostBits(ceil(n / 2));
    x_r = rightmostBits(floor(n / 2));
    y_l = leftmost(ceil(n / 2));
    y_r = rightmostBits(floor(n / 2));

    p_1 = multiply(x_l, y_l);
    p_2 = multiplu(x_r, y_r);
    p_3 = multiply(x_l + x_r, y_l + y_r);

    return p_1 x 2^n + (p_3 - p_1 - p_2) x 2^(n / 2) + p_2;
}
</code></pre>

<ul>
<li>
<p>Run time</p>
<ul>
<li>
This running time can be derived by looking at the algorithm's pattern of recursive calls, which form a tree-structure.
</li>
<li>
At aech successive level of recursion the subproblems get halved in size. At the <span class="math">\(log_2 n\)</span>th level, the subproblems get down to size 1, and so the recursion ends.
</li>
<li>
Therefore, the high of the tree is <span class="math">\(\log_2 n\)</span>.
</li>
<li>
The branching factor is 4, each problem recursive produces three smaller ones - with the result that at depth <span class="math">\(k\)</span> in the tree there are <span class="math">\(3^k\)</span> subproblems, each of size <span class="math">\(n / 2^k\)</span>.
</li>
</ul></li>
</ul></li>
<li><p>
<strong>What does the Master theorem specify? Prove it.</strong>
</p>

<blockquote>
  <p>
<strong>Master theorem</strong>: If <span class="math">\(T(n) = aT(\lceil n / b \rceil) + O(n^d)\)</span> for some constants <span class="math">\(a &gt; 0, b &gt; 1, d \ge 0\)</span>, then: <span class="math">\[\begin{cases} O(n^d) &amp;amp;\mbox{if } d &gt; \log_b a &amp;#92; O(n^d \log n) &amp;amp;\mbox{if } d = \log_b a &amp;#92; O(n^{\log_b a}) &amp;amp;\mbox{if } d &amp;lt; lob_b a &amp;#92; \end{cases}\]</span>
</p>
</blockquote>

<ul>
<li>
<p>Proof</p>
<ol>
<li>
Assume that <span class="math">\(n\)</span> is a power of <span class="math">\(b\)</span>, for convinience rounding.
</li>
<li>
Notice that the size of the subproblems decreases by a factor of <span class="math">\(b\)</span> with each level of recursion, and therefore reaches the base case after <span class="math">\(log_b n\)</span> levels. This is the hight of the tree.
</li>
<li>
Its branching factor is <span class="math">\(a\)</span>, so the <span class="math">\(k\)</span>th level of the tree is made up of <span class="math">\(a^k\)</span> subproblems, each of size <span class="math">\(n / b^k\)</span>. The total work is done: <span class="math">\[a^k \times O(\frac{n}{b^k})^d = O(n^d) \times (\frac{a}{b^d})^k\]</span>
</li>
<li>
<p>As <span class="math">\(k\)</span> goes from 0 (the root) to <span class="math">\(\log_b n\)</span> (the leaves), these numbers form a geometric series with the ration <span class="math">\(a / b^d\)</span>. Finding the sum of such a series in big-O notation is easy and comes down to three cases:</p>
<ol>
<li>
The ratio is less than 1.
</li>
<li>
The ratio is greater than 1.
</li>
<li>
The ratio is exactly 1.
</li>
</ol></li>
</ol></li>
</ul></li>
<li><p>
<strong>You may be provided a divide-and-conquer algorithm and asked to argue about its running time by using the Master theorem.</strong>
</p></li>
<li><p>
<strong>How does mergesort work, what is its running time and why? How does the iterative version of mergesort work?</strong>
</p>

<p>
Mergesort splits an unsorted array into two and sorts and concatenates the subarrays.
</p>

<pre><code>function interative-mergesort(a[1 .... n]) {
    Q = [] (empty queue);
    for i = 1 to n:
         inject(Q, a[i])l

    while(count(Q) &gt; 1):
         inject(Q, merge(eject(Q), eject(Q));

    return eject(Q);
}
</code></pre></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 2.3, 2.4, 2.5, 2.12, 2.13, 2.14, 2.19, 2.25, 2.26, 2.31
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 2.3-1, 2.3-3, 4.3-1, 4.3-2, 4.3-3, 4.3-4
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 4-1, 4-2, 4-3, 4-4
</li>
</ul>

<h3>
Quicksort, Lower bounds for comparison-based sorting
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapter 7.1, 7.2, 7.3, 7.4, 8.1 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
<li>
Chapter 2.3 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li>
<strong>What are comparison sorting algorithms? What is the lower limit for the running time of comparison sorting algorithms?</strong>
</li>
<li><p>
<strong>How does quick-sort work? When does the worst-case running time arise? When does the best-case running time arise?</strong>
</p>

<ol>
<li>
Pick an element, called a pivot, from the list.
</li>
<li>
Reorder the list so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.
</li>
<li>
Recursively apply the above steps to the sub-list of elements with smaller values and separately the sub-list of elements with greater values.
</li>
</ol>

<blockquote>
  <p>
The best case for the algorithm now occurs when all elements are equal. The worst case for the algorithm occurs when the elements are already sorted.
</p>
</blockquote></li>
<li><p>
<strong>Provide a rigorous proof for the worst-case performance of quick-sort.</strong>
</p></li>
<li>
<strong>Provide a rigorous proof for the expected running time of quick-sort.</strong>
</li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 7.1-4, 7.2-2, 7.2-3, 7.2-4, 7.2-5, 7.3-1, 7.3-2, 7.4-1, 7.4-2, 7.4-3, 7.4-4, 7.4-5, 8.1-1, 8.1-3
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 7-3, 7-4, 7-5
</li>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 2.17, 2.18, 2.24
</li>
</ul>

<h3>
Computing Medians and Other Order Statistics, Linear-time Sorting Solutions
</h3>

<h4>
Reading material
</h4>

<ul>
<li>
Chapters 2.4 from <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</li>
<li>
Chapters 8.2, 8.3, 8.4, 9.1, 9.2, 9.3 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li>
<strong>How can we efficiently compute the median of a set of numbers? What is the running time of this solution?</strong>
</li>
<li><p>
<strong>What is the main idea behind counting sort? Provide a description of the algorithm and argue about its running time.</strong>
</p>

<blockquote>
  <p>
the algorithm loops over the items, computing a histogram of the number of times each key occurs within the input collection. It then performs a prefix sum computation (a second loop, over the range of possible keys) to determine, for each key, the starting position in the output array of the items having that key. Finally, it loops over the items again, moving each item into its sorted position in the output array.
</p>
</blockquote>

<pre><code># variables:
#    input -- the array of items to be sorted; key(x) returns the key for item x
#    n -- the length of the input
#    k -- a number such that all keys are in the range 0..k-1
#    count -- an array of numbers, with indexes 0..k-1, initially all zero
#    output -- an array of items, with indexes 0..n-1
#    x -- an individual input item, used within the algorithm
#    total, oldCount, i -- numbers used within the algorithm

# calculate the histogram of key frequencies:
for x in input:
    count[key(x)] += 1

# calculate the starting index for each key:
total = 0
for i in range(k):   # i = 0, 1, ... k-1
    oldCount = count[i]
    count[i] = total
    total += oldCount

# copy to output array, preserving order of inputs with equal keys:
for x in input:
    output[count[key(x)]] = x
    count[key(x)] += 1

return output
</code></pre>

<p>
Because the algorithm uses only simple for loops, without recursion or subroutine calls, it is straightforward to analyze. The initialization of the Count array, and the second for loop which performs a prefix sum on the count array, each iterate at most k + 1 times and therefore take O(k) time. The other two for loops, and the initialization of the output array, each take O(n) time. Therefore the time for the whole algorithm is the sum of the times for these steps, O(n + k).
</p>

<p>
Because it uses arrays of length k + 1 and n, the total space usage of the algorithm is also O(n + k).[1] For problem instances in which the maximum key value is significantly smaller than the number of items, counting sort can be highly space-efficient, as the only storage it uses other than its input and output arrays is the Count array which uses space O(k).
</p></li>
<li><p>
<strong>What is the main idea behind radix sort? Provide a description of the algorithm and argue about its running time.</strong>
</p>

<p>
<em>Radix sort</em> works by first sorting by the least significant bit, then the next most significant bit, all the way to the most significant bit.
</p></li>
<li><p>
<strong>Prove the correctness of radix sort.</strong>
</p>

<p>
For a value <span class="math">\(r \le b\)</span>, we view each key as having <span class="math">\(d = \lceil b / r \rceil\)</span> digits of <span class="math">\(r\)</span> bits each. Each digit is an integer in the range of 0 to <span class="math">\(2^r - 1\)</span>, so that we can use counting sort with <span class="math">\(k = 2^r - 1\)</span>. Each pass of counting sort takes time <span class="math">\(\Omega(n + k) = \Omega(n + 2^r)\)</span> and there are <span class="math">\(d\)</span> passes. This makes a total running time of <span class="math">\(\Omega(d(n + 2^r))\)</span>.
</p></li>
<li><p>
<strong>What is the main idea behind bucket sort? Provide a description of the algorithm. Prove its running time.</strong>
</p>

<ul>
<li><p>
Idea
</p>

<ol>
<li>
Set up an array of initially empty &quot;buckets&quot;.
</li>
<li>
Scatter: Go over the original array, putting each object in its bucket.
</li>
<li>
Sort each non-empty bucket.
</li>
<li>
Gather: Visit the buckets in order and put all elements back into the original array.
</li>
</ol>

<blockquote>
  <p>
The idea behind bucket sort is that if we know the range of our elements to be sorted, we can set up buckets for each possible element, and just toss elements into their corresponding buckets. We then empty the buckets in order, and the result is a sorted list.
</p>
</blockquote></li>
<li><p>
Psuedocode
</p>

<pre><code>function bucketSort(array, n) is
    buckets ← new array of n empty lists
    for i = 0 to (length(array)-1) do
        insert array[i] into buckets[msbits(array[i], k)]

    for i = 0 to n - 1 do
        nextSort(buckets[i]);

    return the concatenation of buckets[0], ...., buckets[n-1]
</code></pre></li>
</ul></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 2.15, 2.16, 2.17, 2.20, 2.21, 2.22, 2.23
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 8.2-2, 8.2-4, 8.3-2, 8.3-3, 8.3-4, 8.4-2, 8.4-3, 9.1-1, 9.3-5, 9-3.7, 9.3-8, 9.3-9
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 8-2, 8-3, 8-6, 9-1, 9-2, 9-3
</li>
</ul>

<h3>
Greedy Algorithms: Huffman encoding
</h3>

<h4>
Reading material
</h4>

<h5>
Chapter 5.2 <abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>
</h5>

<ul>
<li><p>
In general, how do we find the optimal coding tree, given the frequencies <span class="math">\(f_1 \cdots f_n\)</span> of <span class="math">\(n\)</span> symbols? To make the problem precise, we want a tree whose leaves each correspond to a symbol and which minimizes the overall length of the encoding. <strong>Cost of tree</strong>, where <span class="math">\(d\)</span> is the depth of the <span class="math">\(i\)</span>th symbol in the tree:
</p>

<p>
<span class="math">\[\sum_{i = 1}^n f_i \times d\]</span>
</p></li>
<li><p>
We can define the frequency of any internal node to be the sum of the frequencies of its descendant leaves; this is, after all, the number of times the internal node is visit during encoding or decoding.
</p>

<ul>
<li><p>
The total cost can be expressed thusly:
</p>

<blockquote>
  <p>
The cost of a tree is the sum of the frequences of all leaves and internal nodes, except the root.
</p>
</blockquote></li>
</ul>

<pre><code>function Huffman(f)
input: Any array f[1 ... n] of frequencies
output: An encoding tree with n leaves

let H be a priority queue of integers, ordered b f
for i = 1 to n: insert(H, i)
for k = n + 1 to 2n - 1:
    i = deletemin(h), j = deletemin(h)
    create a node numbered k with children i, j
    f[k] = f[i] + f[j]
    insert(H, k)
</code></pre></li>
</ul>

<h5>
Chapter 16.2 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</h5>

<dl>
<dt>
Greedy algorithm
</dt>
<dd>
<p>
A greedy algorithm always makes the choice that looks best at the moment. That is, it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution.
</p>
</dd>
</dl>

<ul>
<li>
Greedy algorithms do not always yield optimal solution, but for many problems they do.
</li>
<li>
<p>The following steps apply to a greedy algorithm strategy:</p>
<ol>
<li>
Determine the optimal substructure of the problem.
</li>
<li>
Develop a recursive solution.
</li>
<li>
Prove that at any stage of the recursion, one of the optimal choices is the greedy choise. Thus, it always safe to make the greedy choice.
</li>
<li>
Show that all but one of the subproblems induced by having made the greedy choice are empty.
</li>
<li>
Develop a recursive algorithm that implements the greedy strategy.
</li>
<li>
Convert the recursive algorithm to an iterative algorithm.
</li>
</ol></li>
</ul>

<h5>
Chapter 16.3 from <abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>
</h5>

<h5>
Wikipedia
</h5>

<ul>
<li><p>
A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum.
</p>

<ul>
<li>
For instance, to solve the travelling salesman problem with a greedy strategy, adopt the following strategy: &quot;At each stage visit an unvisited city nearest to the current city.&quot;
</li>
</ul></li>
<li><p>
There are five components:
</p>

<ol>
<li>
A candidate set, from which a solution is created.
</li>
<li>
A selection function, which chooses the best candate to be added to the solution.
</li>
<li>
A feasibility function, that is used to determine if a candidate can be used to contribute to a solution.
</li>
<li>
An objective function, which assigns a value to a solution, or a partial solution, and
</li>
<li>
A solution function, which will indicate when we have discovered a complete solution.
</li>
</ol></li>
</ul>

<h4>
Practice questions
</h4>

<ul>
<li><p>
<strong>What is the main principle behind greedy algorithms?</strong>
</p>

<ul>
<li>
The main principle behind greedy algorithms is to always, in any given decision point, to select the locally optimal choice and hope that it leads to a globally optimal choice (which has no guarentee).
</li>
</ul></li>
<li><p>
<strong>Why does the greedy algorithm work for the coin changing problem given the US coin system?</strong>
</p>

<ul>
<li>
The US coin system placed in a set of integers has the property of being a <strong>matroid</strong>.
</li>
</ul></li>
<li><p>
<strong>What is the idea in Huffman encoding in order to achieve data compression? What is the prefix-free property?</strong>
</p>

<ul>
<li>
Huffman oding is an entropy encoding algorithm used for lossless data compression.
</li>
<li>
The term refers to the use of a variable-length code table for encoding a source symbol (such as a character in a file) where the variable-length code table has been derived in a particular way based on the estimated probability of occurance for each possible value of the source symbol.
</li>
</ul></li>
<li><p>
<strong>Provide the Huffman encoding algorithm and argue its running time.</strong>
</p>

<ol>
<li>
Create a leaf node for each symbol and add it to the priority queue.
</li>
<li><p>
While there is more than one node in the queue:
</p>

<ol>
<li>
Remove the two nodes of highest priority (lowest probability) from the queue.
</li>
<li>
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes probabilities.
</li>
<li>
Add the new node to the queue.
</li>
</ol></li>
<li><p>
The remaining node is the root node and the tree is complete.
</p></li>
</ol></li>
<li><p>
<strong>Prove the greedy choice property for the Huffman encoding algorithm (first lemma).</strong>
</p>

<blockquote>
  <p>
<strong><em>Lemma 16.2</em></strong> Let <span class="math">\(C\)</span> be an alphabet in which each character <span class="math">\(c \in C\)</span> has frequency <span class="math">\(f[c]\)</span>. Let <span class="math">\(x\)</span> and <span class="math">\(y\)</span> be two character in <span class="math">\(C\)</span> having the lowest frequencies. Then there exists an optimal prefix code <span class="math">\(C\)</span> in which the codewords for <span class="math">\(x\)</span> and <span class="math">\(y\)</span> have the same length and differ only in the last bit.
</p>
</blockquote></li>
<li><p>
<strong>Prove the optimal substructure property for the Huffman encoding algorithm (second lemma).</strong>
</p></li>
</ul>

<h4>
Related exercises
</h4>

<ul>
<li>
<abbr title="S. Dasgupta, C. H. Papadimitriou, and U. V. Vazirani">DPV</abbr>: 5.13, 5.14, 5.15, 5.16, 5.17, 5.18, 5.19, 5.29, 5.30, 5.31
</li>
<li>
<abbr title="T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein (Second Edition)">CLRS2</abbr>: 16.1, 16.2-4, 16.2-5, 16.2-7, 16.3-1, 16-3.2, 16.3-3, 16.3-4, 16.3-5, 16.3-6
</li>
</ul>

<h2>
March 5th, 2014 <small>Homework 2</small>
</h2>

<h3>
Topics
</h3>

<ul>
<li>
Master theorem
</li>
<li>
Approximation algorithm
</li>
</ul>

<h2>
March 5th, 2014 <small>Lecture</small>
</h2>

<h3>
High-level steps in greedy algorithm
</h3>

<ul>
<li>
This idea of defining smaller problems, it's part of greedy and recursive algorithms.
</li>
<li><p>
Steps
</p>

<ol>
<li><p>
Cast the problemas one where we make a choice, and then we're left with a subproblem to solve.
</p>

<blockquote>
  <p>
Which characters should I place at the bottom of my tree? <em>Huffman coding</em>
</p>
</blockquote></li>
<li><p>
Show there is always an optimal solution that makes the greedy choice.
</p></li>
<li>
Show that the subproblems optimal solution can be combined with the greedy choice to provide an optimal solution to the original problem.
</li>
</ol></li>
</ul>

<blockquote>
  <p>
Can all optimization problems be solved with greedy algorithms?
</p>
</blockquote>

<ul>
<li>
For instance, in the coin problem, it depends on the coin denominations you have.
</li>
<li>
In Huffman coding, it works nicely.
</li>
</ul>

<h3>
Set cover example and a greedy approach
</h3>

<ul>
<li><p>
<strong>Example</strong>: Consider a collectino of cities. We need to place a school to the minimum number of cities so that each one has a direction connection with a school.
</p>

<table>
<thead>
<tr>
  <th>
<span class="math">\(x\)</span>
</th>
  <th>
<span class="math">\(S_x\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
  <td>
<span class="math">\(a\)</span>
</td>
  <td>
<span class="math">\(a, b, c\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(b\)</span>
</td>
  <td>
<span class="math">\(a, b, c, d, e\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(c\)</span>
</td>
  <td>
<span class="math">\(a, b, c, g\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(d\)</span>
</td>
  <td>
<span class="math">\(b, d, e, f\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(e\)</span>
</td>
  <td>
<span class="math">\(b, d, e, f\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(f\)</span>
</td>
  <td>
<span class="math">\(e, d, f\)</span>
</td>
</tr>
<tr>
  <td>
<span class="math">\(g\)</span>
</td>
  <td>
<span class="math">\(e, g\)</span>
</td>
</tr>
</tbody>
</table>

<ul>
<li>
The greedy solution: <span class="math">\(b, c, d\)</span> (3 schools).
</li>
<li>
But there are optimal solutions with 2 schools, which in this case would be something like <span class="math">\(c\)</span> and <span class="math">\(f\)</span>.
</li>
</ul></li>
</ul>

<blockquote>
  <p>
<strong>Lemma</strong>: Given <span class="math">\(n\)</span> elemnts and <span class="math">\(k\)</span> sets in the optimal solution, the greedy algorithm will use at most <span class="math">\(k \times \ln n\)</span>.
</p>
</blockquote>

<ul>
<li><p>
This is a little reassuring
</p>

<ul>
<li>
Not the optimum solution, but we know it'll be <em>pretty good</em> relative to the optimum.
</li>
</ul></li>
<li><p>
<strong>Proof</strong>
</p>

<ul>
<li>
<span class="math">\(n_t\)</span>: number of elements after <span class="math">\(t\)</span> iterations
</li>
<li>
<span class="math">\(n_0\)</span> will be <span class="math">\(n\)</span> and after all elements are solution <span class="math">\(\to 0\)</span>.
</li>
<li><p>
Given that we're going to be using <span class="math">\(k\)</span> sets in the optimum solution, can we say anything about the number of elements in sets in the optimum solution?
</p>

<ul>
<li>
All the elements in the optimum solution are going to have less than ... something something.
</li>
</ul></li>
<li><p>
<em>Optimum solution</em> has <span class="math">\(k\)</span> sets covering <span class="math">\(n\)</span> elements.
</p></li>
<li>
There is at least one set that has at least <span class="math">\(\frac{n}{k}\)</span> elements.
</li>
<li>
Now lets think how the greedy choice will be making the somehting selection.
</li>
<li><p>
Then, <span class="math">\[n_{t + 1} \le n_t - \frac{n_t}{k} = n_t(1 - \frac{1}{k})\]</span>
</p></li>
<li><p>
Repeated applications of this rule imply <span class="math">\(n_t \le n_0 (1 - \frac{1}{k}^t)\)</span>.
</p>

<ul>
<li>
Note that <span class="math">\(1 - x \le e^{-x} \forall x\)</span>.
</li>
</ul></li>
<li><p>
Thus, <span class="math">\[n_t \le n_0 (e^{- \frac{1}{k}})^t = n \cdot e^{\frac{-t}{k}}\]</span>
</p></li>
<li><p>
When will I cover all elemnts with the greedy approach?
</p>

<ul>
<li>
It has to be that <span class="math">\(n_t = 0\)</span>, i.e., for <span class="math">\(t = k \cdot \ln n : n e^{\frac{-t}{k}} = n \cdot e \cdot \frac{k \ln n}{k} = 1\)</span><span class="math">\(&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;At that point \)</span>(t = k n)$, the greedy choice has covered every element.
</p></li>
</ul></li>
</ul>

<h3>
Alternative for optimization problems: Dynamic Programming
</h3>

<ul>
<li><p>
Think of this as a more general approach than the greedy algorithm approach.
</p>

<ul>
<li>
Any problem that can be solved by a greedy algorithm can be soloved by a dynamic programming apporach.
</li>
<li>
Now you have to work out what is the subproblem.
</li>
<li>
This is more powerful than greedy apporached.
</li>
</ul></li>
<li><p>
<strong>Example</strong>
</p>

<ul>
<li>
Lets say you have matrices and you want to multiply big matrics and supposed you have to do it in such a way that is computationally effieicne.t
</li>
<li>
Say it's four matrices.
</li>
<li>
There are many different ays to compute this product depending where we place the parentheses.
</li>
<li>
You should come up with an algorithm that tells you the optimum order.
</li>
<li>
Greedy choice corresponds to not necessarily the optimum solution.
</li>
<li><p>
So how do we determine the optimum case?
</p>

<ul>
<li>
Again we can think of a graphical way of representing the cases.
</li>
</ul></li>
<li><p>
<strong>Clue</strong>: If a tree corresponds to the minumum number of multiplications, then each subtree of it has to be optimum.
</p></li>
<li>
Define the cost of multiplying: <span class="math">\(A_i \times A_{i + 1} \times \cdots \times A_j\)</span>.
</li>
</ul></li>
</ul>

<h2>
March 7th, 2014 <small>Lecture</small>
</h2>

<h3>
Dynamic Programming
</h3>

<ul>
<li><p>
<strong>Example</strong>: Matrix multiplication
</p>

<p>
<span class="math">\[\mathcal{A}&lt;em&gt;{m \times n} \times \mathcal{A}&lt;/em&gt;{n \times p}\]</span>
</p>

<ul>
<li><p>
For each element in the resultimg <span class="math">\(m \times p\)</span> matrix you have to perform <span class="math">\(O(n)\)</span> operations. So overall,
</p>

<p>
<span class="math">\[O(m p n)\]</span>
</p></li>
<li><p>
And if you have a large number of matrices?
</p>

<p>
<span class="math">\[\mathcal{A}_1 \times \mathcal{A}_2 \times \mathcal{A}_3 \times \mathcal{A}_4 \times \mathcal{A}_5\]</span>
</p>

<ul>
<li>
You have <span class="math">\(n - 1\)</span> choices for which matrix to multiply first.
</li>
<li>
This is starting to define subproblems.
</li>
<li>
If you've made the choice that the way you're going to make this product is split the first one from the others, then you're going to be able to decided how to compute the next one. This form a tree.
</li>
<li>
If you write this ut, you'll see that you're performing the same operation again and again. <em>What does this make a computer scientist think?</em> <strong>Caching! Woo!</strong>
</li>
<li>
You need to follow a bottom-up approach.
</li>
<li>
The useful way to represent it is the matrix of cost operations.
</li>
</ul></li>
</ul></li>
<li><p>
Lets say you have have to compute the product of matrices from <span class="math">\(\mathcal{A}_i\)</span> to <span class="math">\(\mathcal{A}_j\)</span>.
</p>

<ul>
<li><p>
You need to split these things into two subsets, that is: <span class="math">\[\mathcal{A}&lt;em&gt;{i} \cdots \mathcal{A}&lt;/em&gt;{k - 1}\]</span>
</p>

<p>
and
</p>

<p>
<span class="math">\[\mathcal{A}&lt;em&gt;{k} \cdots \mathcal{A}&lt;/em&gt;{j}\]</span>
</p></li>
</ul></li>
</ul>

<!-- Abbreviations --> 
{% endraw %}